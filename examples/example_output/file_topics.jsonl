{"url": "https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/", "title": "IBM Developer", "content": "", "metadata": {"length": 0, "has_title": true, "domain": "developer.ibm.com"}}
{"url": "https://www.projectpro.io/article/deep-learning-architectures/996", "title": "8 Deep Learning Architectures Data Scientists Must Master", "content": "Project Library Courses Custom Project Path Resources 8 Deep Learning Architectures Data Scientists Must Master From artificial neural networks to transformers, explore 8 deep learning architectures every data scientist must know. Deep learning architectures are at the forefront of transforming artificial intelligence AI by introducing innovative capabilities. These advanced structures, inspired by the human brains neural networks, empower machines to comprehend, learn, and make independent decisions. Through intricate processing layers, deep learning models excel in deciphering complex patterns and adapting to sophisticated applications. Deep learning architectures have led to remarkable advancements in applications like image recognition and natural language processing, revolutionizing how machines interact with and interpret information. Their capacity to extract meaningful features from data and continually learn enables machines to evolve, promising a future where intelligent systems play a pivotal role in various aspects of our lives. Build CNN for Image Colorization using Deep Transfer Learning Downloadable solution code  Explanatory videos  Tech Support In this blog, we will explore the inner workings of popular deep-learning architectures in a beginner-friendly manner. You will learn about the evolution of these models over time and their diverse applications. The blog will also highlight recent developments in the industry, providing invaluable insights into the dynamic landscape of deep learning architecture. Table of Contents Introduction to Deep Learning Architectures Deep learning architectures have evolved significantly over time, driven by advancements in research, computational power, and data availability. Initially, simple architectures like perceptrons paved the way for more complex models such as convolutional neural networks CNNs and recurrent neural networks RNNs. Introducing dropout regularization, batch normalization, and attention mechanisms improved model performance. Recent trends include the rise of Transformer models for natural language processing NLP systems and the exploration of hybrid architectures combining neural networks with symbolic reasoning. Overall, the evolution has been characterized by continuous innovation, leading to increasingly powerful and versatile deep-learning architectures, as summarized in this post by Diji Jayakaran. Before we unravel the nitty grits of complex deep learning architectures, lets take a step back and explore the fundamental components that form the backbone of these intelligent systems. Key Components of Deep Learning Architectures The journey of exploring the world of deep learning architectures is like navigating a complex maze. However, fear not, as we will begin our exploration with a quick revision to the simplest form of a neural network perceptron. A perceptron makes binary decisions  a simple yes or no based on the weighted sum of input features. We recommend you dive into the details of this model in this Neural Network blog. Here is a quick recap that will prepare you for the details of the deep learning architectures we will soon discuss. Neurons Neons are at the heart of any neural network, including the perceptron. These digital entities receive inputs, apply weights, and produce an output. Neurons are the building blocks through which information flows in a neural network. Just as neurons in our brains communicate, these fundamental blocks communicate in the language of numbers. Activation Functions Activation functions, such as the step function in the perceptron, determine whether the neuron fires. In other words, they decide whether the information flowing through should be allowed to contribute to the output. Picture it as a thresholdif the incoming data is above a certain level, the perceptron fires or produces an output otherwise, it remains silent. This binary decision-making process showcases the essence of activation functions in shaping the output of our digital neurons. Weights and Biases Weights and biases are the adjustable parameters in the network that influence the importance of input features and establish a threshold for activation. Each input data feature is given a certain weight, indicating its importance in the decision. Biases act as the minimum requirement for a feature to contribute. Adjusting weights and biases during model training refines its ability to make accurate predictions. Loss Functions Loss functions quantify the difference between the predicted output and the actual target. The objective is to minimize this difference during the training process. In the context of the perceptron, the concept of error or loss becomes evident. The perceptron learns by reducing the difference between its prediction and the actual target, laying the groundwork for understanding more sophisticated loss functions in advanced neural networks. Mean Squared Error MSE and Cross-Entropy Loss are examples of loss functions commonly used in different use cases.. Optimizers Although more straightforward in the perceptron context, optimizers are crucial for adjusting weights and biases based on the computed loss. They fine-tune the model parameters to minimize the loss and improve overall performance. This mechanism hints at the broader optimization techniques in more complex deep learning architectures. Popular optimizing techniques include Stochastic Gradient Descent SGD, Adam, and RMSprop. Understanding these foundational deep learning concepts will help us explore more complex deep learning architectures, where multiple layers of neurons interact to solve intricate problems. Now that you are thoroughly familiar with the basics, lets explore the different types of deep learning architectures. Check Out ProjectPros Deep Learning Course to Gain Practical Skills in Building and Training Neural Networks! Heres what valued users are saying about ProjectPro Abhinav Agarwal Graduate Student at Northwestern University Ed Godalle Director Data Analytics at EY  EY Tech Not sure what you are looking for? Deep Learning Architectures Types This section will discuss different deep learning architectures used for AI applications. We will start with the simplest architectures and gradually move to the complex ones. Artificial Neural Networks Artificial Neural Networks ANNs serve as the foundational architecture in deep learning. At its core, an ANN is characterized by layers of interconnected nodes comprising an input layer, one or more hidden layers, and an output layer. Information flows in a unidirectional manner, progressing from the input layer through the hidden layers to generate the final output. Each node in the network represents a neuron and is connected to nodes in the adjacent layers. These connections are associated with weights, which determine the strength of the connection. The input data is fed into the network, and as it passes through each layer, the weighted sum is computed, and an activation function determines the output of each node. This process continues until the final layer produces the networks output. Next, the backpropagation technique improves the models performance and accuracy. Backpropagation, a vital concept, facilitates learning by adjusting these weights based on prediction errors. Input data traverses the network, accumulating weighted sums at each layer, where activation functions govern node outputs. This iterative process reaches its peak in the final output layer, generating the networks output. Video httpswww.youtube.comwatch?v5bxZ4hd7wcA The advantages of ANNs lie in their simplicity, ease of implementation, and the ability to model complex relationships within data. Their capacity to learn from large datasets enables them to generalize well to new, unseen data, making them a preferred choice in various real-world scenarios. Applications ANNs find applications in diverse fields, showcasing their adaptability and effectiveness. They excel in pattern recognition systems like image and speech recognition. The layered structure allows them to learn hierarchical representations of features, making them adept at discerning intricate patterns within data. These networks are widely employed in classification problems, where the goal is to assign inputs to specific categories, and regression problems, where the network predicts numerical values. Applications range from predicting stock prices to classifying emails as spam or non-spam. One of the key strengths is their capability to approximate any continuous function. This makes ANNs versatile, enabling them to model complex relationships in diverse domains. Video httpswww.youtube.comwatch?vIyxfkXOxRrA Convolutional Neural Networks CNNs A Convolutional Neural Network CNN is a robust architecture for image processing, feature learning, and classification projects. It comprises three essential layers and is tailored to efficiently process input image data, making it indispensable in various applications. Convolutional Layer CONV At the core of CNNs, convolutional layers execute crucial operations. These layers utilize kernels or filters to perform convolution operations, adjusting horizontally and vertically based on the stride rate. The convolutional layer incorporates non-linear activation functions, with Rectified Linear Unit ReLU being the most widely used. ReLU enhances the networks ability to capture complex patterns by introducing non-linearity. Pooling Layer POOL The pooling layer, responsible for dimensionality reduction, minimizes computational requirements while retaining essential features. Two common types are maximum pooling, which selects the maximum value within a kernel area, and average pooling, which calculates the average values within the kernel. Pooling enhances the networks robustness and helps manage computational resources efficiently. Fully Connected Layers FC Operating on a flattened input, the fully connected layer connects each input to every neuron, initiating the classification process. Typically located near the networks end, FC layers perform mathematical operations, finalizing the classification task. Video httpswww.youtube.comshorts5bxZ4hd7wcA Beyond these layers, additional components contribute to CNN functionality. The activation function influences the classification outcome, especially in the last fully connected layer. The softmax function, often employed, normalizes output values to represent target class probabilities. Dropout layers prevent overfitting during training, nullify specific neurons contributions, and promote more robust learning across different training data batches. Understanding these building blocks forms the basis for exploring popular CNN architectures such as ImageNet, VGG-16, VGG-19, etc., each tailored to specific projects within the expansive domain of deep learning algorithms. Applications Convolutional Neural Networks CNNs are pivotal in diverse applications, revolutionizing image and pattern recognition. In computer vision, CNNs excel in object detection, facial recognition, and autonomous vehicle navigation systems. Their prowess extends to medical imaging, aiding in diagnosis through accurate analysis of medical scans. In natural language processing, CNNs contribute to sentiment analysis and language translation. Additionally, CNNs find applications in fields such as finance for fraud detection and robotics to enhance perception capabilities. The adaptability and efficiency of CNNs make them indispensable across industries, shaping the forefront of innovative technological advancements. Video httpswww.youtube.comwatch?vW9PkTnNywE0 Recurrent Neural Networks RNNs Recurrent Neural Networks RNNs stand out in their ability to handle sequential data by introducing a memory element, allowing them to capture temporal dependencies. Unlike feedforward networks, RNNs possess connections that form loops, enabling them to retain information about previous inputs. This recursive architecture allows RNNs to process sequences that are well-suited for speech recognition, language modeling, and time-series prediction problems. However, traditional RNNs have limitations in capturing long-term dependencies. Long Short-Term Memory LSTM and Gated Recurrent Unit GRU architectures address this issue, enhancing RNNs efficacy in handling and learning from sequential data. Applications In Natural Language Processing NLP, RNNs shine in language modeling, machine translation, and sentiment analysis. Their ability to understand and generate sequences makes them invaluable in context comprehension problems. Additionally, RNNs are significant in time-series analysis, forecasting trends in financial markets, predicting stock prices, and analyzing physiological data. The recurrent nature of these networks enables them to model dependencies over time, providing accurate predictions and insights in dynamic and sequential data scenarios. The versatility of RNNs positions them as essential tools in extracting meaningful patterns from temporal information across various domains. Long Short-Term Memory LSTM Long Short-Term Memory LSTM networks represent a crucial advancement in recurrent neural network RNN architecture, specifically tailored to overcome the challenges posed by traditional RNNs in capturing long-range dependencies within sequential data. LSTMs excel in tasks such as speech recognition, language modeling, and more, offering several key features Memory Cells LSTMs are equipped with specialized memory cells capable of retaining information over extended periods. This attribute allows LSTMs to capture and maintain context and dependencies crucial for understanding sequential data over long sequences. Gates LSTMs integrate three essential gates the input gate, forget gate, and output gate. These gates play a pivotal role in regulating the flow of information into and out of the memory cell. The input gate manages the flow of new information into the memory cell, the forget gate decides which information to discard, and the output gate controls the information flow to the next layer. Vanishing Gradient Problem LSTMs effectively address the vanishing gradient problem encountered in traditional RNNs. By maintaining a constant error flow through time, LSTMs ensure the unhindered propagation of gradients during backpropagation. This capability facilitates better learning of long-term dependencies, enhancing the networks ability to capture intricate patterns in sequential data. Applications LSTMs have emerged as a backbone in various applications that demand sequential data processing. Their unique ability to capture and retain intricate patterns and dependencies over extended sequences makes them indispensable in natural language processing, speech recognition, and time-series analysis. Incorporating memory cells and gating mechanisms sets LSTMs apart, enabling them to excel in scenarios where traditional RNNs struggle to capture the complexity of long-term dependencies. Gated Recurrent Unit GRU Gated Recurrent Units GRUs are a distinctive recurrent neural network RNN architecture designed to overcome challenges like the vanishing gradient problem and enhance the modeling of long-term dependencies in sequential training datasets. GRUs share similarities with Long-Short-Term Memory LSTM networks but feature a simpler structure for the learning process. GRUs use a gating mechanism crucial for controlling information flow within the network. This mechanism comprises two gates the update gate and the reset gate. The update gate determines the proportion of information to retain or discard from the previous state, while the reset gate regulates the resetting of the internal state. GRUs have a more straightforward architecture than LSTMs, featuring fewer parameters and computations. This simplicity increases efficiency in training time and computational resources, making GRUs advantageous for various applications. Applications GRUs find widespread use across diverse applications demanding the processing of sequential data GRUs are extensively used in NLP applications, such as language modeling, sentiment analysis, and machine translation. Their ability to capture context and dependencies in sequential data makes them valuable for understanding and generating human-like language. In speech recognition systems, GRUs play a pivotal role in modeling sequential patterns, aiding in accurately recognizing spoken language. GRUs are well-suited to solve complex problems in time series prediction projects, forecasting future values based on historical data. Their efficiency in capturing temporal dependencies makes them valuable in financial forecasting, weather prediction, and other time-dependent domains. Despite their more straightforward structure, GRUs have effectively captured long-term dependencies. Their successful applications in natural language processing, speech recognition, and time series prediction underscore their versatility in handling sequential data across various domains. The balance between efficiency and performance makes GRUs compelling for problems requiring recurrent neural networks with streamlined architecture. Autoencoders Autoencoders are a specialized class of deep learning algorithms proficient in learning efficient representations of input data without explicit labels. Tailored for unsupervised learning, they focus on compressing and effectively representing input data through a two-fold structure comprising an encoder and a decoder. Encoder The encoder transforms raw input data into a reduced-dimensional representation termed the latent space or encoding. This encoding captures essential features and patterns from the input data, facilitating the extraction of meaningful information. Bottleneck Layer Situated at the end of the encoder, the bottleneck layer drastically reduces the dimensionality of the input data. It represents a compressed encoding of the original information, essential for efficient representation learning. Decoder The decoder reconstructs the initial input data from the encoded representation. Its objective is to rebuild the input as accurately as possible from the compressed encoding generated by the encoder, effectively performing the inverse operation. Autoencoders have emerged as a potent tool in deep learning, offering a unique approach to unsupervised learning. Their versatility extends to domains like image processing, anomaly detection, and beyond, showcasing their value in diverse applications across the field of artificial intelligence. Autoencoders are extensively used for dimensionality reduction tasks, compressing high-dimensional data into lower-dimensional representations while retaining crucial information. They excel in anomaly detection systems, identifying deviations from standard patterns by comparing reconstructed data with original inputs. Autoencoders find application in data denoising tasks, cleaning up noisy images or audio data by reconstructing clean versions from corrupted inputs. Generative Adversarial Networks GANs Generative Adversarial Networks GANs are a class of deep neural networks designed for generating new, realistic data samples. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have become a powerful and widely used technique in machine learning and deep learning. The key idea behind GANs is to pit two neural networks against each other in a kind of adversarial game, leading to the generation of high-quality, realistic data. A generative adversarial network GAN system consists of a generator and a discriminator network engaged in adversarial training. The generator analyzes and modifies data attributes, while the discriminator evaluates the generated outputs authenticity. Through iterative training, the generator aims to maximize the discriminators error, while the discriminator seeks to minimize errors. This dynamic interplay continues until an equilibrium is reached, rendering the discriminator unable to distinguish between real and synthesized data. This signifies the completion of the training process, illustrating GANs iterative, competitive nature in producing realistic data. Generative Adversarial Networks GANs encompass various types tailored for specific applications. Vanilla GANs involve basic adversarial training, while Conditional GANs incorporate additional conditional information. Deep Convolutional GANs DCGAN use convolutional networks for image generation. Wasserstein GANs WGAN employ Wasserstein distance for stability. Progressive GANs PGAN gradually increase image resolution. CycleGAN enables unpaired image-to-image translation, and StyleGAN focuses on style control. Numerous other variants, such as InfoGAN and BigGAN, address diverse challenges, showcasing GANs versatility in generating realistic data for various purposes. Applications A few typical applications of GANs include GANs are widely used to generate high-quality, realistic images. They have been applied to create faces, objects, artwork, and even deepfake videos. GANs can be used for training data augmentation. This helps improve model generalization by exposing it to a more diverse set of examples. GANs can be employed for style transfer, allowing an image to transform its style while preserving its content. Transformer Architecture The Transformer model is a revolutionary encoder-decoder architecture that redefined machine learning, particularly in natural language processing NLP. It was introduced by Vaswani et al. in 2017 and has revolutionized sequence processing problems in NLP by replacing recurrent layers with self-attention mechanisms. In a traditional RNN, each element in a sequence is processed one at a time. In contrast, self-attention allows the model to weigh the importance of different elements in the sequence concerning each other. It also enables parallelization of computation, as each component can attend to all others simultaneously. This results in faster training compared to sequential processing. Applications Transformers have become the go-to architecture for various natural language processing NLP projects, such as machine translation, sentiment analysis, and text summarization. The attention mechanism in Transformers allows the model to consider each words context in a sentence when making predictions. This helps in capturing long-range dependencies in language. BERT, a pre-trained Transformer-based model, performs well on various NLP benchmarks. It considers both left and right context during pre-training, improving contextual understanding. Having explored so many architectures of deep learning algorithms, it is time to compare them and summarize their key differences. Comparison of Deep Learning Architectures in Python Lets compare the strengths and weaknesses of different architectures based on factors such as performance, training time, and resource requirements. Architecture Strengths Weaknesses Performance Training Time Resource Requirements Convolutional Neural Networks CNNs Excell in image-related projects Require substantial data for optimal performance High accuracy in image-related projects Moderate training time Moderate to high GPU requirements Recurrent Neural Networks RNNs Effective for sequential data Sensitive to vanishingexploding gradient problems Strong in projects involving time dependencies Longer training time for deep networks Moderate GPU requirements Long Short-Term Memory LSTM Mitigate vanishingexploding gradient issues Higher computational demands compared to basic RNNs Efficient in capturing long-term dependencies Longer training time for complex networks Moderate to high GPU requirements Gated Recurrent Unit GRU Simplifies architecture compared to LSTM May not perform as well on certain projects Balances performance and computational cost Faster training than LSTM Moderate GPU requirements Autoencoders Useful for unsupervised learning, data compression Sensitive to noise and outliers Efficient in learning data representations Moderate training time Moderate GPU requirements Generative Adversarial Networks GANs Exceptional at generating realistic data Prone to mode collapse, training instability High-quality data generation Sensitive to hyperparameter tuning High GPU requirements Transformer Architecture Enables efficient parallelization May struggle with very long sequences State-of-the-art in NLP projects Faster training due to parallelization Moderate to high GPU requirements Now that weve thoroughly reviewed deep learning algorithms and architectures, lets delve into their diverse applications across various domains of artificial intelligence. Deep Learning Architectures Applications Deep learning architectures have made significant strides in computer vision and natural language processing NLP, revolutionizing how we perceive and understand visual and textual data. Deep Learning Architectures for Computer Vision Deep learning architectures have brought a new era of image understanding and analysis. Convolutional Neural Networks CNNs are the critical element in this domain, offering unparalleled performance in systems like image classification, object detection, and semantic segmentation. CNNs leverage hierarchical layers of learnable filters to extract meaningful features from images, enabling accurate recognition and classification. Recent advancements in deep learning architectures for image classification have further enhanced the capabilities of CNNs. Techniques like transfer learning, where pre-trained CNN models are fine-tuned on specific datasets, have democratized access to state-of-the-art image classification models. Additionally, architectures like ResNet, DenseNet, and EfficientNet have pushed the boundaries of accuracy and efficiency, showcasing the continuous evolution of deep learning in computer vision. Here are a few project ideas that you can practice and realize the significance of these multilayer neural network architectures. OpenCV Project for Beginners to Learn Computer Vision Basics Medical Image Segmentation Deep Learning Project Image Segmentation using Mask R-CNN with Tensorflow Build a Multi Class Image Classification Model Python using CNN OpenCV Project to Master Advanced Computer Vision Concepts Build a CNN Model with PyTorch for Image Classification Build CNN for Image Colorization using Deep Transfer Learning Deep Learning Architectures for Natural Language Processing Deep learning architectures have also transformed how we analyze and understand textual data. Recurrent Neural Networks RNNs and their variants, such as Long Short-Term Memory LSTM networks and Gated Recurrent Units GRUs, have been instrumental in projects like language modeling, sentiment analysis, and machine translation. These architectures excel at processing sequential data, capturing temporal dependencies and contextual information within text. Text Classification with Transformers-RoBERTa and XLNet Model Build a Text Classification Model with Attention Mechanism NLP NLP Project for Multi Class Text Classification using BERT Model Build Multi Class Text Classification Models with RNN and LSTM Deep learning architectures continue to drive innovation and breakthroughs in computer vision and natural language processing. From CNNs serving as deep learning architectures for image classification to RNNs for language understanding, these architectures represent the foundation upon which cutting-edge AI applications are built, shaping the future of technology and human-machine interaction. As we explored the evolution of deep learning architectures and their evolution, its evident that these applications grow alongside algorithm updates. Lets explore the latest deep learning architectures and a few trends. Recent Trends in Deep Learning Architectures Several trends and breakthroughs, including the emergence of novel architectures and significant improvements in existing ones, have marked recent advancements in deep learning architectures. Some of the latest trends and developments include Self-supervised learning has gained traction, particularly in computer vision and NLP. Models are trained to predict certain portions of the input data, leading to representations that capture rich semantic information. Contrastive learning and momentum contrast methods have shown promising results in this area. GANs continue to evolve with advancements in training stability and quality of generated samples. Techniques like progressive growing, spectral normalization, and self-attention mechanisms have generated high-fidelity images and diverse data samples. Capsule networks, inspired by the human visual system, offer a new perspective on representing hierarchical relationships within data. Recent research focuses on enhancing the robustness and interpretability of capsule networks for projects like image recognition and object detection. GNNs have gained prominence in analyzing structured data such as graphs and networks. Recent advancements include graph attention networks GATs and graph convolutional networks GCNs, enabling effective relational information and node embedding modeling. Diffusion Convolutional Recurrent Neural Network DCRNN is tailored to enhance the efficiency of deep learning models, particularly in resource-constrained environments. It addresses issues like the vanishing gradient problem, enabling more effective utilization of computational resources. Federated Learning is an approach that facilitates collaborative model training across multiple devices without sharing raw data, mitigating privacy concerns and reducing computing and storage demands. Federated learning represents a paradigm shift in decentralized machine learning, offering scalable and privacy-preserving solutions. Explainable Artificial Intelligence XAI is dedicated to increasing the transparency and interpretability of machine learning models. By providing insights into model decision-making processes, XAI ensures fairness and impartiality, fostering trust and understanding between humans and AI systems. Hybrid architectures combining neural networks with symbolic reasoning or probabilistic models have emerged to address the limitations of purely data-driven approaches. These architectures aim to integrate the strengths of different paradigms for improved performance and interpretability. Keeping up with these trends might be challenging, and implementing them in your system could be even more complex. But dont worryin the next section, weve got a secret to share. Master Deep Learning Architectures with ProejctPro Yann LeCun, VP and Chief AI Scientist at Meta, rightfully underscores the importance of practical implementation beyond theory in this linkedin post. If you are looking for one platform that guides you through the path of deep learning through real-world projects, then ProjectPro stands as the answer. ProjectPro offers a repository of solved projects in data science and big data crafted by industry experts. Through hands-on experience, subscribers grasp practical limitations and fundamentals. Continuously updated with the latest tech tools, ProjectPro ensures staying ahead of trends. The dynamic platform provides a comprehensive learning experience, bridging the gap between theory and practice. Dont hesitatesubscribe today to ride on a journey of mastering deep learning architectures and keeping pace in the ever-evolving field of AI. FAQs 1. What is the architecture of the deep learning model? The architecture of a deep learning model refers to its structure, including the arrangement and connectivity of its layers, such as convolutional, recurrent, or dense layers, along with activation functions and connections between neurons. 2. Is CNN a deep learning architecture? Yes, CNN Convolutional Neural Network is a deep learning architecture commonly used in computer vision tasks, characterized by hierarchical layers of convolutional and pooling operations, followed by fully connected layers for classification or regression. 3. What are the 3 types of architecture of the neural network? The three popular types of architecture of neural networks are Feed forward Neural Networks FNNs Recurrent Neural Networks RNNs Convolutional Neural Networks CNNs PREVIOUS NEXT About the Author Manika Manika Nagpal is a versatile professional with a strong background in both Physics and Data Science. As a Senior Analyst at ProjectPro, she leverages her expertise in data science and writing to create engaging and insightful blogs that help businesses and individuals stay up-to-date with the Meet The Author Start Your First Project Learn By Doing Related Blogs on Deep Learning Projects Trending Blog Categories Project Categories Projects Blogs Certification Courses Tutorials ProjectPro  2024  2024 Iconiq Inc. About us Contact us Privacy policy User policy Write for ProjectPro", "metadata": {"length": 32992, "has_title": true, "domain": "projectpro.io"}}
{"url": "https://addepto.com/blog/deep-learning-architecture/", "title": "Deep Learning Architecture Examples - Addepto", "content": "Deep Learning Architecture Examples Author CSO  Co-Founder Reading time As you know from our previous article about machine learning and deep learning, DL is an advanced technology based on neural networks that try to imitate the way the human cortex works. Today, we want to get deeper into this subject. You have to know that neural networks are by no means homogenous. In fact, we can indicate at least six types of neural networks and deep learning architectures that are built on them. In this article, we are going to show you the most popular and versatile types of deep learning architecture. Soon, abbreviations like RNN, CNN, or DSN will no longer be mysterious. First of all, we have to state that deep learning architecture consists of deepneural networks of varying topologies. The general principle is that neural networks are based on several layers that proceed dataan input layer raw data, hidden layers they process and combine input data, and an output layer it produces the outcome result, estimation, forecast, etc.. Thanks to the development of numerous layers of neural networks each providing some function, deep learning is now more practical. Its a bit like a machine learning frameworkit allows you to make more practical use of this technology, accelerates your work, and enables various endeavors without the need to build an ML algorithm entirely from scratch. When it comes to deep learning, you have various types of neural networks. And deep learning architectures are based on these networks. Today, we can indicate six of the most common deep learning architectures Dont worry if you dont know these abbreviations we are going to explain each one of them. Lets start with the first one. What Are the Different Deep Learning Architectures RNN Recurrent Neural Networks RNNs RNN is one of the fundamental network architectures from which other deep learning architectures are built. RNNs consist of a rich set of deep learning architectures. They can use their internal state memory to process variable-length sequences of inputs. Lets say that RNNs have a memory. Every processed information is captured, stored, and utilized to calculate the final outcome. This makes them useful when it comes to, for instance, speech recognition1. Moreover, the recurrent network might have connections that feedback into prior layers or even into the same layer. This feedback allows them to maintain the memory of past inputs and solve problems in time. RNNs are very useful when it comes to fields where the sequence of presented information is key. They are commonly used in NLP i.a. chatbots, speech synthesis, and machine translations. Currently, we can indicate two types of RNN LSTM Long Short-Term Memory Its also a type of RNN. However, LSTM has feedback connections. This means that it can process not only single data points such as images but also entire sequences of data such as audio or video files3. LSTM derives from neural network architectures and is based on the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember whats essential and not just its last computed value. A typical LSTM architecture is composed of a cell, an input gate, an output gate, and a forget gate. The cell remembers values over arbitrary time intervals, and these three gates regulate the flow of information into and out of the cell. Today, LSTMs are commonly used in such fields as text compression, handwriting recognition, speech recognition, gesture recognition, and image captioning4. GRU This abbreviation stands for Gated Recurrent Unit. Its a type of LSTM. The major difference is that GRU has fewer parameters than LSTM, as it lacks an output gate5. GRUs are used for smaller and less frequent datasets, where they show better performance. CNN Convolutional Neural Networks CNNs This architecture is commonly used for image processing, image recognition, video analysis, and NLP. CNN can take in an input image, assign importance to various aspectsobjects in the image, and be able to differentiate one from the others6. The name convolutional derives from a mathematical operation involving the convolution of different functions. CNNs consist of an input and an output layer, as well as multiple hidden layers. The CNNs hidden layers typically consist of a series of convolutional layers. Heres how CNNs work First, the input is received by the network. Each input for instance, image will pass through a series of convolution layers with various filters. The control layer controls how the signal flows from one layer to the other. Next, you have to flatten the output and feed it into the fully connected layer where all the layers of the network are connected with every neuron from a preceding layer to the neurons from the subsequent layer. As a result, you can classify the output. DBN Deep Belief Network DBN is a multilayer network typically deep, including many hidden layers in which each pair of connected layers is a Restricted Boltzmann Machine RBM. Therefore, we can state that DBN is a stack of RBMs. DBN is composed of multiple layers of latent variables hidden units, with connections between the layers but not between units within each layer7. DBNs use probabilities and unsupervised learning to produce outputs. Unlike other models, each layer in DBN learns the entire input. In CNNs, the first layers only filter inputs for basic features, and the latter layers recombine all the simple patterns found by the previous layers. DBNs work holistically and regulate each layer in order. DBNs can be used i.a. in image recognition and NLP. DSN Deep Stacking Network We saved DSN for last because this deep learning architecture is different from the others. DSNs are also frequently called DCNDeep Convex Network. DSNDCN comprises a deep network, but its actually a set of individual deep networks. Each network within DSN has its own hidden layers that process data. This architecture has been designed in order to improve the training issue, which is quite complicated when it comes to traditional deep learning models. Thanks to many layers, DSNs consider training, not a single problem that has to be solved but a set of individual problems. According to a paper An Evaluation of Deep Learning Miniature Concerning in Soft Computing8 published in 2015, the central idea of the DSN design relates to the concept of stacking, as proposed originally, where simple modules of functions or classifiers are composed first and then they are stacked on top of each other in order to learn complex functions or classifiers. Typically, DSNs consist of three or more modules. Each module consists of an input layer, a hidden layer, and an output layer. These modules are stacked one on top of another, which means that the input of a given module is based on the output of prior moduleslayers. This construction enables DSNs to learn more complex classification than it would be possible with just one module. These six architectures are the most common ones in the modern deep learning architecture world. At this point, we should also mention the last, and considered the most straightforward, architecture. Lets talk for a second about autoencoders. Transformer The Transformer is a powerful deep learning architecture that has significantly impacted the field of natural language processing NLP. It was first introduced in a 2017 paper by Google researchers and has since become a cornerstone in various advanced language models. Unlike traditional models that rely on Recurrent Neural Networks RNNs for sequential information extraction, Transformers leverage self-attention mechanisms to understand context and relationships between different elements in a sequence. Key points about the Transformer architecture include The Transformers ability to capture complex relationships in data, its parallel processing capabilities, and its impact on various NLP tasks make it a fundamental architecture in modern deep learning research, driving advancements in language understanding and generation. Generative Adversarial Networks GANs Generative Adversarial Networks GANs are a powerful class of deep learning models used for generative tasks, where they automatically learn and generate new data instances that resemble the original dataset. GANs consist of two primary components Key points about GANs include GANs have revolutionized generative modeling by enabling the creation of high-quality, realistic data that can be used in various domains such as image synthesis, content creation, and pattern recognition. Their ability to learn complex patterns and generate new data has made them a fundamental tool in the field of deep learning. Read more Deep Learning Applications What Is the Architecture of the Deep Learning Model? Deep learning models, including various architectures like recurrent neural networks RNN, convolutional neural networks CNN, and deep belief networks DBN, are structured in a specific manner to enable learning from complex data and making predictions or classifications. The architecture of a deep learning model typically consists of several interconnected layers, each serving a specific purpose in processing and transforming input data to generate useful outputs. Heres an overview of the architecture components Input Layer This is the initial layer of the deep learning model where raw data is fed into the network. The number of neurons in this layer corresponds to the dimensionality of the input data. Each neuron represents a feature or attribute of the input data. Hidden Layers These are intermediate layers between the input and output layers where the actual processing of data occurs. Each hidden layer comprises multiple neurons, and each neuron performs a weighted sum of inputs followed by the application of an activation function. The number of hidden layers and neurons in each layer can vary depending on the complexity of the problem and the architecture of the model. Output Layer This is the final layer of the model where the predictions or classifications are generated. The number of neurons in the output layer depends on the nature of the task. For instance, in a binary classification task, there might be one neuron representing each class with a sigmoid activation function to produce probabilities. In a multi-class classification task, there would be one neuron per class with a softmax activation function. Connections and Weights Each neuron in a layer is connected to every neuron in the subsequent layer, forming a fully connected or dense network. These connections have associated weights that are learned during the training process. The weights determine the strength of the connections between neurons and are adjusted iteratively to minimize the difference between the models predictions and the actual outputs. Activation Functions Activation functions introduce non-linearity into the model, enabling it to learn complex patterns and relationships in the data. Common activation functions include sigmoid, tanh, ReLU Rectified Linear Unit, and softmax. Loss Function The loss function measures the difference between the models predictions and the actual targets. It serves as the objective function during training, guiding the optimization process to minimize prediction errors. The choice of loss function depends on the nature of the task, such as mean squared error for regression tasks and categorical cross-entropy for classification tasks. Optimization Algorithm Optimization algorithms, such as stochastic gradient descent SGD, Adam, or RMSprop, are used to update the weights of the model iteratively based on the gradients of the loss function with respect to the weights. These algorithms aim to find the optimal set of weights that minimize the loss function. Overall, the architecture of a deep learning model is designed to efficiently process and learn from complex data, enabling it to make accurate predictions or classifications on unseen examples. The effectiveness of the architecture depends on various factors, including the choice of layers, activation functions, optimization algorithms, and hyperparameters, which are often determined through experimentation and tuning. Deep Learning Architecture  Autoencoders Autoencoders are a specific type of feedforward neural network. The general idea is that the input and the output are pretty much the same. What does it mean? Simply put, Autoencoders condense the input into a lower-dimensional code. Based on this, the outcome is produced. In this model, the code is a compact version of the input. One of Autoencoders main tasks is to identify and determine what constitutes regular data and then identify the anomalies or aberrations. Autoencoders comprise three components Autoencoders are mainly used for dimensionality reduction and, naturally, anomaly detection for instance, frauds. Simplicity is one of their greatest advantages. They are easy to build and train. However, theres also the other side of the coin. You need high-quality, representative training data. If you dont, the information that comes out of the Autoencoder can be unclear or biased. Deep Learning Architecture  conclusion As you can see, although deep learning architectures are, generally speaking, based on the same idea, there are various ways to achieve a goal. Thats why its so important to choose deep learning architecture correctly. If you want to find out more about this tremendous technology, get in touch with us. With our help, your organization can benefit from deep learning architecture. Let us show you how! This article is an updated version of the publication from Jul, 21 2020. References 1 Wikipedia. Recurrent neural network. URL httpsen.wikipedia.orgwikiRecurrent_neural_network. Accessed Jul 21, 2020. 2 Wikipedia. Bidirectional recurrent neural networks. URL httpsen.wikipedia.orgwikiBidirectional_recurrent_neural_networks. Accessed Jul 21, 2020. 3 Wikipedia. Long short-term memory. URL httpsen.wikipedia.orgwikiLong_short-term_memory. Accessed Jul 21, 2020. 4 Samaya Madhavan. Deep learning architectures. Jan 25, 2021. URL httpsdeveloper.ibm.comtechnologiesartificial-intelligencearticlescc-machine-learning-deep-learning-architectures. Accessed Jul 21, 2020. 5 Wikipedia. Gated recurrent unit. URL httpsen.wikipedia.orgwikiGated_recurrent_unit. Accessed Jul 21, 2020. 6 Sumit Saha. A Comprehensive Guide to Convolutional Neural Networks  the ELI5 way. Dec 15, 2018. URL httpstowardsdatascience.coma-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53. Accessed Jul 21, 2020. 7 Wikipedia. Deep belief network. URL httpsen.wikipedia.orgwikiDeep_belief_network. Accessed Jul 21, 2020. 8 Dr. Yusuf Perwej. An Evaluation of Deep Learning Miniature Concerning in Soft Computing. Feb 2015. URL httpswww.researchgate.netfigureA-Deep-Stacking-Network-Architecture_fig1_272885058. Accessed Jul 21, 2020. Category Machine Learning Consulting Sign Up for Newsletter See our Privacy Policy Related articles Case Study Addeptos AI Solutions for The Aviation Industry The collaboration between Addepto and a leading global aviation technology company exemplifies a successful long-term pa... Use Case Leveraging Gen AI and Machine Learning in Investment Management with BlackRocks Thematic Robot The private investment sector presents unique challenges that general-purpose generative AI solutions have struggled to ... Machine learning and Deep Learning in Economics Although machine learning ML continues to gain interest among economists, there is still a lack of practical informati... AI  ML Churn Prediction. Calculate Customer Churn Prediction ROI Many companies use statistical models to optimize their activities. An example of this type of models are scoring system... Addepto sp. z o.o. \u015awieradowska 47, 02-662 Warsaw, Poland AI-powered Knowledge Base Assistant for your business Document research, report generation, and code migration, is here to streamline and accelerate your entire knowledge base operations.", "metadata": {"length": 16135, "has_title": true, "domain": "addepto.com"}}
{"url": "https://en.wikipedia.org/wiki/Deep_learning", "title": "Deep learning - Wikipedia", "content": "Contents Deep learning Deep learning is a subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and training them to process data. The adjective deep refers to the use of multiple layers ranging from three to several hundred or thousands in the network. Methods used can be either supervised, semi-supervised or unsupervised.2 Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.345 Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.6 Overview Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.7 Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image represented as a tensor of pixels. The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.82 The word deep in deep learning refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path CAP depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one as the output layer is also parameterized. For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.9 No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.10 Beyond that, more layers do not add to the function approximator ability of the network. Deep models CAP  two are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method.11 Deep learning helps to disentangle these abstractions and pick out which features improve performance.8 Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.812 The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,13 and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.1415 Although the history of its appearance is apparently more complicated.16 Interpretations Deep neural networks are generally interpreted in terms of the universal approximation theorem1718192021 or probabilistic inference.22238924 The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.17181920 In 1989, the first proof was published by George Cybenko for sigmoid activation functions17 and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.18 Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushimas rectified linear unit.2526 The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.21 proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation24 derives from the field of machine learning. It features inference,237891224 as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.24 The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.27 History Before 1980 There are two types of artificial neural network ANN feedforward neural network FNN or multilayer perceptron MLP and recurrent neural networks RNN. RNNs have cycles in their connectivity structure, FNNs dont. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model2829 which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shunichi Amari made this architecture adaptive.3031 His learning RNN was republished by John Hopfield in 1982.32 Other early recurrent neural networks were published by Kaoru Nakano in 1971.3334 Already in 1948, Alan Turing produced work on Intelligent Machinery that was not published in his lifetime,35 containing ideas related to artificial evolution and learning RNNs.31 Frank Rosenblatt 195836 proposed the perceptron, an MLP with 3 layers an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons with adaptive preterminal networks where the last two layers have learned weights here he credits H. D. Block and B. W. Knight.37 section 16 The book cites an earlier network by R. D. Joseph 196038 functionally equivalent to a variation of this four-layer system the book mentions Joseph over 30 times. Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression,39 or a generalization of Rosenblatts perceptron.40 A 1971 paper described a deep network with eight layers trained by this method,41 which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or gates.31 The first deep learning multilayer perceptron trained by stochastic gradient descent42 was published in 1967 by Shunichi Amari.43 In computer experiments conducted by Amaris student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.31 Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU rectified linear unit activation function.2531 The rectifier has become the most popular activation function for deep learning.44 Deep learning architectures for convolutional neural networks CNNs with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.4546 Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 167347 to networks of differentiable nodes. The terminology back-propagating errors was actually introduced in 1962 by Rosenblatt,37 but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.48 The modern form of backpropagation was first published in Seppo Linnainmaas master thesis 1970.495031 G.M. Ostrovski et al. republished it in 1971.5152 Paul Werbos applied backpropagation to neural networks in 198253 his 1974 PhD thesis, reprinted in a 1994 book,54 did not yet describe the algorithm52. In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.5556 1980s-2000s The time delay neural network TDNN was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.5758 In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.59 In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.60 In 1990, Wei Zhang implemented a CNN on optical computing hardware.61 In 1991, a CNN was applied to medical image object segmentation62 and breast cancer detection in mammograms.63 LeNet-5 1998, a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.64 Recurrent neural networks RNN2830 were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network 198665 and the Elman network 1990,66 which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.6768 This neural history compressor uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.676831 In 1993, a neural history compressor solved a Very Deep Learning task that required more than 1000 subsequent layers in an RNN unfolded in time.69 The P in ChatGPT refers to such pre-training. Sepp Hochreiters diploma thesis 199170 implemented the neural history compressor,67 and identified and analyzed the vanishing gradient problem.7071 Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory LSTM, published in 1995.72 LSTM can learn very deep learning tasks9 with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a forget gate, introduced in 1999,73 which became the standard RNN architecture. In 1991, J\u00fcrgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one networks gain is the other networks loss.7475 The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called artificial curiosity. In 2014, this principle was used in generative adversarial networks GANs.76 During 19851995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,77 restricted Boltzmann machine,78 Helmholtz machine,79 and the wake-sleep algorithm.80 These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. p. 112 81. A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.82 Both shallow and deep learning e.g., recurrent nets of ANNs for speech recognition have been explored for many years.838485 These methods never outperformed non-uniform internal-handcrafting Gaussian mixture modelHidden Markov model GMM-HMM technology based on generative models of speech trained discriminatively.86 Key difficulties have been analyzed, including gradient diminishing70 and weak temporal correlation structure in neural predictive models.8788 Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US governments NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.8990 It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.91 The principle of elevating raw features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the raw spectrogram or linear filter-bank features in the late 1990s,90 showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.92 2000s Neural networks entered a null, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines SVMs became the preferred choices in the 1990s and 2000s, because of artificial neural networks computational cost and a lack of understanding of how the brain wires its biological networks.citation needed In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.93 In 2006, Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification CTC94 in stacks of LSTMs.95 In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.969 In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh9798 deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.99 They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.100101102 The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10 to 20 of all the checks written in the US, according to Yann LeCun.103 Industrial applications of deep learning to large-scale speech recognition started around 2010. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets DBN would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model GMMHidden Markov Model HMM and also than more-advanced generative model-based systems.104 The nature of the recognition errors produced by the two types of systems was characteristically different,105 offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.23106107 Analysis around 20092010, contrasting the GMM and other generative speech models vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.105 That analysis was done with comparable performance less than 1.5 in error rate between discriminative DNNs and generative models.104105108 In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.109110111106 Deep learning revolution The deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,112 including CNNs,113 faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.114 A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.112113 In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.115 In 2011, a CNN named DanNet116117 by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.9 It then won more contests.118119 They also showed how max-pooling CNNs on GPU improved performance significantly.3 In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.120 In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton4 won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman121 and Googles Inceptionv3.122 The success in image classification was then extended to the more challenging task of generating descriptions captions for images, often as a combination of CNNs and LSTMs.123124125 In 2014, the state of the art was training very deep neural network with 20 to 30 layers.126 Stacking too many layers led to a steep reduction in training accuracy,127 known as the degradation problem.128 In 2015, two techniques were developed to train very deep networks the Highway Network was published in May 2015, and the residual neural network ResNet129 in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream 2015, and neural style transfer 2015,130 both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network GAN by Ian Goodfellow et al., 2014131 based on J\u00fcrgen Schmidhubers principle of artificial curiosity7476 became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidias StyleGAN 2018132 based on the Progressive GAN by Tero Karras et al.133 Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.134 Diffusion models 2015135 eclipsed GANs in generative modeling since then, with systems such as DALLE 2 2022 and Stable Diffusion 2022. In 2015, Googles speech recognition improved by 49 by an LSTM-based model, which they made available through Google Voice Search on smartphone.136137 Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition ASR. Results on commonly used evaluation sets such as TIMIT ASR and MNIST image classification, as well as a range of large-vocabulary speech recognition tasks have steadily improved.104138 Convolutional neural networks were superseded for ASR by LSTM.137139140141 but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.142 Neural networks Artificial neural networks ANNs or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn progressively improve their ability to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as cat or no cat and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, analogous to biological neurons in a biological brain. Each connection synapse between neurons can transmit a signal to another neuron. The receiving postsynaptic neuron can process the signals and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first input, to the last output layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans e.g., recognizing faces, or playing Go144. Deep neural networks A deep neural network DNN is an artificial neural network with multiple layers between the input and output layers.79 There are different types of neural networks but they always consist of the same components neurons, synapses, weights, biases, and functions.145 These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.citation needed For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display above a certain threshold, etc. and return the proposed label. Each mathematical manipulation as such is considered a layer, 146 and complex DNN have many layers, hence the name deep networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.147 The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.7 For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.148 Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.146 DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or weights, to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.149 That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.150151152153154 Long short-term memory is particularly effective for this use.155156 Convolutional neural networks CNNs are used in computer vision.157 CNNs also have been applied to acoustic modeling for automatic speech recognition ASR.158 Challenges As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenkos unit pruning41 or weight decay  \u2113 2 displaystyle ell _2 -regularization or sparsity  \u2113 1 displaystyle ell _1 -regularization can be applied during training to combat overfitting.159 Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.160 Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.161 Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.162 DNNs must consider many training parameters, such as the size number of layers and number of units per layer, the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching computing the gradient on several training examples at once rather than individual examples163 speed up computation. Large processing capabilities of many-core architectures such as GPUs or the Intel Xeon Phi have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.164165 Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC cerebellar model articulation controller is one such kind of neural network. It doesnt require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.166167 Hardware Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.168 By 2019, graphics processing units GPUs, often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .169 OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet 2012 to AlphaZero 2017 and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.170171 Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units NPUs in Huawei cellphones172 and cloud computing servers such as tensor processing units TPU in the Google Cloud Platform.173 Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine WSE-2.174175 Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors FGFETs.176 In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.177 The authors identify two key advantages of integrated photonics over its electronic counterparts 1 massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and 2 extremely high data modulation speeds.177 Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.177 Applications Automatic speech recognition Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn Very Deep Learning tasks9 that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates156 is competitive with traditional speech recognizers on certain tasks.93 The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.178 Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates PER, have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 20032007, accelerated progress in eight major areas23108106 All major commercial speech recognition systems e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc. are based on deep learning.23183184 Image recognition A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.185 Deep learning-based image recognition has become superhuman, producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.186187 Deep learning-trained vehicles now interpret 360 camera views.188 Another example is Facial Dysmorphology Novel Analysis FDNA used to analyze cases of human malformation connected to a large database of genetic syndromes. Visual art processing Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of Natural language processing Neural networks have been used for implementing language models since the early 2000s.150 LSTM helped to improve machine translation and language modeling.151152153 Other key techniques in this field are negative sampling191 and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar PCFG implemented by an RNN.192 Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.192 Deep neural architectures provide the best results for constituency parsing,193 sentiment analysis,194 information retrieval,195196 spoken language understanding,197 machine translation,151198 contextual entity linking,198 writing style recognition,199 named-entity recognition token classification,200 text classification, and others.201 Recent developments generalize word embedding to sentence embedding. Google Translate GT uses a large end-to-end long short-term memory LSTM network.202203204205 Google Neural Machine Translation GNMT uses an example-based machine translation method in which the system learns from millions of examples.203 It translates whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.203 The network encodes the semantics of the sentence rather than simply memorizing phrase-to-phrase translations.203206 GT uses English as an intermediate between most language pairs.206 Drug discovery and toxicology A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy on-target effect, undesired interactions off-target effects, or unanticipated toxic effects.207208 Research has explored use of deep learning to predict the biomolecular targets,209210 off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.211212213 AtomNet is a deep learning system for structure-based rational drug design.214 AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus215 and multiple sclerosis.216215 In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.217 In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.218219 Customer relationship management Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.220 Recommendation systems Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.221222 Multi-view deep learning has been applied for learning user preferences from multiple domains.223 The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. Bioinformatics An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.224 In medical informatics, deep learning was used to predict sleep quality based on data from wearables225 and predictions of health complications from electronic health record data.226 Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.227228 Deep Neural Network Estimations Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator NJEE.229 Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.229 Medical image analysis Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.230231 Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.232233 Mobile advertising Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.234 Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the requestserveclick internet advertising cycle. This information can form the basis of machine learning to improve ad selection. Image restoration Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.235 These applications include learning methods such as Shrinkage Fields for Effective Image Restoration236 which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration. Financial fraud detection Deep learning is being successfully applied to financial fraud detection, tax evasion detection,237 and anti-money laundering.238 Materials science In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The systems predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.239240241 Military The United States Department of Defense applied deep learning to train robots in new tasks through observation.242 Partial differential equations Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.243 One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.244245 Deep backward stochastic differential equation method Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation BSDE. This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations PDEs, effectively reducing the computational burden.246 In addition, the integration of Physics-informed neural networks PINNs into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems. Image reconstruction Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging 247 and ultrasound imaging.248 Weather prediction Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.249250 Epigenetic clock An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using 6,000 blood samples.251 The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity. Relation to human cognitive and brain development Deep learning is closely related to a class of theories of brain development specifically, neocortical development proposed by cognitive neuroscientists in the early 1990s.252253254255 These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain e.g., a wave of nerve growth factor support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer or the operating environment, and then passes its output and possibly the original input, to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ...the infants brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.256 A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.257258 Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.259260 In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.261 Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons262 and neural populations.263 Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system264 both at the single-unit265 and at the population266 levels. Commercial activity Facebooks AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.267 Googles DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.268269270 Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.271 As of 2008,272 researchers at The University of Texas at Austin UT developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.242 First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory ARL and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.242 Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as good job and bad job.273 Criticism and comment Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory A main criticism concerns the lack of theory surrounding some methods.274 Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.citation needed e.g., Does it converge? If so, how fast? What is it approximating? Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.275 Others point out that deep learning should be looked at as a step towards realizing strong AIdisambiguation needed, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships ... have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson ... use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.276 In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep 20-30 layers neural networks attempting to discern within essentially random data the images on which they were trained277 demonstrate a visual appeal the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardians278 website. Errors Some deep learning architectures display problematic behaviors,279 such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images 2014280 and misclassifying minuscule perturbations of correctly classified images 2013.281 Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence AGI architectures.279 These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar282 decompositions of observed entities and events.279 Learning a grammar visual or linguistic from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition283 and artificial intelligence AI.284 Cyber threat As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.285 By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an adversarial attack.286 In 2016 researchers used one ANN to doctor images in trial and error fashion, identify anothers focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.287 One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.288 Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.287 ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.287 In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could serve as a stepping stone for further attacks e.g., opening a web page hosting drive-by malware.287 In data poisoning, false data is continually smuggled into a machine learning systems training set to prevent it from achieving mastery.287 Data collection ethics The deep learning systems that are trained using supervised learning often rely on data that is created andor annotated by humans.289 It has been argued that not only low-paid clickwork such as on Amazon Mechanical Turk is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.290 The philosopher Rainer M\u00fchlhoff distinguishes five types of machinic capture of human microwork to generate training data 1 gamification the embedding of annotation or computation tasks in the flow of a game, 2 trapping and tracking e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages, 3 exploitation of social motivations e.g. tagging faces on Facebook to obtain labeled facial images, 4 information mining e.g. by leveraging quantified-self devices such as activity trackers and 5 clickwork.290 See also References Further reading", "metadata": {"length": 56121, "has_title": true, "domain": "en.wikipedia.org"}}
{"url": "https://medium.com/dataseries/exploring-the-different-architectures-of-deep-learning-abc5eabafb8d", "title": "Exploring the Different Architectures of Deep Learning  by Albert Christopher  DataSeries  Medium", "content": "Sign up Sign in Sign up Sign in Exploring the Different Architectures of Deep Learning Albert Christopher Follow DataSeries -- Listen Share Deep learning has a spectrum of architectures capable of constructing solutions across various domains. Explore the most popular types of deep learning architecture. Deep learning algorithms span a diverse array of architectures, each capable of crafting solutions for a wide range of problem domains. Among these, long short-term memory LSTM and convolutional neural networks CNNs are not only some of the earliest but also the most widely applied approaches across various fields. In this article, we will guide you through the Nine Essential Deep Learning Architectures that every data scientist should know. Read on! Understanding Deep Learning Architectures Deep learning architectures have evolved significantly over time, driven by advancements in research, computational power, and data availability. They represent a pivotal advancement in artificial intelligence AI, leveraging inspiration from the neural networks of the human brain to equip machines with transformative capabilities. Deep learning architectures are complex, but understanding their key components can simplify the journey With these foundational concepts, we can delve into more complex deep learning architectures where multiple layers of neurons work together to solve intricate problems. Know the Different Deep Learning Architectures Lets explore some of the most common and influential models in the deep learning landscape, each with unique strengths and applications. RNNs are foundational deep-learning architectures designed to handle sequential data. They use their internal state memory to process variable-length sequences of inputs, making them ideal for tasks like speech recognition. RNNs are especially useful in fields where the sequence of information is crucial, such as natural language processing NLP, speech synthesis, and machine translation. Two common types of RNNs are 2. Convolutional Neural Networks CNNs Yann LeCun first created this multilayer neural network, drawing inspiration from the animal visual cortex. Initially, the architecture was designed for tasks like handwritten character recognition, such as interpreting postal codes. Today, Convolutional Neural Networks CNNs are essential for image processing and classification. Key layers include Additional elements like softmax activation for output normalization and dropout layers to prevent overfitting enhance CNN functionality. CNNs excel in applications like object detection, facial recognition, medical imaging, NLP, and more, making them vital across various industries. 3. Long Short-Term Memory LSTM Created in 1997 by Hochreiter and Schimdhuber, LSTM has grown in popularity in recent years as an RNN architecture for various applications. LSTMs are advanced RNNs designed to capture long-range dependencies in sequential data, addressing the limitations of traditional RNNs. Key features include 4. Gated Recurrent Unit GRU GRUs are a simplified version of LSTMs that enhance the modeling of long-term dependencies while being more computationally efficient. Key features include 5. ResNet Residual Networks ResNet is a deep learning architecture known for its ability to build very deep networks using residual modules. Key innovations include 6. Generative Adversarial Networks GANs GANs are powerful models for generative tasks, capable of learning and generating new data that resemble the original dataset. Key components include 7. Transformer Architecture Transformers are a revolutionary encoder-decoder architecture introduced by Vaswani et al. in 2017. Unlike traditional RNNs, Transformers use self-attention mechanisms to weigh the importance of different sequence elements, allowing for parallel computation and faster training. Applications include NLP Tasks Machine translation, sentiment analysis, and text summarization. Models like BERT leverage Transformers to improve contextual understanding by considering both the left and right context during pre-training. 8. Deep Belief Network DBN DBNs are multilayer networks composed of stacked Restricted Boltzmann Machines RBMs. Each pair of connected layers forms an RBM, with DBNs learning entire input probabilities through unsupervised learning. This holistic approach enables DBNs to excel in tasks like image recognition and NLP. 9. Deep Stacking Network DSN DSNs, also known as Deep Convex Networks DCN, consist of multiple individual deep networks, each with hidden layers. This architecture addresses the complexity of traditional deep-learning models by viewing training as a set of problems. DSNs improve training efficiency through modular stacking, where each module comprises an input layer, hidden layer, and output layer. Conclusion The impact of deep learning architectures is profound, particularly in fields such as image recognition and natural language processing, where they have revolutionized how machines interpret and interact with information. Their ability to extract meaningful features from data and continuously refine their understanding facilitates ongoing evolution in intelligent systems. As a result, deep learning algorithms promise a future where AI plays a pivotal role across diverse facets of human life, driving unprecedented innovation and progress. -- -- Published in DataSeries Imagine the future of data Written by Albert Christopher AI Researcher, Writer, Tech Geek. Contributing to Data Science  Deep Learning Projects. coding algorithms machinelearning No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams", "metadata": {"length": 5674, "has_title": true, "domain": "medium.com"}}
{"url": "https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414", "title": "Explained Neural networks  MIT News  Massachusetts Institute of Technology", "content": "Suggestions or feedback? MIT News  Massachusetts Institute of Technology Browse By Topics Departments Centers, Labs,  Programs Schools Breadcrumb Explained Neural networks Press Contact Media Download Terms of Use Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a Creative Commons Attribution Non-Commercial No Derivatives license. You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images if one is not provided below, credit the images to MIT. Previous image Next image In the past 10 years, the best-performing artificial-intelligence systems  such as the speech recognizers on smartphones or Googles latest automatic translator  have resulted from a technique called deep learning. Deep learning is in fact a new name for an approach to artificial intelligence called neural networks, which have been going in and out of fashion for more than 70 years. Neural networks were first proposed in 1944 by Warren McCullough and Walter Pitts, two University of Chicago researchers who moved to MIT in 1952 as founding members of whats sometimes called the first cognitive science department. Neural nets were a major area of research in both neuroscience and computer science until 1969, when, according to computer science lore, they were killed off by the MIT mathematicians Marvin Minsky and Seymour Papert, who a year later would become co-directors of the new MIT Artificial Intelligence Laboratory. The technique then enjoyed a resurgence in the 1980s, fell into eclipse again in the first decade of the new century, and has returned like gangbusters in the second, fueled largely by the increased processing power of graphics chips. Theres this idea that ideas in science are a bit like epidemics of viruses, says Tomaso Poggio, the Eugene McDermott Professor of Brain and Cognitive Sciences at MIT, an investigator at MITs McGovern Institute for Brain Research, and director of MITs Center for Brains, Minds, and Machines. There are apparently five or six basic strains of flu viruses, and apparently each one comes back with a period of around 25 years. People get infected, and they develop an immune response, and so they dont get infected for the next 25 years. And then there is a new generation that is ready to be infected by the same strain of virus. In science, people fall in love with an idea, get excited about it, hammer it to death, and then get immunized  they get tired of it. So ideas should have the same kind of periodicity! Weighty matters Neural nets are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples. Usually, the examples have been hand-labeled in advance. An object recognition system, for instance, might be fed thousands of labeled images of cars, houses, coffee cups, and so on, and it would find visual patterns in the images that consistently correlate with particular labels. Modeled loosely on the human brain, a neural net consists of thousands or even millions of simple processing nodes that are densely interconnected. Most of todays neural nets are organized into layers of nodes, and theyre feed-forward, meaning that data moves through them in only one direction. An individual node might be connected to several nodes in the layer beneath it, from which it receives data, and several nodes in the layer above it, to which it sends data. To each of its incoming connections, a node will assign a number known as a weight. When the network is active, the node receives a different data item  a different number  over each of its connections and multiplies it by the associated weight. It then adds the resulting products together, yielding a single number. If that number is below a threshold value, the node passes no data to the next layer. If the number exceeds the threshold value, the node fires, which in todays neural nets generally means sending the number  the sum of the weighted inputs  along all its outgoing connections. When a neural net is being trained, all of its weights and thresholds are initially set to random values. Training data is fed to the bottom layer  the input layer  and it passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer. During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs. Minds and machines The neural nets described by McCullough and Pitts in 1944 had thresholds and weights, but they werent arranged into layers, and the researchers didnt specify any training mechanism. What McCullough and Pitts showed was that a neural net could, in principle, compute any function that a digital computer could. The result was more neuroscience than computer science The point was to suggest that the human brain could be thought of as a computing device. Neural nets continue to be a valuable tool for neuroscientific research. For instance, particular network layouts or rules for adjusting weights and thresholds have reproduced observed features of human neuroanatomy and cognition, an indication that they capture something about how the brain processes information. The first trainable neural network, the Perceptron, was demonstrated by the Cornell University psychologist Frank Rosenblatt in 1957. The Perceptrons design was much like that of the modern neural net, except that it had only one layer with adjustable weights and thresholds, sandwiched between input and output layers. Perceptrons were an active area of research in both psychology and the fledgling discipline of computer science until 1959, when Minsky and Papert published a book titled Perceptrons, which demonstrated that executing certain fairly common computations on Perceptrons would be impractically time consuming. Of course, all of these limitations kind of disappear if you take machinery that is a little more complicated  like, two layers, Poggio says. But at the time, the book had a chilling effect on neural-net research. You have to put these things in historical context, Poggio says. They were arguing for programming  for languages like Lisp. Not many years before, people were still using analog computers. It was not clear at all at the time that programming was the way to go. I think they went a little bit overboard, but as usual, its not black and white. If you think of this as this competition between analog computing and digital computing, they fought for what at the time was the right thing. Periodicity By the 1980s, however, researchers had developed algorithms for modifying neural nets weights and thresholds that were efficient enough for networks with more than one layer, removing many of the limitations identified by Minsky and Papert. The field enjoyed a renaissance. But intellectually, theres something unsatisfying about neural nets. Enough training may revise a networks settings to the point that it can usefully classify data, but what do those settings mean? What image features is an object recognizer looking at, and how does it piece them together into the distinctive visual signatures of cars, houses, and coffee cups? Looking at the weights of individual connections wont answer that question. In recent years, computer scientists have begun to come up with ingenious methods for deducing the analytic strategies adopted by neural nets. But in the 1980s, the networks strategies were indecipherable. So around the turn of the century, neural networks were supplanted by support vector machines, an alternative approach to machine learning thats based on some very clean and elegant mathematics. The recent resurgence in neural networks  the deep-learning revolution  comes courtesy of the computer-game industry. The complex imagery and rapid pace of todays video games require hardware that can keep up, and the result has been the graphics processing unit GPU, which packs thousands of relatively simple processing cores on a single chip. It didnt take long for researchers to realize that the architecture of a GPU is remarkably like that of a neural net. Modern GPUs enabled the one-layer networks of the 1960s and the two- to three-layer networks of the 1980s to blossom into the 10-, 15-, even 50-layer networks of today. Thats what the deep in deep learning refers to  the depth of the networks layers. And currently, deep learning is responsible for the best-performing systems in almost every area of artificial-intelligence research. Under the hood The networks opacity is still unsettling to theorists, but theres headway on that front, too. In addition to directing the Center for Brains, Minds, and Machines CBMM, Poggio leads the centers research program in Theoretical Frameworks for Intelligence. Recently, Poggio and his CBMM colleagues have released a three-part theoretical study of neural networks. The first part, which was published last month in the International Journal of Automation and Computing, addresses the range of computations that deep-learning networks can execute and when deep networks offer advantages over shallower ones. Parts two and three, which have been released as CBMM technical reports, address the problems of global optimization, or guaranteeing that a network has found the settings that best accord with its training data, and overfitting, or cases in which the network becomes so attuned to the specifics of its training data that it fails to generalize to other instances of the same categories. There are still plenty of theoretical questions to be answered, but CBMM researchers work could help ensure that neural networks finally break the generational cycle that has brought them in and out of favor for seven decades. Share this news article on Related Links Related Topics Related Articles Voice control everywhere Model sheds light on purpose of inhibitory neurons Learning words from pictures Computer learns to recognize sounds by watching video Previous item Next item More MIT News Creating innovative health solutions for individuals and populations Read full story  MITs Science Policy Initiative holds 14th annual Executive Visit Days Read full story  Troy Van Voorhis to step down as department head of chemistry Read full story  Is there enough land on Earth to fight climate change and feed the world? Read full story  Decarbonizing heavy industry with thermal batteries Read full story  The MIT Press releases report on the future of open access publishing and policy Read full story  More about MIT News at Massachusetts Institute of Technology This website is managed by the MIT News Office, part of the Institute Office of Communications. News by SchoolsCollege Resources Tools Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA", "metadata": {"length": 10954, "has_title": true, "domain": "news.mit.edu"}}
{"url": "https://aws.amazon.com/what-is/neural-network/", "title": "What is a Neural Network? - Artificial Neural Network Explained - AWS", "content": "What is a Neural Network? What is a neural network? A neural network is a method in artificial intelligence AI that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning ML process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously. Thus, artificial neural networks attempt to solve complicated problems, like summarizing documents or recognizing faces, with greater accuracy. Why are neural networks important? Neural networks can help computers make intelligent decisions with limited human assistance. This is because they can learn and model the relationships between input and output data that are nonlinear and complex. For instance, they can do the following tasks. Make generalizations and inferences Neural networks can comprehend unstructured data and make general observations without explicit training. For instance, they can recognize that two different input sentences have a similar meaning A neural network would know that both sentences mean the same thing. Or it would be able to broadly recognize that Baxter Road is a place, but Baxter Smith is a persons name. What are neural networks used for? Neural networks have several use cases across many industries, such as the following We give four of the important applications of neural networks below. Computer vision Computer vision is the ability of computers to extract information and insights from images and videos. With neural networks, computers can distinguish and recognize images similar to humans. Computer vision has several applications, such as the following Speech recognition Neural networks can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Virtual assistants like Amazon Alexa and automatic transcription software use speech recognition to do tasks like these Natural language processing Natural language processing NLP is the ability to process natural, human-created text. Neural networks help computers gather insights and meaning from text data and documents. NLP has several use cases, including in these functions Recommendation engines Neural networks can track user activity to develop personalized recommendations. They can also analyze all user behavior and discover new products or services that interest a specific user. For example, Curalate, a Philadelphia-based startup, helps brands convert social media posts into sales. Brands use Curalates intelligent product tagging IPT service to automate the collection and curation of user-generated social content. IPT uses neural networks to automatically find and recommend products relevant to the users social media activity. Consumers dont have to hunt through online catalogs to find a specific product from a social media image. Instead, they can use Curalates auto product tagging to purchase the product with ease. How do neural networks work? The human brain is the inspiration behind neural network architecture. Human brain cells, called neurons, form a complex, highly interconnected network and send electrical signals to each other to help humans process information. Similarly, an artificial neural network is made of artificial neurons that work together to solve a problem. Artificial neurons are software modules, called nodes, and artificial neural networks are software programs or algorithms that, at their core, use computing systems to solve mathematical calculations. Simple neural network architecture A basic neural network has interconnected artificial neurons in three layers Input Layer Information from the outside world enters the artificial neural network from the input layer. Input nodes process the data, analyze or categorize it, and pass it on to the next layer. Hidden Layer Hidden layers take their input from the input layer or other hidden layers. Artificial neural networks can have a large number of hidden layers. Each hidden layer analyzes the output from the previous layer, processes it further, and passes it on to the next layer. Output Layer The output layer gives the final result of all the data processing by the artificial neural network. It can have single or multiple nodes. For instance, if we have a binary yesno classification problem, the output layer will have one output node, which will give the result as 1 or 0. However, if we have a multi-class classification problem, the output layer might consist of more than one output node. Deep neural network architecture Deep neural networks, or deep learning networks, have several hidden layers with millions of artificial neurons linked together. A number, called weight, represents the connections between one node and another. The weight is a positive number if one node excites another, or negative if one node suppresses the other. Nodes with higher weight values have more influence on the other nodes. Theoretically, deep neural networks can map any input type to any output type. However, they also need much more training as compared to other machine learning methods. They need millions of examples of training data rather than perhaps the hundreds or thousands that a simpler network might need. What are the types of neural networks? Artificial neural networks can be categorized by how the data flows from the input node to the output node. Below are some examples Feedforward neural networks Feedforward neural networks process data in one direction, from the input node to the output node. Every node in one layer is connected to every node in the next layer. A feedforward network uses a feedback process to improve predictions over time. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth. How to train neural networks? Neural network training is the process of teaching a neural network to perform a task. Neural networks learn by initially processing several large sets of labeled or unlabeled data. By using these examples, they can then process unknown inputs more accurately. Supervised learning In supervised learning, data scientists give artificial neural networks labeled datasets that provide the right answer in advance. For example, a deep learning network training in facial recognition initially processes hundreds of thousands of images of human faces, with various terms related to ethnic origin, country, or emotion describing each image. The neural network slowly builds knowledge from these datasets, which provide the right answer in advance. After the network has been trained, it starts making guesses about the ethnic origin or emotion of a new image of a human face that it has never processed before. What is deep learning in the context of neural networks? Artificial intelligence is the field of computer science that researches methods of giving machines the ability to perform tasks that require human intelligence. Machine learning is an artificial intelligence technique that gives computers access to very large datasets and teaches them to learn from this data. Machine learning software finds patterns in existing data and applies those patterns to new data to make intelligent decisions. Deep learning is a subset of machine learning that uses deep learning networks to process data. Machine learning vs. deep learning Traditional machine learning methods require human input for the machine learning software to work sufficiently well. A data scientist manually determines the set of relevant features that the software must analyze. This limits the softwares ability, which makes it tedious to create and manage. On the other hand, in deep learning, the data scientist gives only raw data to the software. The deep learning network derives the features by itself and learns more independently. It can analyze unstructured datasets like text documents, identify which data attributes to prioritize, and solve more complex problems. For example, if you were training a machine learning software to identify an image of a pet correctly, you would need to take these steps What are deep learning services on AWS? AWS deep learning services harness the power of cloud computing so that you can scale your deep learning neural networks at a lower cost and optimize them for speed. You can also use AWS services like these to fully manage specific deep learning applications Get started with deep learning neural networks on AWS with Amazon SageMaker and quickly and easily build, train, and deploy models at scale. You can also use the AWS Deep Learning AMIs to build custom environments and workflows for deep learning. Create a free AWS account to get started today! Next steps on AWS Learn About AWS Resources for AWS Developers on AWS Help Ending Support for Internet Explorer", "metadata": {"length": 9799, "has_title": true, "domain": "aws.amazon.com"}}
{"url": "https://www.ibm.com/topics/neural-networks", "title": "What is a Neural Network?  IBM", "content": "A neural network is a machine learning program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions. Every neural network consists of layers of nodes, or artificial neuronsan input layer, one or more hidden layers, and an output layer. Each node connects to others, and has its own associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network. Neural networks rely on training data to learn and improve their accuracy over time. Once they are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the best-known examples of a neural network is Googles search algorithm. Neural networks are sometimes called artificial neural networks ANNs or simulated neural networks SNNs. They are a subset of machine learning, and at the heart of deep learning models. Learn how to choose the right approach in preparing data sets and employing foundation models. Register for the ebook on generative AI Think of each individual node as its own linear regression model, composed of input data, weights, a bias or threshold, and an output. The formula would look something like this wixi  bias  w1x1  w2x2  w3x3  bias output  fx  1 if w1x1  b 0 0 if w1x1  b  0 Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it fires or activates the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network. Lets break down what one single node might look like using binary values. We can apply this concept to a more tangible example, like whether you should go surfing Yes 1, No 0. The decision to go or not to go is our predicted outcome, or y-hat. Lets assume that there are three factors influencing your decision-making Then, lets assume the following, giving us the following inputs Now, we need to assign some weights to determine importance. Larger weights signify that particular variables are of greater importance to the decision or outcome. Finally, well also assume a threshold value of 3, which would translate to a bias value of 3. With all the various inputs, we can start to plug in values into the formula to get the desired output. Y-hat  15  02  14  3  6 If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers. In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network. As we start to think about more practical use cases for neural networks, like image recognition or classification, well leverage supervised learning, or labeled datasets, to train the algorithm. As we train the model, well want to evaluate its accuracy using a cost or loss function. This is also commonly referred to as the mean squared error MSE. In the equation below, \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc39\ud835\udc62\ud835\udc5b\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc40\ud835\udc46\ud835\udc3812\ud835\udc5a 129_\ud835\udc561\ud835\udc5a\ud835\udc66 \ud835\udc56 \ud835\udc66\ud835\udc56  2 Ultimately, the goal is to minimize our cost function to ensure correctness of fit for any given observation. As the model adjusts its weights and bias, it uses the cost function and reinforcement learning to reach the point of convergence, or the local minimum. The process in which the algorithm adjusts its weights is through gradient descent, allowing the model to determine the direction to take to reduce errors or minimize the cost function. With each training example, the parameters of the model adjust to gradually converge at the minimum. See this IBM Developer article for a deeper explanation of the quantitative concepts involved in neural networks. Most deep neural networks are feedforward, meaning they flow in one direction only, from input to output. However, you can also train your model through backpropagation that is, move in the opposite direction from output to input. Backpropagation allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the parameters of the models appropriately. The all new enterprise studio that brings together traditional machine learning along with new generative AI capabilities powered by foundation models. Neural networks can be classified into different types, which are used for different purposes. While this isnt a comprehensive list of types, the below would be representative of the most common types of neural networks that youll come across for its common use cases The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958. Feedforward neural networks, or multi-layer perceptrons MLPs, are what weve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, its important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks. Convolutional neural networks CNNs are similar to feedforward networks, but theyre usually utilized for image recognition, pattern recognition, andor computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image. Recurrent neural networks RNNs are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting. Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, its worth noting that the deep in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layerswhich would be inclusive of the inputs and the outputcan be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network. To learn more about the differences between neural networks and other forms of artificial intelligence, like machine learning, please read the blog post AI vs. Machine Learning vs. Deep Learning vs. Neural Networks Whats the Difference? The history of neural networks is longer than most people think. While the idea of a machine that thinks can be traced to the Ancient Greeks, well focus on the key events that led to the evolution of thinking around neural networks, which has ebbed and flowed in popularity over the years 1943 Warren S. McCulloch and Walter Pitts published A logical calculus of the ideas immanent in nervous activity link resides outside ibm.com This research sought to understand how the human brain could produce complex patterns through connected brain cells, or neurons. One of the main ideas that came out of this work was the comparison of neurons with a binary threshold to Boolean logic i.e., 01 or truefalse statements. 1958 Frank Rosenblatt is credited with the development of the perceptron, documented in his research, The Perceptron A Probabilistic Model for Information Storage and Organization in the Brain link resides outside ibm.com. He takes McCulloch and Pitts work a step further by introducing weights to the equation. Leveraging an IBM 704, Rosenblatt was able to get a computer to learn how to distinguish cards marked on the left vs. cards marked on the right. 1974 While numerous researchers contributed to the idea of backpropagation, Paul Werbos was the first person in the US to note its application within neural networks within his PhD thesis link resides outside ibm.com. 1989 Yann LeCun published a paper link resides outside ibm.com illustrating how the use of constraints in backpropagation and its integration into the neural network architecture can be used to train algorithms. This research successfully leveraged a neural network to recognize hand-written zip code digits provided by the U.S. Postal Service. Design complex neural networks. Experiment at scale to deploy optimized learning models within IBM Watson Studio. Build and scale trusted AI on any cloud. Automate the AI lifecycle for ModelOps. Take the next step to start operationalizing and scaling generative AI and machine learning for business. Granite is IBMs flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. We surveyed 2,000 organizations about their AI initiatives to discover whats working, whats not and how you can get ahead. Learn the building blocks and best practices to help your teams accelerate responsible AI. These terms are often used interchangeably, but what differences make each a unique technology? Register for our ebook for insights into the opportunities, challenges and lessons learned from infusing AI into businesses. Get an in-depth understanding of neural networks, their basic functions and the fundamentals of building one. Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.", "metadata": {"length": 11003, "has_title": true, "domain": "ibm.com"}}
{"url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)", "title": "Neural network machine learning - Wikipedia", "content": "Contents Neural network machine learning In machine learning, a neural network also artificial neural network or neural net, abbreviated ANN or NN is a model inspired by the structure and function of biological neural networks in animal brains.12 An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The signal is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer the input layer to the last layer the output layer, possibly passing through multiple intermediate layers hidden layers. A network is typically called a deep neural network if it has at least two hidden layers.3 Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. Training Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the networks parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.4 Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.4 During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.5 This method allows the network to generalize to unseen data. History Early work Todays deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network FNN is a linear network, which consists of a single layer of output nodes with linear activation functions the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre 1805 and Gauss 1795 for the prediction of planetary movement.7891011 Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts12 1943 considered a non-learning computational model for neural networks.13 This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb14 proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatts perceptron and the Hopfield network. Farley and Clark15 1954 used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda 1956.16 In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks,17181920 funded by the United States Office of Naval Research.21 R. D. Joseph 196022 mentions an even earlier perceptron-like device by Farley and Clark10 Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device. However, they dropped the subject. The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to the Golden Age of AI fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.23 The first perceptrons did not have adaptive hidden units. However, Joseph 196022 also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt 196224 section 16 cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. Deep learning breakthroughs in the 1960s and 1970s Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in Ukraine 1965. They regarded it as a form of polynomial regression,25 or a generalization of Rosenblatts perceptron.26 A 1971 paper described a deep network with eight layers trained by this method,27 which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or gates.10 The first deep learning multilayer perceptron trained by stochastic gradient descent28 was published in 1967 by Shunichi Amari.29 In computer experiments conducted by Amaris student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.10 Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU rectified linear unit activation function.103031 The rectifier has become the most popular activation function for deep learning.32 Nevertheless, research stagnated in the United States following the work of Minsky and Papert 1969,33 who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko 1965 and Amari 1967. In 1976 transfer learning was introduced in neural networks learning. 34 35 Deep learning architectures for convolutional neural networks CNNs with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.363738 Backpropagation Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 167339 to networks of differentiable nodes. The terminology back-propagating errors was actually introduced in 1962 by Rosenblatt,24 but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.40 In 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis 1970.414210 G.M. Ostrovski et al. republished it in 1971.4344 Paul Werbos applied backpropagation to neural networks in 19824546 his 1974 PhD thesis, reprinted in a 1994 book,47 did not yet describe the algorithm44. In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.48 Convolutional neural networks Kunihiko Fukushimas convolutional neural network CNN architecture of 197936 also introduced max pooling,49 a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision. The time delay neural network TDNN was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.5051 In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.52 In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.53 In 1990, Wei Zhang implemented a CNN on optical computing hardware.54 In 1991, a CNN was applied to medical image object segmentation55 and breast cancer detection in mammograms.56 LeNet-5 1998, a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 3232 pixel images.57 From 1988 onward,5859 the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles matrices produced by multiple sequence alignments.60 Recurrent neural networks One origin of RNN was statistical mechanics. In 1972, Shunichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.61 This was popularized as the Hopfield network by John Hopfield1982.62 Another origin of RNN was neuroscience. The word recurrent is used to describe loop-like structures in anatomy. In 1901, Cajal observed recurrent semicircles in the cerebellar cortex.63 Hebb considered reverberating circuit as an explanation for short-term memory.64 The McCulloch and Pitts paper 1943 considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.12 In 1982 a recurrent neural network, with an array architecture rather than a multilayer perceptron architecture, named Crossbar Adaptive Array 6566 used direct recurrent connections from the output to the supervisor teaching  inputs. In addition of computing actions decisions, it computed internal state evaluations emotions of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks. In cognitive psychology, the journal American Psychologist in early 1980s carried out a debate on relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. 6768 In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. 6569 It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology. Two early influential works were the Jordan network 1986 and the Elman network 1990, which applied RNN to study cognitive psychology. In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed the neural sequence chunker or neural history compressor7071 which introduced the important concepts of self-supervised pre-training the P in ChatGPT and neural knowledge distillation.10 In 1993, a neural history compressor system solved a Very Deep Learning task that required more than 1000 subsequent layers in an RNN unfolded in time.72 In 1991, Sepp Hochreiters diploma thesis 73 identified and analyzed the vanishing gradient problem7374 and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory LSTM, which set accuracy records in multiple applications domains.7576 This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.77 It became the default choice for RNN architecture. During 19851995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,78 restricted Boltzmann machine,79 Helmholtz machine,80 and the wake-sleep algorithm.81 These were designed for unsupervised learning of deep generative models. Deep learning Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.8283 In 2011, a CNN named DanNet8485 by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.38 It then won more contests.8687 They also showed how max-pooling CNNs on GPU improved performance significantly.88 In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton89 won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman90 and Googles Inceptionv3.91 In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.92 Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as deep learning.5 Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.93 Generative adversarial network GAN Ian Goodfellow et al., 201494 became state of the art in generative modeling during 20142018 period. The GAN principle was originally published in 1991 by J\u00fcrgen Schmidhuber who called it artificial curiosity two neural networks contest with each other in the form of a zero-sum game, where one networks gain is the other networks loss.9596 The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidias StyleGAN 201897 based on the Progressive GAN by Tero Karras et al.98 Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.99 Diffusion models 2015100 eclipsed GANs in generative modeling since then, with systems such as DALLE 2 2022 and Stable Diffusion 2022. In 2014, the state of the art was training very deep neural network with 20 to 30 layers.101 Stacking too many layers led to a steep reduction in training accuracy,102 known as the degradation problem.103 In 2015, two techniques were developed to train very deep networks the highway network was published in May 2015,104 and the residual neural network ResNet in December 2015.105106 ResNet behaves like an open-gated Highway Net. During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.107 It requires computation time that is quadratic in the size of the context window. J\u00fcrgen Schmidhubers fast weight controller 1992108 scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.10911010 Transformers have increasingly become the model of choice for natural language processing.111 Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture. Models ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.112 An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one nodes influence on another,113 allowing weights to choose the signal between neurons. Artificial neurons ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.114 The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.citation needed To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum.115 This weighted sum is sometimes called the activation. This weighted sum is then passed through a usually nonlinear activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.116 Organization The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be fully connected, with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.117 Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.118 Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.119 Hyperparameter A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size.citation needed The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.citation needed Learning Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights and optional thresholds of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output almost certainly a cat and the correct answer cat is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.112120 Learning rate The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.121 A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.122 The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.citation needed Cost function While it is possible to define a cost function ad hoc, frequently the choice is determined by the functions desirable properties such as convexity or because it arises from the model e.g. in a probabilistic model the models posterior probability can be used as an inverse cost.citation needed Backpropagation Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient the derivative of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines,123 no-prop networks,124 training without backtracking,125 weightless networks,126127 and non-connectionist neural networks.citation needed Learning paradigms Machine learning is commonly separated into three main learning paradigms, supervised learning,128 unsupervised learning129 and reinforcement learning.130 Each corresponds to a particular learning task. Supervised learning Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions.131 A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the networks output and the desired output. Tasks suited for supervised learning are pattern recognition also known as classification and regression also known as function approximation. Supervised learning is also applicable to sequential data e.g., for handwriting, speech and gesture recognition. This can be thought of as learning with a teacher, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. Unsupervised learning In unsupervised learning, input data is given along with the cost function, some function of the data x displaystyle textstyle x and the networks output. The cost function is dependent on the task the model domain and any a priori assumptions the implicit properties of the model, its parameters and the observed variables. As a trivial example, consider the model f  x   a displaystyle textstyle fxa where a displaystyle textstyle a is a constant and the cost C  E   x  f  x   2  displaystyle textstyle CEx-fx2 . Minimizing this cost produces a value of a displaystyle textstyle a that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application for example, in compression it could be related to the mutual information between x displaystyle textstyle x and f  x  displaystyle textstyle fx , whereas in statistical modeling, it could be related to the posterior probability of the model given the data note that in both of those examples, those quantities would be maximized rather than minimized. Tasks that fall within the paradigm of unsupervised learning are in general estimation problems the applications include clustering, the estimation of statistical distributions, compression and filtering. Reinforcement learning In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive lowest cost responses. In reinforcement learning, the aim is to weight the network devise a policy to perform actions that minimize long-term expected cumulative cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some usually unknown rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally the environment is modeled as a Markov decision process MDP with states s 1 , . . . , s n  S displaystyle textstyle s_1,...,s_nin S and actions a 1 , . . . , a m  A displaystyle textstyle a_1,...,a_min A . Because the state transitions are not known, probability distributions are used instead the instantaneous cost distribution P  c t  s t  displaystyle textstyle Pc_ts_t , the observation distribution P  x t  s t  displaystyle textstyle Px_ts_t and the transition distribution P  s t  1  s t , a t  displaystyle textstyle Ps_t1s_t,a_t , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain MC. The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications.132133 Dynamic programming coupled with ANNs giving neurodynamic programming134 has been applied to problems such as those involved in vehicle routing,135 video games, natural resource management136137 and medicine138 because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. Self-learning Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array CAA.139 It is a system with only one input, situation s, and only one output, action or behavior a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions feelings about encountered situations. The system is driven by the interaction between cognition and emotion.140 Given the memory matrix, W wa,s, the crossbar self-learning algorithm in each iteration performs the following computation The backpropagated value secondary reinforcement is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector species vector from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.141 Neuroevolution Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches.142143 One advantage of neuroevolution is that it may be less prone to get caught in dead ends.144 Stochastic neural network Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the networks artificial neurons stochastic transfer functions citation needed, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.145 Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.146 Other In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods,147 gene expression programming,148 simulated annealing,149 expectationmaximization, non-parametric methods and particle swarm optimization150 are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller CMAC neural networks.151152 Modes Two modes of learning are available stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces noise into the process, using the local gradient calculated from one data point this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batchs average error. A common compromise is to use mini-batches, small batches with samples in each batch selected stochastically from the entire data set. Types ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allowrequire learning to be supervised by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers. Some of the main breakthroughs include Network design Using artificial neural networks requires an understanding of their characteristics. Neural architecture search NAS uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.164 Available systems include AutoML and AutoKeras.165 scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras. Hyperparameters must also be defined as part of the design they are not learned, governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding for CNNs, etc.166 citation needed Applications Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include ANNs have been used to diagnose several types of cancers184185 and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.186187 ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters188189 and to predict foundation settlements.190 It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.191 ANNs have also been used for building black-box models in geoscience hydrology,192193 ocean modelling and coastal engineering,194195 and geomorphology.196 ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware,197 for identifying domains belonging to threat actors and for detecting URLs posing a security risk.198 Research is underway on ANN systems designed for penetration testing, for detecting botnets,199 credit cards frauds200 and network intrusions. ANNs have been proposed as a tool to solve partial differential equations in physics201202203 and simulate the properties of many-body open quantum systems.204205206207 In brain research ANNs have studied short-term behavior of individual neurons,208 the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level. It is possible to create a profile of a users interests from pictures, using artificial neural networks trained for object recognition.209 Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks GNNs have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.210 Theoretical properties Computational power The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters. A specific recurrent architecture with rational-valued weights as opposed to full precision real number-valued weights has the power of a universal Turing machine,211 using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.212213failed verification Capacity A models capacity property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity. Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKays book214 which summarizes work by Thomas Cover.215 The capacity of a network of standard neurons not convolutional can be derived by four rules216 that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in,214 the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.217 Convergence Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. Another issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction. The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.218219 Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks.220221222223 This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.224 Generalization and statistics Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic Bayesian framework, where regularization can be performed by selecting a larger prior probability over simpler models but also in statistical learning theory, where the goal is to minimize over two quantities the empirical risk and the structural risk, which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. Supervised neural networks that use a mean squared error MSE cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network or a softmax component in a component-based network for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is Criticism Training A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.225 Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches andor introducing a recursive least squares algorithm for CMAC.151 Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads single lane, multi-lane, dirt, etc., and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained if, for example, it is presented with a series of right turnsit should not learn to always turn right.226 Theory A central claimcitation needed of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimedby whom? that they are emergent from the network itself. This allows simple statistical association the basic function of artificial neural networks to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand or mind intervenes solutions are found as if by magic and no one, it seems, has learned anything.227 One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft228 to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, what hasnt? but also because you could create a successful net without understanding how it worked the bunch of numbers that captures its behaviour would in all probability be an opaque, unreadable table...valueless as a scientific resource. In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.229 Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun 2007 wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.230 Biological brains use both shallow and deep circuits as reported by brain anatomy,231 displaying a wide variety of invariance. Weng232 argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies. Hardware Large and effective neural networks require considerable computing resources.233 While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons  which require enormous CPU power and time.citation needed Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware from 1991 to 2015, computing power, especially as delivered by GPGPUs on GPUs, has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.38 The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.233234 Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.235 Practical counterexamples Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.236 Hybrid approaches Advocates of hybrid models combining neural networks and symbolic approaches say that such a mixture can better capture the mechanisms of the human mind.237238 Dataset bias Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.239240 These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.239 This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.240241 For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.241 The program would penalize any resume with the word woman or the name of any womens college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.242 Gallery Recent advancements and future directions Artificial neural networks ANNs have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.citation needed Image processing In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks CNNs have been important in handwritten digit recognition, achieving state-of-the-art performance.243 This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.243 Speech recognition By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.243244 These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.citation needed Natural language processing In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.243244 This has implications for automated customer service, content moderation, and language understanding technologies.citation needed Control systems In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.citation needed Finance ANNs are used for stock market prediction and credit scoring ANNs require high-quality data and careful tuning, and their black-box nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.citation needed Medicine ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.244 In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.243 Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.244 Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.citation needed Content creation ANNs such as generative adversarial networks GAN and transformers are used for content creation across numerous industries.245 This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.246 In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.247 In the marketing industry generative models are used to create personalized advertisements for consumers.245 Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.248 Furthermore, neural networks have found uses in video game creation, where Non Player Characters NPCs can make decisions based on all the characters currently in the game.249 See also References Bibliography External links", "metadata": {"length": 50183, "has_title": true, "domain": "en.wikipedia.org"}}
{"url": "https://medium.com/@sadafsaleem5815/neural-networks-in-10mins-simply-explained-9ec2ad9ea815", "title": "Neural Networks in 10mins. Simply Explained!  by Sadaf Saleem  Medium", "content": "Sign up Sign in Sign up Sign in Neural Networks in 10mins. Simply Explained! Sadaf Saleem Follow -- 5 Listen Share What are Neural Networks? Neural networks are the fundamental building blocks of deep learning algorithms. A neural network is a type of machine learning algorithm that is designed to simulate the behavior of the human brain. It is made up of interconnected nodes, also known as artificial neurons, which are organized into layers. How Neural Network differs from Machine Learning? Neural networks are a type of machine learning algorithm, but they differ from traditional machine learning in several key ways. Most importantly, neural networks learn and improve on their own, without human intervention. It learns features directly from the data, making it better suited for large datasets. However, in traditional machine learning, features are manually provided. Why to use Deep Learning? One of the key benefits of deep learning is its ability to handle big data. As the volume of data increases, traditional machine learning techniques can become inefficient in terms of performance and accuracy. Deep learning, on the other hand, continues to perform well, making it an ideal choice for data-heavy applications. Its a valid question to ask why we should bother studying the structure of deep learning, rather than simply relying on the computer to generate outputs for us. However, there are many compelling reasons why a deeper understanding of the underlying structure of deep learning can lead to better results. Benefits of understanding the structure? By analyzing the structure of a neural network, we can identify ways to optimize it for better performance. For example, we can adjust the number of layers or nodes, or tweak the way the network processes input data. we can also we can develop neural networks that are better suited for analyzing medical images, or for predicting the stock market. If we know which nodes in the network are activated for a particular input, we can better understand how the network arrived at its decision or prediction. How Neural Network works? Each neuron represents a unit of computation that takes in a set of inputs, performs a set of calculations, and produces an output that is passed on to the next layer. Just like the neurons in our brains, each node in a neural network receives input, processes it, and passes the output on to the next node. As the data moves through the network, the connections between the nodes are strengthened or weakened, depending on the patterns in the data. This allows the network to learn from the data and make predictions or decisions based on what it has learned. Imagine a 28 by 28 grid, where a number is drawn in such a way that some pixels are darker than others. By identifying the brighter pixels, we can decipher the number that was written on the grid. This grid serves as the input for a neural network. The rows of the grid are arranged in a horizontal 1-D array, which is then transformed into a vertical array, forming the first layer of neurons. Just like this In the case of the first layer, each neuron corresponds to a single pixel in the input image, and the value inside each neuron represents the activation or intensity of that pixel. The input layer of a neural network is responsible for taking in the raw data in this case, an image and transforming it into a format that can be processed by the rest of the network. In this case, we have 28x28 input pixels, which gives us a total of 784 neurons in the input layer. Each neuron will have an activation value of either 0 or 1, depending on whether the corresponding pixel in the input image is black or white, respectively. The output layer of the neural network consists of 10 neurons in this case, each of which represents a possible output class in this case, the digits 0 through 9. The output of each neuron in the output layer represents the probability that the input image belongs to that particular class. The highest probability value determines the predicted class for that input image. Hidden Layers In between the input and output layers, we have one or more hidden layers, which perform a series of non-linear transformations on the input data. The purpose of these hidden layers is to extract higher-level features from the input data that are more meaningful for the task at hand. It is upto you how many hidden layers you want to add in your network. Each neuron in the hidden layer receives inputs from all neurons in the previous layer, and applies a set of weights and biases to those inputs before passing the result through a non-linear activation function. This process is repeated across all neurons in the hidden layer until the output layer is reached. Forward Propagation Forward propagation is the process by which input data is passed through a neural network to generate an output. It involves computing the output of each neuron in each layer of the network by applying the weights and biases to the inputs and passing the results through an activation function. Mathematical Equation where y is the output of the neural network, g is the non-linear activation function, xi refers to the i-th input feature or input variable, wi is the weight associated with the i-th input feature or variabl, and wo is the bias term, which is a constant value that is added to the linear combination of inputs. Sigma xi. wi This is the linear combination of the input features and their associated weights. This term is also sometimes referred to as the weighted sum of the inputs. Backpropagation Backpropagation is a popular algorithm used in training neural networks. It involves calculating the gradient, which is a measure of the change in the loss function with respect to each weight in the network. The loss function is a measure of how well the neural network is able to predict the correct output for a given input. By calculating the gradient of the loss function, backpropagation allows the neural network to update its weights in a way that reduces the overall error or loss during training. The algorithm works by propagating the error from the output layer back through the layers of the network, using the chain rule of calculus to calculate the gradient of the loss function with respect to each weight. This gradient is then used in gradient descent optimization to update the weights and minimize the loss function. Terminologies used in Neural Networks Lets remember the terms with real-life examples 1.Consider a scenario where a company wants to maximize their profit by selling a product. They may have a model that predicts the profit based on various factors like price, marketing spend, etc. Bias could refer to any fixed factors that affect the profit of the product, but are not directly related to the price or marketing spend. For example, if the product is a seasonal item, there may be a bias towards higher profits during certain times of the year. The difference between the actual profit and predicted profit is the loss function. Gradient descent involves calculating the gradient of the loss function with respect to each input feature, and using this gradient to iteratively adjust the feature values until the optimal values are found and the process that involves finding the optimal values for the input features to minimize the loss function is the loss optimization. The profit prediction model could use a non-linear activation function to transform the input features e.g. price, marketing spend into a predicted profit value. This function could be used to introduce non-linear relationships between the input features and the output profit. 2.Imagine you are playing a video game where you are a character trying to reach a destination, but you can only move in two dimensions forwardbackward and leftright. You know the exact coordinates of the destination, but you dont know how to get there. Your goal is to find the shortest path to the destination. In this scenario, the loss function could be the distance between your current position and the destination. The gradient of the loss function would be the direction and magnitude of the steepest slope towards the destination, which you can use to adjust your movement and get closer to the destination. As you move closer to the destination, the loss function decreases since you are getting closer to your goal, and the gradient changes accordingly. By repeatedly using the gradient to adjust your movement, you can eventually reach the destination in the shortest possible path. I hope you are leaving while having a good knowledge towards neural networks. For better understanding of this concept watch the video in reference. References P.S I pay my thanks to 3Blue1Brown community from whom I got inspired to write this article..  -- -- 5 Written by Sadaf Saleem I believe learning is all about sharing and uplifting each other. Lets connect! Responses 5 Help Status About Careers Press Blog Privacy Terms Text to speech Teams", "metadata": {"length": 9028, "has_title": true, "domain": "medium.com"}}
{"url": "https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article", "title": "10 Types of Machine Learning Algorithms and Models", "content": "Table of Contents Top 10 Machine Learning Algorithms? Types of Machine Learning Algorithms List of Popular Machine Learning Algorithms Supervised vs. Unsupervised vs. Reinforcement Learning Algorithms When to Use Supervised, Unsupervised, or Reinforcement Learning Factors to Consider When Choosing a Machine Learning Algorithm Conclusion FAQs 10 Types of Machine Learning Algorithms and Models Table of Contents Top 10 Machine Learning Algorithms? Types of Machine Learning Algorithms List of Popular Machine Learning Algorithms Supervised vs. Unsupervised vs. Reinforcement Learning Algorithms When to Use Supervised, Unsupervised, or Reinforcement Learning Factors to Consider When Choosing a Machine Learning Algorithm Conclusion FAQs In a world where nearly all manual tasks are being automated, the definition of manual is changing. There are now many different types of Machine Learning algorithms, some of which can help computers play chess, perform surgeries, and get smarter and more personal. We are living in an era of constant technological progress, and looking at how computing has advanced over the years, we can predict whats to come in the days ahead. In the fast-evolving field of machine learning, understanding the right algorithms is crucial for any aspiring engineer or data scientist. This article highlights the top 10 machine learning algorithms that every machine learning engineer should be familiar with to build effective models and derive meaningful insights from data. Become a AI  Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Heres what learners are saying regarding our programs Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice we could use the knowledge we acquired while doing the projects and apply it in real life. Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority LTA Singapore I completed a Masters Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience. Top 10 Machine Learning Algorithms? Below is the list of the top 10 commonly used Machine Learning Algorithms Types of Machine Learning Algorithms 1. Supervised Learning Supervised learning algorithms are trained using labeled data, which means the input data is tagged with the correct output. The goal of these algorithms is to learn a mapping from inputs to outputs, making it possible to predict the output for new data. Common supervised learning algorithms include Linear Regression Used for predicting continuous outcomes. It models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. Logistic Regression Used for binary classification tasks e.g., predicting yesno outcomes. It estimates probabilities using a logistic function. Decision Trees These models predict the value of a target variable by learning simple decision rules inferred from the data features. Random Forests An ensemble of decision trees, typically used for classification and regression, improving model accuracy and overfitting control. Support Vector Machines SVM Effective in high-dimensional spaces, SVM is primarily used for classification but can also be used for regression. Neural Networks These are powerful models that can capture complex non-linear relationships. They are widely used in deep learning applications. Become a AI  Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Heres what learners are saying regarding our programs Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice we could use the knowledge we acquired while doing the projects and apply it in real life. Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority LTA Singapore I completed a Masters Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience. 2. Unsupervised Learning Unsupervised learning algorithms are used with data sets without labeled responses. The goal here is to infer the natural structure present within a set of data points. Common unsupervised learning techniques include Clustering Algorithms like K-means, hierarchical clustering, and DBSCAN group a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. Association These algorithms find rules that describe large portions of your data, such as market basket analysis. Principal Component Analysis PCA A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables. Autoencoders Special type of neural network used to learn efficient codings of unlabeled data. 3. Reinforcement Learning Reinforcement learning algorithms learn to make a sequence of decisions. The algorithm learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an agent makes decisions by following a policy based on which actions to take, and it learns from the consequences of these actions through rewards or penalties. Q-learning This is a model-free reinforcement learning algorithm that learns the value of an action in a particular state. Deep Q-Networks DQN It combines Q-learning with deep neural networks, allowing the approach to learn successful policies directly from high-dimensional sensory inputs. Policy Gradient Methods These methods optimize the parameters of a policy directly as opposed to estimating the value of actions. Monte Carlo Tree Search MCTS Used in decision processes for finding optimal decisions by playing out scenarios, notably used in games like Go. These categories provide a broad overview of the most common types of machine learning algorithms. Each has its strengths and ideal use cases, making them better suited for certain types of tasks over others. Become a AI  Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Heres what learners are saying regarding our programs Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice we could use the knowledge we acquired while doing the projects and apply it in real life. Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority LTA Singapore I completed a Masters Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience. List of Popular Machine Learning Algorithms 1. Linear Regression To understand the working functionality of Linear Regression, imagine how you would arrange random logs of wood in increasing order of their weight. There is a catch however  you cannot weigh each log. You have to guess its weight just by looking at the height and girth of the log visual analysis and arranging them using a combination of these visible parameters. This is what linear regression in machine learning is like. In this process, a relationship is established between independent and dependent variables by fitting them to a line. This line is known as the regression line and is represented by a linear equation Y a X  b. In this equation The coefficients a  b are derived by minimizing the sum of the squared difference of distance between data points and the regression line. 2. Logistic Regression Logistic Regression is used to estimate discrete values usually binary values like 01 from a set of independent variables. It helps predict the probability of an event by fitting data to a logit function. It is also called logit regression. These methods listed below are often used to help improve logistic regression models 3. Decision Tree Decision Tree algorithm in machine learning is one of the most popular algorithm in use today this is a supervised learning algorithm that is used for classifying problems. It works well in classifying both categorical and continuous dependent variables. This algorithm divides the population into two or more homogeneous sets based on the most significant attributes independent variables. 4. SVM Support Vector Machine Algorithm SVM algorithm is a method of a classification algorithm in which you plot raw data as points in an n-dimensional space where n is the number of features you have. The value of each feature is then tied to a particular coordinate, making it easy to classify the data. Lines called classifiers can be used to split the data and plot them on a graph. 5. Naive Bayes Algorithm A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Even if these features are related to each other, a Naive Bayes classifier would consider all of these properties independently when calculating the probability of a particular outcome. A Naive Bayesian model is easy to build and useful for massive datasets. Its simple and is known to outperform even highly sophisticated classification methods. 6. KNN K- Nearest Neighbors Algorithm This algorithm can be applied to both classification and regression problems. Apparently, within the Data Science industry, its more widely used to solve classification problems. Its a simple algorithm that stores all available cases and classifies any new cases by taking a majority vote of its k neighbors. The case is then assigned to the class with which it has the most in common. A distance function performs this measurement. KNN can be easily understood by comparing it to real life. For example, if you want information about a person, it makes sense to talk to his or her friends and colleagues! Things to consider before selecting K Nearest Neighbours Algorithm 7. K-Means It is an unsupervised learning algorithm that solves clustering problems. Data sets are classified into a particular number of clusters lets call that number K in such a way that all the data points within a cluster are homogenous and heterogeneous from the data in other clusters. How K-means forms clusters Become a AI  Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Heres what learners are saying regarding our programs Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice we could use the knowledge we acquired while doing the projects and apply it in real life. Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority LTA Singapore I completed a Masters Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience. 8. Random Forest Algorithm A collective of decision trees is called a Random Forest. To classify a new object based on its attributes, each tree is classified, and the tree votes for that class. The forest chooses the classification having the most votes over all the trees in the forest. Each tree is planted  grown as follows 9. Dimensionality Reduction Algorithms In todays world, vast amounts of data are being stored and analyzed by corporates, government agencies, and research organizations. As a data scientist, you know that this raw data contains a lot of information - the challenge is to identify significant patterns and variables. Dimensionality reduction algorithms like Decision Tree, Factor Analysis, Missing Value Ratio, and Random Forest can help you find relevant details. 10. Gradient Boosting Algorithm and AdaBoosting Algorithm Gradient Boosting Algorithm and AdaBoosting Algorithm are boosting algorithms used when massive loads of data have to be handled to make predictions with high accuracy. Boosting is an ensemble learning algorithm that combines the predictive power of several base estimators to improve robustness. In short, it combines multiple weak or average predictors to build a strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix. These are the most preferred machine learning algorithms today. Use them, along with Python and R Codes, to achieve accurate outcomes. You can also watch our in-demand video on top Machine Learning Algorithms. Supervised vs. Unsupervised vs. Reinforcement Learning Algorithms Lets look at how supervised, unsupervised, and reinforcement learning really stack up across a few key areas. Data Labeling In supervised learning, you have labeled data at your disposal, meaning the answers are already known for each example, making it easier to train the model. Unsupervised learning, on the other hand, doesnt come with labels, so the algorithm has to figure out patterns on its own. Reinforcement learning also skips labeled data instead, it learns by taking actions, getting feedback through rewards or penalties, and using that feedback to keep improving. Goal Orientation Supervised learning has a clear goal in mind youre trying to predict specific outcomes using labeled data. Unsupervised learning isnt as structured its more about exploring the data to uncover hidden patterns or clusters. With reinforcement learning, the goal is all about maximizing rewards over time, adjusting actions based on past mistakes and successes to do better as it goes along. Learning Approach In supervised learning, it involves giving the model numerous examples with a known result and the model is trained to achieve the results through such examples. Unsupervised learning takes the algorithm in a different role discovering structure within the data. For example, finding clusters or associations. Reinforcement learning in its approach is relatively different, it is more fluid in that it evolves through interacting with the environment and learning as it progresses through its strategy. Application Scenarios Supervised learning is best suited for tasks such as outcome forecasting and pattern recognition. It involves classification and prediction. On the other hand, unsupervised learning is most useful in identifying groups within the data, detecting outliers, or reducing the dimensionality of the data. Reinforcement learning is particularly useful in areas where real-time decisions are required such as robotics, games, etc. in which performance can be enhanced through experience. Become a AI  Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Heres what learners are saying regarding our programs Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice we could use the knowledge we acquired while doing the projects and apply it in real life. Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority LTA Singapore I completed a Masters Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience. When to Use Supervised, Unsupervised, or Reinforcement Learning Supervised learning works best when labeled data is readily available, and you need precise predictions. Its often used in spam detection, stock price prediction, and medical diagnosis. Unsupervised learning is great when exploring new data to find patterns or clusters, such as customer segmentation or anomaly detection. Reinforcement learning is suitable for scenarios involving continuous learning, like training a robot to navigate or optimizing game strategies, where feedback is given over time. Factors to Consider When Choosing a Machine Learning Algorithm Lets explore what to consider when making choosing a machine learning algorithm Type of Data The first thing to look at is determining the type of data that you have. For instance, labeled datasets or those with defined outputs can be entrusted in the hands of supervised methods. On the other hand, in the case of unlabeled data, unsupervised approaches are required to locate hidden structures. In scenarios where learning is carried out through interactions, reinforcement learning seems to be a useful candidate. Complexity of the Problem After that, evaluate the complexity of the problem you are trying to solve. In tasks that are less complex, simpler algorithms can do the job. However, if youre tackling a more complex issue with intricate relationships, you might want to use more advanced methods, like neural networks or ensemble techniques. Just be prepared for a bit more effort and tuning. Computational Resources Another important factor is the computational power at your disposal. Some algorithms, like deep learning models, can be resource-intensive and require powerful hardware. If youre working with limited resources, simpler algorithms like logistic regression or k-nearest neighbors can still deliver solid results without putting too much strain on your system. Interpretability vs. Accuracy Finally, think about whether you need an algorithm thats easy to understand or one that prioritizes accuracy, even if its a bit of a black box. Decision trees and linear regression are generally easier to interpret, making them great for explaining to stakeholders. In contrast, more complex models like neural networks might give you better accuracy but can be harder to explain. Become a AI  Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Heres what learners are saying regarding our programs Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice we could use the knowledge we acquired while doing the projects and apply it in real life. Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority LTA Singapore I completed a Masters Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience. Conclusion IMastering these Machine Learning Algorithms are a great way to build a career in machine learning. The field is proliferating, and the sooner you understand the scope of machine learning tools, the sooner youll be able to provide solutions to complex work problems. However, if you are experienced in the field and want to boost your career, you can take-up the Post Graduate Program in AI and Machine Learning in partnership with Purdue University collaborated with IBM. This program gives you an in-depth knowledge of Python, Deep Learning algorithm with the Tensor flow, Natural Language Processing, Speech Recognition, Computer Vision, and Reinforcement Learning. Explore and enroll today! FAQs 1. What is an algorithm in machine learning? Algorithms in machine learning are mathematical procedures and techniques that allow computers to learn from data, identify patterns, make predictions, or perform tasks without explicit programming. These algorithms can be categorized into various types, such as supervised learning, unsupervised learning, reinforcement learning, and more. 2. What are the three types of machine learning algorithms? The three basic machine learning algorithms are 3. What are the 4 machine learning algorithm? The 4 machine learning algorithms are 4. Which ML algorithm is best for prediction? The best ML algorithm for prediction depends on variety of factors such as the nature of the problem, the type of data, and the specific requirements. Popular algorithms for prediction tasks include Support Vector Machines, Random Forests, and Gradient Boosting methods. However, the choice of an algorithm should be based on experimentation and evaluation of the specific problem and dataset at hand. 5. What is the difference between supervised and unsupervised learning algorithms? The primary difference between supervised and unsupervised learning lies in the type of data used for training. Supervised learning algorithms use labeled data, where the target output is known, to learn patterns and make predictions. Unsupervised learning algorithms work with unlabeled data, relying on intrinsic patterns and relationships to group data points or discover hidden structures. 6. Is CNN a machine learning algorithm? A convolutional neural network CNN or convnet is a type of artificial neural network used for various tasks, especially with images and videos. Its a part of machine learning and works with different kinds of data. Our AI  ML Courses Duration And Fees AI  Machine Learning Courses typically range from a few weeks to several months, with fees varying based on program and institution. Cohort Starts 2 Dec, 2024 Cohort Starts 10 Dec, 2024 Cohort Starts 10 Dec, 2024 Cohort Starts 12 Dec, 2024 Cohort Starts 17 Dec, 2024 Cohort Starts 17 Dec, 2024 Recommended Reads Machine Learning Interview Guide What is Epoch in Machine Learning? Different Types of Machine Learning Exploring AIs Core Machine Learning Career Guide A Playbook to Becoming a Machine Learning Engineer What is Machine Learning and How Does It Work An Introduction To Machine Learning Get Affiliated Certifications with Live Class programs Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Caltech Post Graduate Program in AI and Machine Learning  2009 -2024- Simplilearn Solutions. Follow us! Company Work with us Discover For Businesses Learn On the Go! Trending Post Graduate Programs Trending Master Programs Trending Courses Trending Categories Trending Resources", "metadata": {"length": 22961, "has_title": true, "domain": "simplilearn.com"}}
{"url": "https://www.ibm.com/topics/machine-learning-algorithms", "title": "What Is a Machine Learning Algorithm?  IBM", "content": "A machine learning algorithm is a set of rules or processes used by an AI system to conduct tasksmost often to discover new data insights and patterns, or to predict output values from a given set of input variables. Algorithms enable machine learning ML to learn. Industry analysts agree on the importance of machine learning and its underlying algorithms. From Forrester, Advancements in machine-learning algorithms bring precision and depth to marketing data analysis that helps marketers understand how marketing detailssuch as platform, creative, call to action, or messagingimpact marketing performance.1 While Gartner states that, Machine learning is at the core of many successful AI applications, fueling its enormous traction in the market.2 Most often, training ML algorithms on more data will provide more accurate answers than training on less data. Using statistical methods, algorithms are trained to determine classifications or make predictions, and to uncover key insights in data mining projects. These insights can subsequently improve your decision-making to boost key growth metrics. Use cases for machine learning algorithms include the ability to analyze data to identify trends and predict issues before they occur.3 More advanced AI can enable more personalized support, reduce response times, provide speech recognition and improve customer satisfaction. The industries that particularly benefit from machine learning algorithms to create new content from vast amounts of data include supply chain management, transportation and logistics, retail and manufacturing4all embracing generative AI, with its ability to automate tasks, enhance efficiency and provide valuable insights, even to beginners. Learn key benefits of generative AI and how organizations can incorporate generative AI and machine learning into their business. Register for the guide on foundation models Deep learning is a specific application of the advanced functions provided by machine learning algorithms. The distinction is in how each algorithm learns. Deep machine learning models can use your labeled datasets, also known as supervised learning, to inform its algorithm, but it doesnt necessarily require labeled data. Deep learning can ingest unstructured data in its raw form such as text or images, and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of larger data sets. The easiest way to think about artificial intelligence, machine learning, deep learning and neural networks is to think of them as a series of AI systems from largest to smallest, each encompassing the next. Artificial intelligence AI is the overarching system. Machine learning is a subset of AI. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. Its the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three. A paper from UC Berkeley breaks out the learning system of a machine learning algorithm into three main parts.5 3. A model optimization process If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this evaluate and optimize process, updating weights autonomously until a threshold of accuracy has been met. Supervised learning in particular uses a training set to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which enables the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized. There are four types of machine learning algorithms supervised, unsupervised, semi-supervised, and reinforcement. Depending on your budget, need for speed and precision required, each type and variant has its own advantages. Advanced machine learning algorithms require multiple technologiesincluding deep learning, neural networks and natural language processingand are able to use both unsupervised and supervised learning.6 The following are the most popular and commonly used algorithms. Supervised learning can be separated into two types of problems when data mining classification and regression. Various algorithms and computations techniques are used in supervised machine learning processes, often calculated through use of programs such as Python. Supervised learning algorithms include Unlike supervised learning, unsupervised learning uses unlabeled data. From that data, the algorithm discovers patterns that help solve clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set. Common clustering algorithms are hierarchical, K-means, Gaussian mixture models and Dimensionality Reduction Methods such as PCA and t-SNE. Semi-supervised learning algorithms In this case, learning occurs when only part of the given input data has been labeledgiving the algorithm a bit of a head start. This approach can combine the best of both worlds10improved accuracy associated with supervised machine learning and the ability to make use of cost-effective unlabeled data, as in the case of unsupervised machine learning. Reinforcement algorithms In this case, the algorithms are trained just as humans learnthrough rewards and penaltieswhich are measured and tracked by a reinforcement learning agent11 which has a general understanding of the probability of successfully moving the score up vs. moving it down. Through trial and error, the agent learns to take actions that lead to the most favorable outcomes over time. Reinforcement learning is often used12 in resource management, robotics and video games. Design complex neural networks. Experiment at scale to deploy optimized learning models within IBM Watson Studio. Analyze data and build analytics and predictive models of future outcomes. Uncover risks and opportunities for your business. NLP is AI that speaks the language of your business. Build solutions that drive 383 percent ROI over three years with IBM Watson Discovery. Learn the fundamental concepts for AI and generative AI, including prompt engineering, large language models and the best open-source projects. IBM again recognized as a Leader in the 2023 Gartner Magic Quadrant for Enterprise Conversational AI. Learn the tools businesses use to efficiently run and manage AI models and empower their data scientist with technology that can help optimize their data-driven decision making. Explore how machine learning lets you continually learn from data and predict the future. Four strategies to scale AI with a strong data foundation. AI technology has been rapidly evolving over the last couple of decades. Learn how businesses are implementing AI today. Build an AI strategy for your business on one collaborative AI and data platformIBM watsonx. Train, validate, tune and deploy AI models to help you scale and accelerate the impact of AI with trusted data across your business. Footnotes All footnote links below reside outside of IBM. 1 Forrester Use Marketing Analytics To Support Your 2023 Marketing Strategy 2 Gartner What Is Artificial Intelligence? 3 Gartner Peer Community How will AI help facilitate desk and IT support teams? 4 IDC Generative AI Exploring Trends and Use Cases Across AsiaPacific Supply Chains 5 Berkeley School of information What Is Machine Learning ML? 6 Gartner Glossary Machine Learning 7 TechTarget What are machine learning algorithms? 8 GeeksforGeeks Hierarchical Clustering in Data Mining 9 Stanford University K Means 10 Booz Allen How do machines learn? 11 G2 Reinforcement Learning How Machines Learn From Their Mistakes 12 TechTarget What is machine learning and how does it work?", "metadata": {"length": 8019, "has_title": true, "domain": "ibm.com"}}
{"url": "https://www.geeksforgeeks.org/machine-learning-algorithms/", "title": "Machine Learning Algorithms", "content": "Machine Learning Algorithms Machine learning algorithms are computational models that allow computers to understand patterns and forecast or make judgments based on data without explicit programming. These algorithms form the foundation of modern artificial intelligence and are used in various applications, including image and speech recognition, natural language processing, recommendation systems, fraud detection, autonomous cars, etc. Table of Content This Machine learning Algorithms article will cover all the essential algorithms of machine learning like Support vector machine, decision-making, logistics regression, naive bayees classifier, random forest, k-mean clustering, reinforcement learning, vector, hierarchical clustering, xgboost, adaboost, logistics, etc. Types of Machine Learning Algorithms There are four types of machine learning algorithms 1. Supervised Learning Supervised learning involves training a model on labeled data, where the desired output is known. The model learns to map inputs to outputs based on the provided examples. A. Classification 1. Logistic Regression 2. Support Vector Machines SVM 3. k-Nearest Neighbors k-NN 4. Naive Bayes 5. Decision Trees 6. Random Forest 7. Gradient Boosting e.g., XGBoost, LightGBM, CatBoost 8. Neural Networks e.g., Multilayer Perceptron B. Regression 1. Linear Regression 2. Ridge Regression 3. Lasso Regression 4. Support Vector Regression SVR 5. Decision Trees Regression 6. Random Forest Regression 7. Gradient Boosting Regression 8. Neural Networks Regression 2. Unsupervised Learning Unsupervised learning works with unlabeled data and aims to find hidden patterns or intrinsic structures in the input data. A. Clustering 1. k-Means 2. Hierarchical Clustering 3. DBSCAN Density-Based Spatial Clustering of Applications with Noise 4. Gaussian Mixture Models GMM B. Dimensionality Reduction 1. Principal Component Analysis PCA 2. t-Distributed Stochastic Neighbor Embedding t-SNE 3. Linear Discriminant Analysis LDA 4. Independent Component Analysis ICA 5. UMAP Uniform Manifold Approximation and Projection C. Association 1. Apriori Algorithm 2. Eclat Algorithm 3. Reinforcement Learning Reinforcement learning involves training agents to make a sequence of decisions by rewarding them for good actions and penalizing them for bad ones. A. Model-Free Methods 1. Q-Learning 2. Deep Q-Network DQN 3. SARSA State-Action-Reward-State-Action 4. Policy Gradient Methods e.g., REINFORCE B. Model-Based Methods 1. Deep Deterministic Policy Gradient DDPG 2. Proximal Policy Optimization PPO 3. Trust Region Policy Optimization TRPO C. Value-Based Methods 1. Monte Carlo Methods 2. Temporal Difference TD Learning 4. Ensemble Learning Ensemble learning combines multiple models to improve performance by leveraging the strengths of each model. 1. Bagging e.g., Random Forest 2. Boosting e.g., AdaBoost, Gradient Boosting 3. Stacking - 1. Supervised Learning Algorithm Supervised learning is a type of machine learning algorithms where we used labeled dataset to train the model or algorithms. The goal of the algorithm is to learn a mapping from the input data to the output labels, allowing it to make predictions or classifications on new, unseen data. Supervised Machine Learning Algorithms Some of the Supervised Machine Learning Algorithms can be used used for both Classification  Regression with a little bit of modification Metrics for Classification  Regression Algorithms Cross Validation Technique Optimization Technique 2. Unsupervised Learning Algorithm Unsupervised Learning is a type of machine learning algorithms where the algorithms are used to find the patterns, structure or relationship within a dataset using unlabled dataset. It explores the datas inherent structure without predefined categories or labels. Unsupervised Machine Learning Algorithms 3. Reinforcement Learning Reinforcement Learning is a type of machine learning algorithms where an agent learns to make successive decisions by interacting with its surroundings. The agent receives the feedback in the form of incentives or punishments based on its actions. The agents purpose is to discover optimal tactics that maximize cumulative rewards over time through trial and error. Reinforcement learning is frequently employed in scenarios in which the agent must learn how to navigate an environment, play games, manage robots, or make judgments in uncertain situations. Reinforcement Learning Discover the fundamental concepts driving machine learning by learning the top 10 algorithms, such as linear regression, decision trees, and neural networks. Machine Learning Algorithm  FAQs 1. What is an algorithm in Machine Learning? Machine learning algorithms are techniques based on statistical concepts that enable computers to learn from data, discover patterns, make predictions, or complete tasks without the need for explicit programming. These algorithms are broadly classified into the three types, i.e supervised learning, unsupervised learning, and reinforcement learning. 2. What are types of Machine Learning? There are mainly three types of machine learning 3. Which ML algorithm is best for prediction? The ideal machine learning method for prediction is determined by a number of criteria, including the nature of the problem, the type of data, and the unique requirements. Support Vector Machines, Random Forests, and Gradient Boosting approaches are popular for prediction workloads. The selection of an algorithm, on the other hand, should be based on testing and evaluation of the specific problem and dataset at hand. 4. What are the 10 Popular Machine Learning Algorithms? Below is the list of Top 10 commonly used Machine Learning ML Algorithms Similar Reads What kind of Experience do you want to share?", "metadata": {"length": 5779, "has_title": true, "domain": "geeksforgeeks.org"}}
{"url": "https://www.coursera.org/articles/machine-learning-algorithms", "title": "10 Machine Learning Algorithms to Know in 2025  Coursera", "content": "10 Machine Learning Algorithms to Know in 2024 Machine learning algorithms power many services in the world today. Here are 10 to know as you look to start your career. At the core of machine learning are algorithms, which are trained to become the machine learning models used to power some of the most impactful innovations in the world today. In this article, youll learn about 10 of the most popular machine learning algorithms that youll want to know, and explore the different learning styles used to turn machine learning algorithms into functioning machine learning models. Why is machine learning important? Machine learning ML can do everything from analyzing X-rays to predicting stock market prices to recommending binge-worthy television shows. With such a wide range of applications, its not surprising that the global machine learning market is projected to grow from 21.7 billion in 2022 to 209.91 billion by 2029, according to Fortune Business Insights 1. 10 machine learning algorithms to know In simple terms, a machine learning algorithm is like a recipe that allows computers to learn and make predictions from data. Instead of explicitly telling the computer what to do, we provide it with a large amount of data and let it discover patterns, relationships, and insights on its own. Read more 3 Types of Machine Learning You Should Know From classification to regression, here are 10 types of machine learning algorithms you need to know in the field of machine learning 1. Linear regression Linear regression is a supervised machine learning technique used for predicting and forecasting values that fall within a continuous range, such as sales numbers or housing prices. It is a technique derived from statistics and is commonly used to establish a relationship between an input variable X and an output variable Y that can be represented by a straight line. In simple terms, linear regression takes a set of data points with known input and output values and finds the line that best fits those points. This line, known as the regression line, serves as a predictive model. By using this line, we can estimate or predict the output value Y for a given input value X. Linear regression is primarily used for predictive modeling rather than categorization. It is useful when we want to understand how changes in the input variable affect the output variable. By analyzing the slope and intercept of the regression line, we can gain insights into the relationship between the variables and make predictions based on this understanding. 2. Logistic regression Logistic regression, also known as logit regression, is a supervised learning algorithm primarily used for binary classification tasks. It is commonly employed when we want to determine whether an input belongs to one class or another, such as deciding whether an image is a cat or not a cat. Logistic regression predicts the probability that an input can be categorized into a single primary class. However, in practice, it is commonly used to group outputs into two categories the primary class and not the primary class. To accomplish this, logistic regression creates a threshold or boundary for binary classification. For example, any output value between 0 and 0.49 might be classified as one group, while values between 0.50 and 1.00 would be classified as the other group. Consequently, logistic regression is typically used for binary categorization rather than predictive modeling. It enables us to assign input data to one of two classes based on the probability estimate and a defined threshold. This makes logistic regression a powerful tool for tasks such as image recognition, spam email detection, or medical diagnosis where we need to categorize data into distinct classes. Beginner-friendly machine learning courses Interested in learning more about machine learning but arent sure where to start? Consider enrolling in one of these beginner-friendly machine learning courses on Coursera today In Open.AI and Stanfords Machine Learning Specialization, youll master fundamental AI concepts and develop practical machine-learning skills in as little as two months. The University of Londons Machine Learning for All course will introduce you to the basics of how machine learning works and guide you through training a machine learning model with a data set on a non-programming-based platform. 3. Naive Bayes Naive Bayes is a set of supervised learning algorithms used to create predictive models for binary or multi-classification tasks. It is based on Bayes Theorem and operates on conditional probabilities, which estimate the likelihood of a classification based on the combined factors while assuming independence between them. Lets consider a program that identifies plants using a Naive Bayes algorithm. The algorithm takes into account specific factors such as perceived size, color, and shape to categorize images of plants. Although each of these factors is considered independently, the algorithm combines them to assess the probability of an object being a particular plant. Naive Bayes leverages the assumption of independence among the factors, which simplifies the calculations and allows the algorithm to work efficiently with large datasets. It is particularly well-suited for tasks like document classification, email spam filtering, sentiment analysis, and many other applications where the factors can be considered separately but still contribute to the overall classification. 4. Decision tree A decision tree is a supervised learning algorithm used for classification and predictive modeling tasks. It resembles a flowchart, starting with a root node that asks a specific question about the data. Based on the answer, the data is directed down different branches to subsequent internal nodes, which ask further questions and guide the data to subsequent branches. This process continues until the data reaches an end node, also known as a leaf node, where no further branching occurs. Decision tree algorithms are popular in machine learning because they can handle complex datasets with ease and simplicity. The algorithms structure makes it straightforward to understand and interpret the decision-making process. By asking a sequence of questions and following the corresponding branches, decision trees enable us to classify or predict outcomes based on the datas characteristics. This simplicity and interpretability make decision trees valuable for various applications in machine learning, especially when dealing with complex datasets. Learn more about decision trees in this lecture from the University of Michigans Applied Machine Learning in Python course 5. Random forest A random forest algorithm is an ensemble of decision trees used for classification and predictive modeling. Instead of relying on a single decision tree, a random forest combines the predictions from multiple decision trees to make more accurate predictions. In a random forest, numerous decision tree algorithms sometimes hundreds or even thousands are individually trained using different random samples from the training dataset. This sampling method is called bagging. Each decision tree is trained independently on its respective random sample. Once trained, the random forest takes the same data and feeds it into each decision tree. Each tree produces a prediction, and the random forest tallies the results. The most common prediction among all the decision trees is then selected as the final prediction for the dataset. Random forests address a common issue called overfitting that can occur with individual decision trees. Overfitting happens when a decision tree becomes too closely aligned with its training data, making it less accurate when presented with new data. 6. K-nearest neighbor KNN K-nearest neighbor KNN is a supervised learning algorithm commonly used for classification and predictive modeling tasks. The name K-nearest neighbor reflects the algorithms approach of classifying an output based on its proximity to other data points on a graph. Lets say we have a dataset with labeled points, some marked as blue and others as red. When we want to classify a new data point, KNN looks at its nearest neighbors in the graph. The K in KNN refers to the number of nearest neighbors considered. For example, if K is set to 5, the algorithm looks at the 5 closest points to the new data point. Based on the majority of the labels among the K nearest neighbors, the algorithm assigns a classification to the new data point. For instance, if most of the nearest neighbors are blue points, the algorithm classifies the new point as belonging to the blue group. Additionally, KNN can also be used for prediction tasks. Instead of assigning a class label, KNN can estimate the value of an unknown data point based on the average or median of its K nearest neighbors. 7. K-means K-means is an unsupervised algorithm commonly used for clustering and pattern recognition tasks. It aims to group data points based on their proximity to one another. Similar to K-nearest neighbor KNN, K-means clustering utilizes the concept of proximity to identify patterns in data. Each of the clusters is defined by a centroid, a real or imaginary center point for the cluster. K-means is useful on large data sets, especially for clustering, though it can falter when handling outliers. Clustering algorithms are particularly useful for large datasets and can provide insights into the inherent structure of the data by grouping similar points together. It has applications in various fields such as customer segmentation, image compression, and anomaly detection. Read more What is Big Data? A Laypersons Guide 8. Support vector machine SVM A support vector machine SVM is a supervised learning algorithm commonly used for classification and predictive modeling tasks. SVM algorithms are popular because they are reliable and can work well even with a small amount of data. SVM algorithms work by creating a decision boundary called a hyperplane. In two-dimensional space, this hyperplane is like a line that separates two sets of labeled data. The goal of SVM is to find the best possible decision boundary by maximizing the margin between the two sets of labeled data. It looks for the widest gap or space between the classes. Any new data point that falls on either side of this decision boundary is classified based on the labels in the training dataset. Its important to note that hyperplanes can take on different shapes when plotted in three-dimensional space, allowing SVM to handle more complex patterns and relationships in the data. Find out more about support vector machines and how theyre used in this lecture from IBMs Machine Learning with Python course 9. Apriori Apriori is an unsupervised learning algorithm used for predictive modeling, particularly in the field of association rule mining. The Apriori algorithm was initially proposed in the early 1990s as a way to discover association rules between item sets. It is commonly used in pattern recognition and prediction tasks, such as understanding a consumers likelihood of purchasing one product after buying another. The Apriori algorithm works by examining transactional data stored in a relational database. It identifies frequent itemsets, which are combinations of items that often occur together in transactions. These itemsets are then used to generate association rules. For example, if customers frequently buy product A and product B together, an association rule can be generated to suggest that purchasing A increases the likelihood of buying B. By applying the Apriori algorithm, analysts can uncover valuable insights from transactional data, enabling them to make predictions or recommendations based on observed patterns of itemset associations. 10. Gradient boosting Gradient boosting algorithms employ an ensemble method, which means they create a series of weak models that are iteratively improved upon to form a strong predictive model. The iterative process gradually reduces the errors made by the models, leading to the generation of an optimal and accurate final model. The algorithm starts with a simple, naive model that may make basic assumptions, such as classifying data based on whether it is above or below the mean. This initial model serves as a starting point. In each iteration, the algorithm builds a new model that focuses on correcting the mistakes made by the previous models. It identifies the patterns or relationships that the previous models struggled to capture and incorporates them into the new model. Gradient boosting is effective in handling complex problems and large datasets. It can capture intricate patterns and dependencies that may be missed by a single model. By combining the predictions from multiple models, gradient boosting produces a powerful predictive model. How do you train a machine learning algorithm? Everyone learns differentlyincluding machines! Generally, data scientists use three different learning styles to train machine learning algorithms supervised learning, unsupervised learning, and reinforcement learning. Learn more about each of them in this article. Learn from machine learning experts on Coursera With Machine Learning from DeepLearning.AI on Coursera, youll have the opportunity to learn practical machine learning concepts and techniques from industry experts. Develop the skills to build and deploy machine learning models, analyze data, and make informed decisions through hands-on projects and interactive exercises. Not only will you build confidence in applying machine learning in various domains, you could also open doors to exciting career opportunities in data science. Article sources Fortune Business Insights. The global machine learning ML market is expected to grow from 21.17 billion in 2022 to 209.91 billion by 2029, httpswww.fortunebusinessinsights.commachine-learning-market-102226. Accessed April 1, 2024. Keep reading Coursera Staff Editorial Team Courseras editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals. Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More", "metadata": {"length": 14496, "has_title": true, "domain": "coursera.org"}}
{"url": "https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/", "title": "Top 10 Machine Learning Algorithms You Must Know in 2025", "content": "Mastering Pythons Set Difference A Game-Changer for Data Wrangling Reading list Basics of Machine Learning Machine Learning Lifecycle Importance of Stats and EDA Understanding Data Probability Exploring Continuous Variable Exploring Categorical Variables Missing Values and Outliers Central Limit theorem Bivariate Analysis Introduction Continuous - Continuous Variables Continuous Categorical Categorical Categorical Multivariate Analysis Different tasks in Machine Learning Build Your First Predictive Model Evaluation Metrics Preprocessing Data Linear Models KNN Selecting the Right Model Feature Selection Techniques Decision Tree Feature Engineering Naive Bayes Multiclass and Multilabel Basics of Ensemble Techniques Advance Ensemble Techniques Hyperparameter Tuning Support Vector Machine Advance Dimensionality Reduction Unsupervised Machine Learning Methods Recommendation Engines Improving ML models Working with Large Datasets Interpretability of Machine Learning Models Interpretability of Machine Learning Models Automated Machine Learning Model Deployment Deploying ML Models Embedded Devices Top 10 Machine Learning Algorithms You Must Know Googles self-driving cars and robots get a lot of press, but the companys real future is in machine learning, the technology that enables computers to get smarter and more personal. We are probably living in the most defining period of human history. The period when computing moved from large mainframes to PCs to the cloud. But what makes it defining is not what has happened but what is coming our way in years to come. What makes this period exciting and enthralling for someone like me is the democratization of the various tools, techniques, and machine learning algorithms that followed the boost in computing. Welcome to the world of data science! Today, as a data scientist, I can build data-crunching machines with complex algorithms for a few dollars per hour. But reaching here wasnt easy! I had my dark days and nights. This article will cover some popular machine learning algorithms. We will discuss different types of machine learning algorithms and elucidate the categories, such as supervised and unsupervised learning. You will also learn how to use these machine learning algorithms. By the end of this article, you will have the skill to select the appropriate algorithms for your tasks. Table of contents Who Can Benefit the Most From this Guide? What I am giving out today is probably the most valuable guide I have ever created. The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning which is part of artificial intelligence enthusiasts across the world. Through this guide, I will enable you to work on machine-learning problems and gain from experience. I am providing a high-level understanding of various machine learning algorithms along with R  Python codes to run them. These should be sufficient to get your hands dirty. You can also check out our Machine Learning Course. Essentials of machine learning algorithms with implementation in R and Python. I have deliberately skipped the statistics behind these techniques and artificial neural networks, as you dont need to understand them initially. So, if you are looking for a statistical understanding of these algorithms, you should look elsewhere. But, if you want to equip yourself to start building a machine learning project, you are in for a treat. 3 Types of Machine Learning Algorithms Supervised Learning Algorithms These algorithm consists of a targetoutcome variable or dependent variable which is to be predicted from a given set of predictors independent variables. Supervised learning algorithms for classification and regression involve generating a function that maps input data to the desired outputs. Using this set of variables, we generate a function that maps input data to desired outputs. The training process continues until the model achieves the desired level of accuracy on the training data. Best supervised machine learning algorithms include Regression, Decision Tree, Random Forest, KNN, Logistic Regression, etc. Each of these algorithms serves different types of data and problem requirements, making them widely applicable across various fields. Also Read Supervised learning vs unsupervised learning Unsupervised Learning Algorithms These algorithms work with unlabeled data, where there is no targetoutcome variable to predict. Unsupervised learning algorithms for clustering and data mining are designed to identify hidden patterns or structures within the data. Using these patterns, we group data points with similar characteristics, generating a function that maps input data to clusters or groups. This process continues until the model successfully identifies meaningful patterns in the data. Common unsupervised learning algorithms include K-Means Clustering, Hierarchical Clustering, and Principal Component Analysis PCA. Each of these algorithms serves different types of data and problem requirements, making them widely applicable across various fields such as customer segmentation, anomaly detection, and pattern recognition. Reinforcement Learning Algorithms How it works Using this algorithm, the machine is trained to make specific decisions. The machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning Markov Decision Process Also Read Everything You Need to Know about Machine Learning List of Top 10 Common Machine Learning Algorithms Here is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem 1. Linear Regression It is used to estimate real values cost of houses, number of calls, total sales, etc. based on a continuous variables. Here, we establish the relationship between independent and dependent variables by fitting the best line. This best-fit line is known as the regression line and is represented by a linear equation Y aX  b. Example 1 The best way to understand linear regression is to relive this experience of childhood. Let us say you ask a child in fifth grade to arrange people in his class by increasing the order of weight without asking them their weights! What do you think the child will do? They would likely look visually analyze at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to weight by a relationship, which looks like the equation above. In this equation These coefficients a and b are derived based on minimizing the sum of the squared difference of distance between data points and the regression line. Example 2 Look at the below example. Here we have identified the best-fit line having linear equation y0.2811x13.9. Now using this equation, we can find the weight, knowing the height of a person. Linear Regression is mainly of two types Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regressionas the name suggests is characterized by multiple more than 1 independent variables. While finding the best-fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression. Heres a coding window to try out your hand and build your own linear regression model Python R Code 2. Logistic Regression Dont get confused by its name! It is a classification algorithm, not a regression algorithm. It is used to estimate discrete values  Binary values like 01, yesno, truefalse  based on a given set of independent variables. In simple words, it predicts the probability of the occurrence of an event by fitting data to a logistic function. Hence, it is also known as logit regression. Since it predicts the probability, its output values lie between 0 and 1 as expected. Again, let us try and understand this through a simple example. Lets say your friend gives you a puzzle to solve. There are only 2 outcome scenarios  either you solve it, or you dont. Now imagine that you are being given a wide range of puzzlesquizzes in an attempt to understand which subjects you are good at. The outcome of this study would be something like this  if you are given a trigonometry-based tenth-grade problem, you are 70 likely to solve it. On the other hand, if it is a grade fifth history question, the probability of getting an answer is only 30. This is what Logistic Regression provides you. Coming to the math, the log odds of the outcome are modeled as a linear combination of the predictor variables. Above, p is the probability of the presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors like in ordinary regression. Now, you may ask, why take a log? For the sake of simplicity, lets just say that this is one of the best mathematical ways to replicate a step function. I can go into more details, but that will beat the purpose of this article. Build your own logistic regression model in Python here and check the accuracy R Code Furthermore There are many different steps that could be tried in order to improve the model 3. Decision Tree This is one of my favorite algorithms, and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on the most significant attributes independent variables to make as distinct groups as possible. For more details, you can read Decision Tree Simplified. In the image above, you can see that population is classified into four different groups based on multiple attributes to identify if they will play or not. To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, and entropy. The best way to understand how the decision tree works, is to play Jezzball  a classic game from Microsoft image below. Essentially, you have a room with moving walls and you need to create walls such that the maximum area gets cleared off without the balls. So, every time you split the room with a wall, you are trying to create 2 different populations within the same room. Decision trees work in a very similar fashion by dividing a population into as different groups as possible. Read More Simplified Version of Decision Tree Algorithms Lets get our hands dirty and code our own decision tree in Python! R Code 4. SVM Support Vector Machine It is a classification method. In SVM algorithm, we plot each data item as a point in n-dimensional space where n is the number of features you have, with the value of each feature being the value of a particular coordinate. For example, if we only had two features like the Height and Hair length of an individual, wed first plot these two variables in two-dimensional space where each point has two coordinates these co-ordinates are known as Support Vectors Now, we will find some lines that split the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be the farthest away. If there are more variables, a hyperplane is used to separate the classes. In the example shown above, the line which splits the data into two differently classified groups is the black line since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, thats what class we can classify the new data as. Think of this algorithm as playing JezzBall in n-dimensional space. The tweaks in the game are Try your hand and design an SVM model in Python through this coding window R Code 5. Naive Bayes Naive Bayes is a classification technique based on Bayes theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other, a Naive Bayes algorithm for classification would treat each property as independently contributing to the probability that this fruit is an apple. The Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes classification is known to outperform even highly sophisticated methods for tasks such as text classification, spam detection, and sentiment analysis. Naive Bayes Equation Bayes theorem provides a way of calculating posterior probability Pcx from Pc, Px, and Pxc. Look at the equation below Here, Example Lets understand it using an example. Below is a training data set of weather and the corresponding target variable, Play. Now, we need to classify whether players will play or not based on weather conditions. Lets follow the below steps to perform it. Time needed 3 minutes Problem Players will pay if the weather is sunny. Is this statement correct? We can solve it using above discussed method, so PYes  Sunny  P Sunny  Yes  PYes  P Sunny Here we have P Sunny  Yes  39  0.33, PSunny  514  0.36, PYes 914  0.64 Now, P Yes  Sunny  0.33  0.64  0.36  0.60, which has a higher probability. Naive Bayes uses a similar method to predict the probability of different classes based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes. Code for a Naive Bayes classification model in Python R Code 6. kNN k- Nearest Neighbors KNN can be used for both classification and regression problems. However, it is more widely employed as a classification algorithm in machine learning. K-Nearest Neighbors is a simple, intuitive algorithm that stores all available cases and classifies new cases by a majority vote of its k nearest neighbors. The class assigned to a new case is the one most common among its K nearest neighbors as measured by a distance function. The distance functions used in KNN can be Euclidean, Manhattan, Minkowski, or Hamming distances. The first three are typically used for continuous variables, while Hamming distance is applied for categorical variables. If K  1, the case is simply assigned to the class of its nearest neighbor. However, choosing the right value of K can be challenging and often depends on the dataset being used in KNN modeling. In real-life scenarios, KNN can be likened to discovering more about a person based on their close friends or social circles. If you know nothing about someone, their neighbors characteristics can offer insights. Key Considerations for KNN Python Code R Code 7. K-Means It is a type of unsupervised algorithm which solves the clustering problem. Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters assume k clusters. Data points inside a cluster are homogeneous and heterogeneous to peer groups. Remember figuring out shapes from ink blots? k means is somewhat similar to this activity. You look at the shape and spread to decipher how many different clusterspopulations are present! How K-means forms cluster How to determine the value of K In K-means, we have clusters, and each cluster has its own centroid. The sum of the square of the difference between the centroid and the data points within a cluster constitutes the sum of the square value for that cluster. Also, when the sum of square values for all the clusters is added, it becomes a total within the sum of the square value for the cluster solution. We know that as the number of clusters increases, this value keeps on decreasing, but if you plot the result, you may see that the sum of squared distance decreases sharply up to some value of k and then much more slowly after that. Here, we can find the optimum number of clusters. Python Code R Code 8. Random Forest Random Forest is a trademarked term for an ensemble learning of decision trees. In Random Forest, weve got a collection of decision trees also known as Forest. To classify a new object based on attributes, each tree gives a classification, and we say the tree votes for that class. The forest chooses the classification having the most votes over all the trees in the forest. Each tree is planted  grown as follows For more details on this algorithm, compared with the decision tree and tuning model parameters, I would suggest you read these articles Python Code R Code 9. Dimensionality Reduction Algorithms In the last 4-5 years, there has been an exponential increase in data capturing at every possible stage. Corporates Government Agencies Research organizations are not only coming up with new sources, but also they are capturing data in great detail. For example, E-commerce companies are capturing more details about customers like their demographics, web crawling history, what they like or dislike, purchase history, feedback, and many others to give them personalized attention more than your nearest grocery shopkeeper. As data scientists, the data we are offered also consists of many features, this sounds good for building a good robust model, but there is a challenge. Howd you identify highly significant variables out of 1000 or 2000? In such cases, the dimensionality reduction algorithm helps us, along with various other algorithms like Decision Tree, Random Forest, PCA principal component analysis, Factor Analysis, Identity-based on the correlation matrix, missing value ratio, and others. To know more about these algorithms, you can read Beginners Guide To Learn Dimension Reduction Techniques. Python Code R Code 10. Gradient Boosting Algorithms Now, lets look at the 4 most commonly used gradient boosting algorithms. GBM GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms that combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to build a strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, and CrowdAnalytix. Python Code R Code GradientBoostingClassifier and Random Forest are two different boosting tree classifiers, and often people ask about the difference between these two algorithms. XGBoost Another classic gradient-boosting algorithm thats known to be the decisive choice between winning and losing in some Kaggle competitions is the XGBoost. It has an immensely high predictive power, making it the best choice for accuracy in events. It possesses both a linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques. One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modeling and has massive support for a range of languages such as Scala, Java, R, Python, Julia, and C. The support includes various objective functions, including regression, classification, and ranking. Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure, and Yarn clusters. XGBoost can also be integrated with Spark, Flink, and other cloud dataflow systems with built-in cross-validation at each iteration of the boosting process. Read this guide to learn more about XGBoost and parameter tuning. Python Code R Code LightGBM LightGBM is a gradient-boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages The framework is a fast and high-performance gradient-boosting one based on decision tree algorithms used for ranking, classification, and many other machine-learning tasks. It was developed under the Distributed Machine Learning Toolkit Project of Microsoft. Since the LightGBM is based on decision tree algorithms, it splits the tree leaf-wise with the best fit, whereas other boosting algorithms split the tree depth-wise or level-wise rather than leaf-wise. So when growing on the same leaf node in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm, resulting in much better accuracy, which any existing boosting algorithms can rarely achieve. Also, it is surprisingly very fast, hence the word Light. Python Code R Code If youre familiar with the Caret package in R, this is another way of implementing the LightGBM. Catboost CatBoost is one of open-sourced machine learning algorithms from Yandex. It can easily integrate with deep learning frameworks like Googles TensorFlow and Apples Core ML. The best part about CatBoost is that it does not require extensive data training like other ML models and can work on a variety of data formats, not undermining how robust it can be. Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors. Make sure you handle missing data well before you proceed with the implementation. Python Code R Code Practice Problems Now, its time to take the plunge and actually play with some other real-world datasets. So are you ready to take on the challenge? Accelerate your data science journey with the following practice problems End Note By now, I am sure you would have an idea of commonly used machine learning algorithms. My sole intention behind writing this article and providing the codes in R and Python is to get you started right away. If you are keen to master machine learning algorithms, start right away. Take up problems, develop a physical understanding of the process, apply these codes, and watch the fun! Hope you like the article and get full understanding about the data science algoeitrhms, machine learning models and with these you will get full understanding about machine learning algorithms. If you find this article helpful, and have an interent in masterinf your machine learning skills, then enroll in our AIML Blackbelt Plus program. Key Takeaways Frequently Asked Questions A. While the suitable algorithm depends on the problem you are trying to solve. A. In the supervised learning model, the labels associated with the features are given. In unsupervised learning, no labels are provided for the model. A. The 3 main types of ML models are based on Supervised Learning, Unsupervised Learning, and Reinforcement Learning. A. An algorithm in machine learning is a set of rules or procedures that a model follows to learn from data. It processes input data, identifies patterns, and makes predictions or decisions based on that data, enabling computers to improve over time without explicit programming. A. To apply machine learning algorithms, first, define the problem and collect relevant data. Preprocess the data cleaning, normalization, choose an appropriate algorithm based on the task classification, regression, etc., train the model on the dataset, and finally evaluate its performance using metrics like accuracy or F1-score. A. Yes, a Convolutional Neural Network CNN is a deep learning algorithm designed for image and video recognition, using convolutional layers to automatically extract features and enhance visual data processing. Sunil Ray is Chief Content Officer at Analytics Vidhya, Indias largest Analytics community. I am deeply passionate about understanding and explaining concepts from first principles. In my current role, I am responsible for creating top notch content for Analytics Vidhya including its courses, conferences, blogs and Competitions. I thrive in fast paced environment and love building and scaling products which unleash huge value for customers using data and technology. Over the last 6 years, I have built the content team and created multiple data products at Analytics Vidhya. Prior to Analytics Vidhya, I have 7 years of experience working with several insurance companies like Max Life, Max Bupa, Birla Sun Life  Aviva Life Insurance in different data roles. Industry exposure Insurance, and EdTech Major capabilities Content Development, Product Management, Analytics, Growth Strategy. Free Courses Generative AI - A Way of Life Explore Generative AI for beginners create text and images, use top AI tools, learn practical skills, and ethics. Getting Started with Large Language Models Master Large Language Models LLMs with this course, offering clear guidance in NLP and model training made simple. Building LLM Applications using Prompt Engineering This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data. Improving Real World RAG Systems Key Challenges  Practical Solutions Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications. Microsoft Excel Formulas  Functions Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. Recommended Articles Top 100 Data Science Interview Questions ... The Ultimate Guide to 12 Dimensionality Reducti... Principal Component Analysis  its Impleme... Principal Component Analysis Introduction and P... Beginners Guide To Learn Dimension Reduction Te... 20 Questions to Test Your Skills On Dimensional... Principal Component Analysis in Machine Learnin... Building A Gold Price Prediction Model Using Ma... Are You Making These Common Mistakes in Classif... 10 Must Have Machine Learning Engineer Skills i... Responses From Readers ClearSubmit reply \u0394 Awesowe compilation!! Thank you. ClearSubmit reply \u0394 Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. ClearSubmit reply \u0394 Straight, Informative and effective!! Thank you ClearSubmit reply \u0394 Write for us Write, captivate, and earn accolades and rewards for your work We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our Privacy Policy  Cookies Policy. Show details Powered By Cookies This site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our Privacy Policy  Cookies Policy. Necessary 2 Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. Necessary 2 Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. Analytics Vidhya 4 learn more about analytics vidhya privacy Analytics Vidhya 4 brahmaid It is needed for personalizing the website. Expiry Session Type HTTP csrftoken This cookie is used to prevent Cross-site request forgery often abbreviated as CSRF attacks of the website Expiry Session Type HTTPS Identityid Preserves the loginlogout state of users across the whole site. Expiry Session Type HTTPS sessionid Preserves users states across page requests. Expiry Session Type HTTPS Google 1 learn more about google privacy Google 1 g_state Google One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal. Expiry 365 days Type HTTP Statistics 4 Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. Statistics 4 Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. Microsoft 7 learn more about microsoft policy Microsoft 7 MUID Used by Microsoft Clarity, to store and track visits across websites. Expiry 1 Year Type HTTP _clck Used by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. Expiry 1 Year Type HTTP _clsk Used by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording. Expiry 1 Day Type HTTP SRM_I Collects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitors behavior. Expiry 2 Years Type HTTP SM Use to measure the use of the website for internal analytics Expiry 1 Years Type HTTP CLID The cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording. Expiry 1 Year Type HTTP SRM_B Collected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitors behavior. Expiry 2 Months Type HTTP Google 7 learn more about google privacy Google 7 _gid This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form. Expiry 399 Days Type HTTP _ga_ Used by Google Analytics, to store and count pageviews. Expiry 399 Days Type HTTP _gat_ Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. Expiry 1 Day Type HTTP collect Used to send data to Google Analytics about the visitors device and behavior. Tracks the visitor across devices and marketing channels. Expiry Session Type PIXEL AEC cookies ensure that requests within a browsing session are made by the user, and not by other sites. Expiry 6 Months Type HTTP G_ENABLED_IDPS use the cookie when customers want to make a referral from their gmail contacts it helps auth the gmail account. Expiry 2 Years Type HTTP test_cookie This cookie is set by DoubleClick which is owned by Google to determine if the website visitors browser supports cookies. Expiry 1 Year Type HTTP Webengage 2 Learn more about webengage privacy Webengage 2 _we_us this is used to send push notification using webengage. Expiry 1 Year Type HTTP WebKlipperAuth used by webenage to track auth of webenagage. Expiry Session Type HTTP LinkedIn 16 learn more about linkedin privacy LinkedIn 16 ln_or Linkedin sets this cookie to registers statistical data on users behavior on the website for internal analytics. Expiry 1 Day Type HTTP JSESSIONID Use to maintain an anonymous user session by the server. Expiry 1 Year Type HTTP li_rm Used as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device. Expiry 1 Year Type HTTP AnalyticsSyncHistory Used to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries. Expiry 6 Months Type HTTP lms_analytics Used to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries. Expiry 6 Months Type HTTP liap Cookie used for Sign-in with Linkedin andor to allow for the Linkedin follow feature. Expiry 6 Months Type HTTP visit allow for the Linkedin follow feature. Expiry 1 Year Type HTTP li_at often used to identify you, including your name, interests, and previous activity. Expiry 2 Months Type HTTP s_plt Tracks the time that the previous page took to load Expiry Session Type HTTP lang Used to remember a users language setting to ensure LinkedIn.com displays in the language selected by the user in their settings Expiry Session Type HTTP s_tp Tracks percent of page viewed Expiry Session Type HTTP AMCV_14215E3D5995C57C0A495C5540AdobeOrg Indicates the start of a session for Adobe Experience Cloud Expiry Session Type HTTP s_pltp Provides page name value URL for use by Adobe Analytics Expiry Session Type HTTP s_tslv Used to retain and fetch time since last visit in Adobe Analytics Expiry 6 Months Type HTTP li_theme Remembers a users display preferencetheme setting Expiry 6 Months Type HTTP li_theme_set Remembers which users have updated their display  theme preferences Expiry 6 Months Type HTTP Preferences 0 Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. Preferences 0 Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. Marketing 4 Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. Marketing 4 Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. Google 11 learn more about google privacy Google 11 _gcl_au Used by Google Adsense, to store and track conversions. Expiry 3 Months Type HTTP SID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry 2 Years Type HTTP SAPISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry 2 Years Type HTTP __Secure- Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry 2 Years Type HTTP APISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry 2 Years Type HTTP SSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry 2 Years Type HTTP HSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry 2 Years Type HTTP DV These cookies are used for the purpose of targeted advertising. Expiry 6 Hours Type HTTP NID These cookies are used for the purpose of targeted advertising. Expiry 1 Month Type HTTP 1P_JAR These cookies are used to gather website statistics, and track conversion rates. Expiry 1 Month Type HTTP OTZ Aggregate analysis of website visitors Expiry 6 Months Type HTTP Facebook 2 learn more about facebook privacy Facebook 2 _fbp This cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. Expiry 4 Months Type HTTP fr Contains a unique browser and user ID, used for targeted advertising. Expiry 2 Months Type HTTP LinkedIn 6 Learn about linkedin policy LinkedIn 6 bscookie Used by LinkedIn to track the use of embedded services. Expiry 1 Year Type HTTP lidc Used by LinkedIn for tracking the use of embedded services. Expiry 1 Day Type HTTP bcookie Used by LinkedIn to track the use of embedded services. Expiry 6 Months Type HTTP aam_uuid Use these cookies to assign a unique ID when users visit a website. Expiry 6 Months Type HTTP UserMatchHistory These cookies are set by LinkedIn for advertising purposes, including tracking visitors so that more relevant ads can be presented, allowing users to use the Apply with LinkedIn or the Sign-in with LinkedIn functions, collecting information about how visitors use the site, etc. Expiry 6 Months Type HTTP li_sugr Used to make a probabilistic match of a users identity outside the Designated Countries Expiry 90 Days Type HTTP Microsoft 2 Learn more about microsoft privacy. Microsoft 2 MR Used to collect information for analytics purposes. Expiry 1 year Type HTTP ANONCHK Used to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation Expiry 1 Day Type HTTP UnclassNameified 0 UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. UnclassNameified 0 UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. Cookie declaration last updated on 24032023 by Analytics Vidhya. Cookies are small text files that can be used by websites to make a users experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our Privacy Policy. Flagship Courses Free Courses Popular Categories Generative AI Tools and Techniques Popular GenAI Models Data Science Tools and Techniques Company Discover Learn Engage Contribute Enterprise Terms  conditions Refund Policy Privacy Policy Cookies Policy  Analytics Vidhya 2024.All rights reserved. GenAI Pinnacle Program Revolutionizing AI Learning  Development Enroll with us today! Continue your learning for FREE Enter email address to continue Enter OTP sent to Edit Resend OTP Resend OTP in 45s", "metadata": {"length": 38471, "has_title": true, "domain": "analyticsvidhya.com"}}
