{"url": "https://www.atlassian.com/blog/artificial-intelligence/artificial-intelligence-101-the-basics-of-ai", "title": "Artificial Intelligence 101 The basics of AI everyone should know - Work Life by Atlassian", "content": "Atlassian Artificial Intelligence 101 The basics of AI everyone should know AI Explained in Simple Terms Head of Product, Atlassian Intelligence Artificial Intelligence, or AI for short, is a groundbreaking technology thats changing how we do everything. In this article, well explore what AI can do and how its influencing our lives. From smart gadgets to chatbots, AI is everywhere, making tasks easier and faster. When youre ready to use AI in the workplace, try Atlassian Intelligence. Atlassian Intelligence brings cutting-edge AI tools right into your workflow, whether youre managing projects in Jira or collaborating in Confluence. Atlassian Intelligence is a suite of AI capabilities that streamlines tasks, boosts productivity, and provides insights into your workwith features tailored to enhance teamwork. With Atlassian Intelligence, you can simplify complex processes and achieve more effortlessly, using advanced technology for everyday project management and collaboration. But what is AI, and how exactly does it work? If youre new to the concept of AI, youre not alone. Whether youre a project manager, marketer, or an executive, you can learn what AI is, how it works, and how it might influence your daily life. Keep reading to learn more about AI. What is artificial intelligence? Artificial intelligence AI involves creating machines that can think like humans and imitate their actions. This field uses various technologies to enable computers to do things that normally need human intelligence, like recognizing images, understanding speech, making decisions, and translating languages. Essentially, artificial intelligence is like having a smart computer that can learn from experience, solve problems, and make decisions on its own  just like a human. How AI works AI learns and becomes more intelligent. It works similarly to how humans learn how to ride a bike. Just like you get better by practicing, AI systems learn from examples and data to improve their performance over time. Instead of being explicitly programmed for every task, AI uses algorithms to learn from experiences. The more data AI systems have and the more they practice, the better they become at their tasks. This ability to learn and improve without constant human instruction makes AI so powerful and versatile in solving complex problems. AI terminology explained When youre first learning about AI, many of the technical terms may seem complicated. Lets break down some of these terms to make them easier to grasp. Large Language Models LLMs Imagine having a conversation with a knowledgeable computer that can understand what youre saying and respond in a way that makes sense. Thats what large language models LLMS can do. Theyre powerful systems that can generate human-like text and help us with tasks like writing articles or answering questions. Datasets Datasets are large collections of information that AI systems use to learn. They can include things like images, paragraphs of text, or even numbers from sensors. AI systems look at these examples and can figure out patterns, allowing them to make decisions just like we do when we learn from seeing examples repeatedly. Machine learning Machine learning allows computers to learn from data. For instance, it allows a computer to recognize cats in pictures by being trained on many examples of images labeled as cats and images labeled as not cats. This training process involves the computer identifying patterns in the data, allowing it to make predictions or decisions based on new information. Algorithm An algorithm is a set of rules that tells the computer how to solve a problem or perform a task, just like following a recipe to bake a cake. Algorithms are used in everything from sorting numbers to recommending movies on streaming platforms. Neural networks Neural networks are like a team of tiny brains inside a computer. Theyre computational models inspired by how our brains work, designed to recognize patterns and solve complex problems. Each neuron in the network processes information and passes it on to others, working together to solve puzzles or identify objects in images. Natural language processing Natural language processing NLP is a subset of AI that helps computers understand, interpret, and generate human language. It teaches computers to understand and talk like humans do, similar to how we interact with virtual assistants like Siri or Alexa. NLP allows machines to read text, translate languages, and even generate responses in conversations. Big data Big data refers to massive collections of information or data that AI uses to learn and make decisions humans might miss. AI images analyze a large amount of data, such as images, texts, or numbers, to find patterns and insights. This can help businesses and scientists make better decisions based on data rather than guesses. Deep learning Deep learning uses neural network algorithms to process complex data and achieve high accuracy in tasks like recognizing faces in photos or understanding spoken language. Similar to how we learn from examples to get smarter, computers learn from vast amounts of data to improve their performance. Structured query language SQL SQL is a programming language used to communicate with databaseshuge libraries that store information and data. It allows people to ask specific questions queries and get answers quickly. Imagine a huge library where you can ask the librarian to find all the books published after 2010. SQL lets you ask the database similar questions to find specific information quickly. Jira query language Jira query language JQL is a unique language used within Jira. JQL works similarly to SQL, helping users search for and filter issues, which can be tasks, bugs, or other types of work items, within Jira based on specific criteria. For example, in Jira, you might use JQL to find all the tasks assigned to you due this week. This language allows you to specify conditions like who the task is assigned to, when its due, or its status. JQL helps you efficiently manage your work by finding and organizing tasks directly within Jira. Using AI in our everyday lives Artificial intelligence is already seamlessly integrated into our daily routines. When you search the internet, AI algorithms work behind the scenes to understand your query and provide relevant results quickly. Whether youre looking for a recipe, learning about a topic, or shopping, AI streamlines information retrieval and makes browsing more personalized. Think about how AI powers personalized recommendations on streaming platforms like Netflix or music services like Spotify. Artificial intelligence analyzes your viewing or listening habits and suggests content that matches your preferences, making entertainment choices more enjoyable and tailored to your tastes. Meanwhile, voice assistants like Siri, Alexa, and Google Assistant also use AI to understand and respond to voice commands, helping you set reminders, play music, or even control smart home devices with natural language. AI represents a monumental leap in technology. It allows computers to respond to human commands and queries by letting you ask questions like a human and get answers tailored to your needs. Just as you would ask a friend for help, AI allows us to ask questions and get answers in intuitive and natural ways. Incorporate AI into your workflows with Atlassian Intelligence, a suite of AI capabilities embedded within tools like Jira, Confluence, Jira Service Management, and more. These tools streamline workflows by automating repetitive tasks, providing intelligent insights from analytics, and enhancing collaboration among teams. Whether youre managing projects, tracking individual tasks, or sharing knowledge, Atlassian Intelligence helps teams work smarter. Jamil Valliani Head of Product, Atlassian Intelligence More in AI AI Collaboration Report Using AI is not enough  heres what your organization is missing What is conversational AI? How to learn AI for beginners Understanding responsible AI practices More Collections Company Culture Embrace transparency, foster a sense of belonging, form connections  and have fun along the way. Your Personality at Work Navigating and celebrating the complexities of our individuality. Earnings Reports A look at what were sharing with our investors and stakeholders each quarter. Artificial Intelligence 101 The basics of AI everyone should know By Atlassian Culture, tech, teams, and tips, delivered twice a month Advice, stories, and expertise about work life today.", "metadata": {"length": 8587, "has_title": true, "domain": "atlassian.com"}}
{"url": "https://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/what-is-artificial-intelligence", "title": "What is Artificial Intelligence? How AI Works  Key Concepts", "content": "Tutorial Playlist Artificial Intelligence Tutorial for Beginners What is Artificial Intelligence Types, History, and Future 24 Cutting-Edge Artificial Intelligence Applications in 2025 How Does AI Work Types of Artificial Intelligence That You Should Know in 2024 Discover the Differences Between AI vs. Machine Learning vs. Deep Learning What Is NLP? Introductory Guide to Natural Language Processing! How to Become an AI Engineer The Top Five Humanoid Robots What Is Virtual Reality? Everything You Need to Know Best AR Apps in 2025 Want an Easy Idea About What Is Augmented Reality? Read This! The Ultimate Guide to What Is SparkAR and Its Effects on Social Media Introduction to Long Short-Term MemoryLSTM Top 30 AI Projects in 2024 Basic, Mid, Advanced The Ultimate Guide to Forward and Backward Chaining in AI Top Artificial Intelligence Techniques Cracking the Code What are the Major Goals of Artificial Intelligence? Conversational AI Enhancing Customer Engagement and Support Overview of Narrow AI Explainable AI A Complete Guide on the Role of AI in Healthcare Top AI Developer Tools You Need to Know in 2025 What is Artificial Intelligence? How AI Works  Key Concepts Lesson 1 of 22By Nikita Duggal Artificial Intelligence Tutorial for Beginners What is Artificial Intelligence Types, History, and Future 24 Cutting-Edge Artificial Intelligence Applications in 2025 How Does AI Work Types of Artificial Intelligence That You Should Know in 2024 Discover the Differences Between AI vs. Machine Learning vs. Deep Learning What Is NLP? Introductory Guide to Natural Language Processing! How to Become an AI Engineer The Top Five Humanoid Robots What Is Virtual Reality? Everything You Need to Know Best AR Apps in 2025 Want an Easy Idea About What Is Augmented Reality? Read This! The Ultimate Guide to What Is SparkAR and Its Effects on Social Media Introduction to Long Short-Term MemoryLSTM Top 30 AI Projects in 2024 Basic, Mid, Advanced The Ultimate Guide to Forward and Backward Chaining in AI Top Artificial Intelligence Techniques Cracking the Code What are the Major Goals of Artificial Intelligence? Conversational AI Enhancing Customer Engagement and Support Overview of Narrow AI Explainable AI A Complete Guide on the Role of AI in Healthcare Top AI Developer Tools You Need to Know in 2025 Table of Contents Artificial intelligence AI is currently one of the hottest buzzwords in tech and with good reason. The last few years have seen several innovations and advancements that have previously been solely in the realm of science fiction slowly transform into reality. Experts regard artificial intelligence as a factor of production, which has the potential to introduce new sources of growth and change the way work is done across industries. For instance, this PWC article predicts that AI could potentially contribute 15.7 trillion to the global economy by 2035. China and the United States are primed to benefit the most from the coming AI boom, accounting for nearly 70 of the global impact. Become an AI and Machine Learning Expert This tutorial provides an overview of AI, including how it works, its pros and cons, its applications, certifications, and why its a good field to master. What Is Artificial Intelligence? Artificial intelligence AI is the simulation of human intelligence in machines that are programmed to think and act like humans. Learning, reasoning, problem-solving, perception, and language comprehension are all examples of cognitive abilities. Artificial Intelligence is a method of making a computer, a computer-controlled robot, or a software think intelligently like the human mind. AI is accomplished by studying the patterns of the human brain and by analyzing the cognitive process. The outcome of these studies develops intelligent software and systems. For example, Natural Language Processing NLP uses AI to analyze and interpret human language in text or speech. It enables applications like chatbots and virtual assistants to understand user queries, extract meaningful information, and deliver accurate, context-aware responses, transforming unstructured communication into actionable insights. Weak AI vs. Strong AI When discussing artificial intelligence AI, it is common to distinguish between two broad categories weak AI and strong AI. Lets explore the characteristics of each type Weak AI Narrow AI Weak AI refers to AI systems that are designed to perform specific tasks and are limited to those tasks only. These AI systems excel at their designated functions but lack general intelligence. Examples of weak AI include voice assistants like Siri or Alexa, recommendation algorithms, and image recognition systems. Weak AI operates within predefined boundaries and cannot generalize beyond their specialized domain. Strong AI General AI Strong AI, also known as general AI, refers to AI systems that possess human-level intelligence or even surpass human intelligence across a wide range of tasks. Strong AI would be capable of understanding, reasoning, learning, and applying knowledge to solve complex problems in a manner similar to human cognition. However, the development of strong AI is still largely theoretical and has not been achieved to date. Learn Core AI Engineering Skills and Tools Types of Artificial Intelligence Below are the various types of AI 1. Purely Reactive These machines do not have any memory or data to work with, specializing in just one field of work. For example, in a chess game, the machine observes the moves and makes the best possible decision to win. 2. Limited Memory These machines collect previous data and continue adding it to their memory. They have enough memory or experience to make proper decisions, but memory is minimal. For example, this machine can suggest a restaurant based on the location data that has been gathered. 3. Theory of Mind This kind of AI can understand thoughts and emotions, as well as interact socially. However, a machine based on this type is yet to be built. 4. Self-Aware Self-aware machines are the future generation of these new technologies. They will be intelligent, sentient, and conscious. Become an AI and Machine Learning Expert Deep Learning vs. Machine Learning Lets explore the contrast between deep learning and machine learning Machine Learning Machine Learning focuses on the development of algorithms and models that enable computers to learn from data and make predictions or decisions without explicit programming. Here are key characteristics of machine learning Deep Learning Deep Learning is a subset of machine learning that focuses on training artificial neural networks inspired by the human brains structure and functioning. Here are key characteristics of deep learning Artificial Intelligence Engineer How Does Artificial Intelligence Work? Put simply, AI systems work by merging large with intelligent, iterative processing algorithms. This combination allows AI to learn from patterns and features in the analyzed data. Each time an Artificial Intelligence system performs a round of data processing, it tests and measures its performance and uses the results to develop additional expertise. Ways of Implementing AI Lets explore the following ways that explain how we can implement AI Machine Learning It is machine learning that gives AI the ability to learn. This is done by using algorithms to discover patterns and generate insights from the data they are exposed to. Deep Learning Deep learning, which is a subcategory of machine learning, provides AI with the ability to mimic a human brains neural network. It can make sense of patterns, noise, and sources of confusion in the data. Consider an image shown below Here, we segregated the various kinds of images using deep learning. The machine goes through multiple features of photographs and distinguishes them with feature extraction. The machine segregates the features of each photo into different categories, such as landscape, portrait, or others. Let us understand how deep learning works. Consider the image shown below The above image depicts the three main layers of a neural network Input Layer The images that we want to segregate go into the input layer. Arrows are drawn from the image on to the individual dots of the input layer. Each of the white dots in the yellow layer input layer are a pixel in the picture. These images fill the white dots in the input layer. We should clearly know these three layers while going through this artificial intelligence tutorial. Hidden Layer The hidden layers are responsible for all our inputs mathematical computations or feature extraction. In the above image, the layers shown in orange represent the hidden layers. The lines that are seen between these layers are called weights. Each one of them usually represents a float number, or a decimal number, which is multiplied by the value in the input layer. All the weights add up in the hidden layer. The dots in the hidden layer represent a value based on the sum of the weights. These values are then passed to the next hidden layer. You may be wondering why there are multiple layers. The hidden layers function as alternatives to some degree. The more the hidden layers are, the more complex the data that goes in and what can be produced. The accuracy of the predicted output generally depends on the number of hidden layers present and the complexity of the data going in. Master the Right AI Tools for the Right Job! Output Layer The output layer gives us segregated photos. Once the layer adds up all these weights being fed in, itll determine if the picture is a portrait or a landscape. Example - Predicting Airfare Costs This prediction is based on various factors, including We begin with some historical data on ticket prices to train the machine. Once our machine is trained, we share new data that will predict the costs. Earlier, when we learned about four kinds of machines, we discussed machines with memory. Here, we talk about the memory only, and how it understands a pattern in the data and uses it to make predictions for the new prices as shown below AI Programming Cognitive Skills Learning, Reasoning and Self-Correction Artificial Intelligence emphasizes three cognitive skills of learning, reasoning, and self-correction, skills that the human brain possess to one degree or another. We define these in the context of AI as However, researchers and programmers have extended and elaborated the goals of AI to the following Logical Reasoning Knowledge Representation Planning and Navigation Natural Language Processing Perception Emergent Intelligence Some of the tasks performed by AI-enabled devices include Advance Your Career With Top AI Engineering Skills Advantages and Disadvantages of AI Artificial intelligence has its pluses and minuses, much like any other concept or innovation. Heres a quick rundown of some pros and cons. Pros Cons Let us continue this article on What is Artificial Intelligence by discussing the applications of AI. Applications of Artificial Intelligence Artificial intelligence AI has a wide range of applications across various industries and domains. Here are some notable applications of AI Natural Language Processing NLP AI is used in NLP to analyze and understand human language. It powers applications such as speech recognition, machine translation, sentiment analysis, and virtual assistants like Siri and Alexa. Image and Video Analysis AI techniques, including computer vision, enable the analysis and interpretation of images and videos. This finds application in facial recognition, object detection and tracking, content moderation, medical imaging, and autonomous vehicles. Robotics and Automation AI plays a crucial role in robotics and automation systems. Robots equipped with AI algorithms can perform complex tasks in manufacturing, healthcare, logistics, and exploration. They can adapt to changing environments, learn from experience, and collaborate with humans. Recommendation Systems AI-powered recommendation systems are used in e-commerce, streaming platforms, and social media to personalize user experiences. They analyze user preferences, behavior, and historical data to suggest relevant products, movies, music, or content. Financial Services AI is extensively used in the finance industry for fraud detection, algorithmic trading, credit scoring, and risk assessment. Machine learning models can analyze vast amounts of financial data to identify patterns and make predictions. Healthcare AI applications in healthcare include disease diagnosis, medical imaging analysis, drug discovery, personalized medicine, and patient monitoring. AI can assist in identifying patterns in medical data and provide insights for better diagnosis and treatment. Virtual Assistants and Chatbots AI-powered virtual assistants and chatbots interact with users, understand their queries, and provide relevant information or perform tasks. They are used in customer support, information retrieval, and personalized assistance. Gaming AI algorithms are employed in gaming for creating realistic virtual characters, opponent behavior, and intelligent decision-making. AI is also used to optimize game graphics, physics simulations, and game testing. Smart Homes and IoT AI enables the development of smart home systems that can automate tasks, control devices, and learn from user preferences. AI can enhance the functionality and efficiency of Internet of Things IoT devices and networks. Cybersecurity AI helps detect and prevent cyber threats by analyzing network traffic, identifying anomalies, and predicting potential attacks. It can also enhance the security of systems and data through advanced threat detection and response mechanisms. These are just a few examples of how AI is applied in various fields. AIs potential is vast, and its applications continue to expand as technology advances. Become an AI Engineer in 11 Months Artificial Intelligence Examples Artificial Intelligence AI has become an integral part of our daily lives, revolutionizing various industries and enhancing user experiences. Here are some notable examples of AI applications ChatGPT ChatGPT is an advanced language model developed by OpenAI. It can generate human-like responses and engage in natural language conversations. It uses deep learning techniques to understand and generate coherent text, making it useful for customer support, chatbots, and virtual assistants. Google Maps Google Maps utilizes AI algorithms to provide real-time navigation, traffic updates, and personalized recommendations. It analyzes vast amounts of data, including historical traffic patterns and user input, to suggest the fastest routes, estimate arrival times, and even predict traffic congestion. Smart Assistants Smart assistants like Amazons Alexa, Apples Siri, and Google Assistant employ AI technologies to interpret voice commands, answer questions, and perform tasks. These assistants use natural language processing and machine learning algorithms to understand user intent, retrieve relevant information, and carry out requested actions. Snapchat Filters Snapchats augmented reality filters, or Lenses, incorporate AI to recognize facial features, track movements, and overlay interactive effects on users faces in real-time. AI algorithms enable Snapchat to apply various filters, masks, and animations that align with the users facial expressions and movements. Self-Driving Cars Self-driving cars rely heavily on AI for perception, decision-making, and control. Using a combination of sensors, cameras, and machine learning algorithms, these vehicles can detect objects, interpret traffic signs, and navigate complex road conditions autonomously, enhancing safety and efficiency on the roads. Wearables Wearable devices, such as fitness trackers and smartwatches, utilize AI to monitor and analyze users health data. They track activities, heart rate, sleep patterns, and more, providing personalized insights and recommendations to improve overall well-being. MuZero MuZero is an AI algorithm developed by DeepMind that combines reinforcement learning and deep neural networks. It has achieved remarkable success in playing complex board games like chess, Go, and shogi at a superhuman level. MuZero learns and improves its strategies through self-play and planning. These examples demonstrate the wide-ranging applications of AI, showcasing its potential to enhance our lives, improve efficiency, and drive innovation across various industries. Find Our Artificial Intelligence Course in Top Cities India United States Other Countries Different Artificial Intelligence Certifications 1. Introduction to Artificial Intelligence Course Simplilearns Artificial Intelligence basics program is designed to help learners decode the mystery of artificial intelligence and its business applications. The course provides an overview of AI concepts and workflows, machine learning and deep learning, and performance metrics. Youll learn the difference between supervised, unsupervised and reinforcement learning, be exposed to use cases, and see how clustering and classification algorithms help identify AI business applications. 2. Machine Learning Course Simplilearns Machine Learning Course will make you an expert in machine learning, a form of artificial intelligence that automates data analysis to enable computers to learn and adapt through experience to do specific tasks without explicit programming. Youll master machine learning concepts and techniques including supervised and unsupervised learning, mathematical and heuristic aspects, and hands-on modeling to develop algorithms and prepare you for the role of a Machine Learning Engineer. 3. Artificial Intelligence Engineer Masters Program Simplilearns Masters in AI, in collaboration with IBM, gives training on the skills required for a successful career in AI. Throughout this exclusive training program, youll master Deep Learning, Machine Learning, and the programming languages required to excel in this domain and kick-start your career in Artificial Intelligence. Reasons to Get an Artificial Intelligence Certification The Key Takeaways Here are the top reasons why you should get a certification in AI if youre looking to join this exciting and growing field 1. Demand for Certified AI Professionals will Continue to Grow The McKinsey Global Institute predicts that approximately 70 percent of businesses will be using at least one type of Artificial Intelligence technology by 2030, and about half of all big companies will embed a full range of Artificial Intelligence technology in their processes. AI will help companies offer customized solutions and instructions to employees in real-time. Therefore, the demand for professionals with skills in emerging technologies like AI will only continue to grow. 2. New and Unconventional Career Paths A Future of Jobs Report released by the World Economic Forum in 2020 predicts that 85 million jobs will be lost to automation by 2025. However, it goes on to say that 97 new positions and roles will be created as industries figure out the balance between machines and humans. Because of AI, new skill sets are required in the workforce, leading to new job opportunities. Some of the top AI roles include 3. Improve Your Earning Potential Many of the top tech enterprises are investing in hiring talent with AI knowledge. The average Artificial Intelligence Engineer can earn 164,000 per year, and AI certification is a step in the right direction for enhancing your earning potential and becoming more marketable. Start your AI journey with our AI  Machine Learning Bootcamp. 4. Higher Chances of a Discussion If you are looking to join the AI industry, then becoming knowledgeable in Artificial Intelligence is just the first step next, you need verifiable credentials. Certification earned after pursuing Simplilearns AI and Ml course will help you reach the interview stage as youll possess skills that many people in the market do not. Certification will help convince employers that you have the right skills and expertise for a job, making you a valuable candidate. Artificial Intelligence is emerging as the next big thing in technology. Organizations are adopting AI and budgeting for certified professionals in the field, thus the growing demand for trained and certified professionals. As this emerging field continues to grow, it will have an impact on everyday life and lead to considerable implications for many industries. FAQs 1. Where is AI used? Artificial intelligence is frequently utilized to present individuals with personalized suggestions based on their prior searches and purchases and other online behavior. AI is extremely crucial in commerce, such as product optimization, inventory planning, and logistics. Machine learning, cybersecurity, customer relationship management, internet searches, and personal assistants are some of the most common applications of AI. Voice assistants, picture recognition for face unlocking in cellphones, and ML-based financial fraud detection are all examples of AI software that is now in use. 2. What is artificial intelligence in simple words? Artificial Intelligence AI in simple words refers to the ability of machines or computer systems to perform tasks that typically require human intelligence. It is a field of study and technology that aims to create machines that can learn from experience, adapt to new information, and carry out tasks without explicit programming. Artificial Intelligence AI refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. 3. What Are the 4 Types of AI? The current categorization system categorizes AI into four basic categories reactive, theory of mind, limited memory, and self-aware. 4. How Is AI Used Today? Machines today can learn from experience, adapt to new inputs, and even perform human-like tasks with help from artificial intelligence AI. Artificial intelligence examples today, from chess-playing computers to self-driving cars, are heavily based on deep learning and natural language processing. There are several examples of AI software in use in daily life, including voice assistants, face recognition for unlocking mobile phones and machine learning-based financial fraud detection. AI software is typically obtained by downloading AI-capable software from an internet marketplace, with no additional hardware required. 6. How is AI helping in our life? AI and ML-powered software and gadgets mimic human brain processes to assist society in advancing with the digital revolution. AI systems perceive their environment, deal with what they observe, resolve difficulties, and take action to help with duties to make daily living easier. People check their social media accounts on a frequent basis, including Facebook, Twitter, Instagram, and other sites. AI is not only customizing your feeds behind the scenes, but it is also recognizing and deleting bogus news. So, AI is assisting you in your daily life. 7. What are the three types of AI? The three types of AI are 8. Is AI dangerous? Aside from planning for a future with super-intelligent computers, artificial intelligence in its current state might already offer problems. 9. What are the advantages of AI? The advantages of AI include reducing the time it takes to complete a task, reducing the cost of previously done activities, continuously and without interruption, with no downtime, and improving the capacities of people with disabilities. 10. What are the 7 main areas of AI? The main seven areas of AI are Find our Post Graduate Program in AI and Machine Learning Online Bootcamp in top cities About the Author Nikita Duggal is a passionate digital marketer with a major in English language and literature, a word connoisseur who loves writing about raging technologies, digital marketing, and career conundrums. Recommended Programs Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Caltech Post Graduate Program in AI and Machine Learning Lifetime access to high-quality, self-paced e-learning content. Recommended Resources How Does AI Work Artificial Intelligence Career Guide A Comprehensive Playbook to Becoming an AI Expert What is Artificial Intelligence and Why Gain AI Certification How to Become an AI Engineer What is ChatGPT AI and Why Does it Matter? Introduction to Artificial Intelligence A Beginners Guide  2009 -2024- Simplilearn Solutions. Follow us! Company Work with us Discover For Businesses Learn On the Go! Trending Post Graduate Programs Trending Master Programs Trending Courses Trending Categories Trending Resources", "metadata": {"length": 24748, "has_title": true, "domain": "simplilearn.com"}}
{"url": "https://www.coursera.org/articles/how-to-learn-artificial-intelligence", "title": "How to Learn Artificial Intelligence A Beginners Guide  Coursera", "content": "How to Learn Artificial Intelligence A Beginners Guide This guide to learning artificial intelligence is suitable for any beginner, no matter where youre starting from. Every time you shop online, search for information on Google, or watch a show on Netflix, you interact with a form of artificial intelligence AI. The applications of AI are everywhere and will only continue to grow. From factory workers to waitstaff to engineers, AI is quickly impacting jobs. Learning AI can help you understand how technology can improve our lives through products and services. There are also plenty of job opportunities in this field, should you choose to pursue it. Learning AI doesnt have to be difficult, but it does require a basic understanding of math and statistics. In this guide, well take you through how to learn AI and create a learning plan. Beginner AI courses Want to build foundational AI knowledge in just a matter of hours? Consider taking one of these popular courses on Coursera In DeepLearning.AIs AI for Everyone, youll learn what AI is, how to build AI projects, and consider AIs social impact in just six hours. In IBMs Generative AI Prompt Engineering Basics, youll explore and practice different prompt engineering techniques in as little as seven hours. What is artificial intelligence? And, why should you learn it? Artificial intelligence AI is the process of simulating human intelligence and task performance with machines, such as computer systems. Tasks may include recognizing patterns, making decisions, experiential learning, and natural language processing NLP. AI is used in many industries driven by technology, such as health care, finance, and transportation. Learning AI is increasingly important because it is a revolutionary technology that is transforming the way we live, work, and communicate with each other. With organizations across industries worldwide collecting big data, AI helps us make sense of it all. AI engineers earn a median salary of 136,620 a year, according to the US Bureau of Labor Statistics 1. Professionals in this field can expect the number of jobs to grow by 23 percent over the next decade. Besides being a lucrative career path, it is a fast-growing field and an intellectually stimulating discipline to learn. Hear more about AI in this lecture from Stanford and DeepLearning.AIs Machine Learning Specialization How long does it take to learn AI? The amount of time it takes to learn artificial intelligence depends on several factors, including Prerequisite knowledge If you have general knowledge of math and statistics, you can skip straight toward learning AI skills and tools. Career intent If you want to pursue a job in the AI field, youll want a more comprehensive education than someone who simply wants to add context to their data analytics role. Background knowledge If youre switching from another major or field, then itll take longer to learn than someone who is already working in the technology field and has a basic understanding of its complex jargon. Artificial intelligence vs. machine learning Whats the difference? Artificial intelligence is computer software that mimics how humans think in order to perform tasks such as reasoning, learning, and analyzing information. Machine learning is a subset of AI that uses algorithms trained on data to produce models that can perform those tasks. AI is often performed using machine learning, but it actually refers to the general concept, while machine learning refers to only one method within AI. Read more Machine Learning vs. AI Differences, Uses, and Benefits How to learn artificial intelligence Here are four steps to guide your learning. To start your journey into AI, develop a learning plan by assessing your current level of knowledge and the amount of time and resources you can devote to learning. 1. Create a learning plan. Before you dive into a class, we recommend developing a learning plan. This includes a tentative timeline, skill-building goals, and the activities, programs, and resources youll need to gain those skills. First, ask yourself the following questions Your level of knowledge of artificial intelligence Are you a true beginner? Do you have a foundation in math and statistical skills? Are you familiar with basic terminology and concepts? Your intention for learning Are you pursuing a new career or just supplementing your current career? How much time you can spend learning Are you currently employed? Do you want to learn full-time or part-time? How much money you can spend Do you want to invest in a boot camp, take professional courses online, or watch some videos on YouTube and TikTok? How do you want to learn Are you interested in pursuing a degree program, a boot camp, or self-teaching through a variety of online courses? Later in this article, well provide an example of a learning plan to help you develop yours. Read more Artificial Intelligence AI Terms A to Z Glossary 2. Master the prerequisite skills. Before starting your learning journey, youll want to have a foundation in the following areas. These skills form a base for learning complex AI skills and tools. Basic statistics AI skills are much easier to learn when you have a firm grasp of statistics and interpreting data. Youll want to know concepts such as statistical significance, regression, distribution, and likelihood, all of which play a role in AI applications. Basic math Understanding AI, especially machine learning and deep learning, relies on knowing mathematical concepts such as calculus, probability, and linear algebra. These frequently appear in AI algorithms and models. Curiosity and adaptability AI is complex and rapidly evolving, so there is a constant need to keep up with new techniques and tools. Those looking to pursue a career in AI should have an insatiable thirst for learning and an adaptable mindset for problem-solving. The depth to which youll need to learn these prerequisite skills depends on your career goals. An aspiring AI engineer will definitely need to master these, while a data analyst looking to expand their skill set may start with an introductory class in AI. If you already have a baseline understanding of statistics and math and are open to learning, you can move on to Step 3. 3. Start learning AI skills. Once youve covered the prerequisites, lets dive into the essential skills youll need for AI. Your level of mastery will depend on the type of role youre pursuing. Programming Knowing how to code is essential to implementing AI applications because you can develop AI algorithms and models, manipulate data, and use AI programs. Python is one of the more popular languages due to its simplicity and adaptability, R is another favorite, and there are plenty of others, such as Java and C. Read more Python or R for Data Analysis Which Should I Learn? Data structures A data structure is a specialized format for organizing, storing, retrieving, and manipulating data. Knowing the different types, such as trees, lists, and arrays, is necessary for writing code that can turn into complex AI algorithms and models. Data science Data science encompasses a wide variety of tools and algorithms used to find patterns in raw data. Data scientists have a deep understanding of the product or service user, as well as the comprehensive process of extracting insights from tons of data. AI professionals need to know data science so they can deliver the right algorithms. Read more What Is Data Science? Definition, Examples, Jobs, and More Machine learning This popular subset of AI is important because it powers many of our products and services today. Machines learn from data to make predictions and improve a products performance. AI professionals need to know different algorithms, how they work, and when to apply them. Deep learning Deep learning is a subset of machine learning that uses many layers of neural networks to understand patterns in data. Its often used in the most advanced AI applications, such as self-driving cars. 4. Get familiar with AI tools and programs. Along with building your AI skills, youll want to know how to use AI tools and programs, such as libraries and frameworks, that will be critical in your AI learning journey. When choosing the right AI tools, its wise to be familiar with which programming languages they align with, since many tools are dependent on the language used. Here are some popular tools and libraries specifically for Python NumPy Scikit-learn Pandas Tensorflow Seaborn Theano Keras PyTorch Matplotlib Learn more 9 Best Python Libraries for Machine Learning How to develop a learning plan Learning on your own and wondering how to stay on track? Develop a learning plan to outline how and where to focus your time. Below, weve provided a sample of a nine-month intensive learning plan, but your timeline may be longer or shorter depending on your career goals. Month 1-3 Basics of mathematics and statistics, programming, and data structures Math and statistics Learn the basics by studying calculus, algebra, statistics, and probability, which will serve as a foundation for your AI journey. Programming Learn a programming language, like Python or R. Youll then become familiar with libraries and packages. Data structures Start learning how to store, retrieve, and manipulate datasets, and then how to clean and prepare them, which is necessary for any AI project. Month 4-6 Dive into data science, machine learning, and deep learning Data science Learn the basics of data science and how AI can help facilitate extracting and deriving insights from data. Machine learning Dive into the various types of machine learning algorithms, such as supervised, unsupervised, and reinforcement learning. Deep learning Understand neural networks and the concepts of deep learning. Month 7-9 Get familiar with AI tools and choose a specialization AI tools Once youve mastered the basics, you can start using the different libraries associated with the programming language you learned, as well as other AI tools such as ChatGPT. Specialization You may want to specialize in a specific area of AI, such as natural language processing, or perhaps how to apply AI to another field. Further learning and job search Start looking for AI jobs, if that was part of your intention for learning. Continue to keep up with AI trends with blogs, podcasts, and more. Have career questions? We have answers. Subscribe to Coursera Career Chat on LinkedIn to receive our weekly, bite-sized newsletter for more work insights, tips, and updates from our in-house team. Start learning AI today Your journey to a career in artificial intelligence can begin with a single step. DeepLearning.AIs AI For Everyone, taught by top instructor Andrew Ng, provides an excellent introduction. In just 10 hours or less, you can learn the fundamentals of AI, how it exists in society, and how to build it in your company. Start your free 7-day trial today. Article sources 1. US Bureau of Labor Statistics. Computer and Information Research Scientists, httpswww.bls.govoohcomputer-and-information-technologycomputer-and-information-research-scientists.htm. Accessed February 21, 2024. Keep reading Coursera Staff Editorial Team Courseras editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals. Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More", "metadata": {"length": 11704, "has_title": true, "domain": "coursera.org"}}
{"url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "title": "Artificial Intelligence, Explained  Carnegie Mellon Universitys Heinz College", "content": "Artificial Intelligence, Explained By Jennifer Monahan Why It Matters Some Basic History and DefinitionsWhat AI Is Alan Turing, one of the founders of AI, suggested in 1950 that if a machine can have a conversation with a human and the human cant distinguish whether they are conversing with another human or with a machine, the machine has demonstrated human intelligence.Machine learning ML first entered the public consciousness in the 1950s, when television viewers watched a demonstration of Arthur Samuels Checkers program defeating its human opponent, Robert Nealy. For a long time, though, AI remained largely confined to the realm of tech geniuses and science fiction enthusiasts. Those tech geniuses accomplished a number of groundbreaking achievements over the last seven decades, including A Timeline Common Terms The terminology around AI can be intimidating. Heres a glossary of key terms youll often hear when people talk about AI.Algorithm a set of rules or instructions that tell a machine what to do with the data input into the system.Deep Learning a method of machine learning that lets computers learn in a way that mimics a human brain, by analyzing lots of information and classifying that information into categories. Deep learning relies on a neural network.Hallucination a situation where an AI system produces fabricated, nonsensical, or inaccurate information. The wrong information is presented with confidence, which can make it difficult for the human user to know whether the answer is reliable.Large Language Model LLM a computer program that has been trained on massive amounts of text data such as books, articles, website content, etc. An LLM is designed to understand and generate human-like text based on the patterns and information it has learned from its training. LLMs use natural language processing NLP techniques to learn to recognize patterns and identify relationships between words. Understanding those relationships helps LLMs generate responses that sound humanits the type of model that powers AI chatbots such as ChatGPT.Machine Learning ML a type of artificial intelligence that uses algorithms which allow machines to learn and adapt from evidence often historical data, without being explicitly programmed to learn that particular thing.Natural Language Processing NLP the ability of machines to use algorithms to analyze large quantities of text, allowing the machines to simulate human conversation and to understand and work with human language.Neural Network a deep learning technique that loosely mimics the structure of a human brain. Just as the brain has interconnected neurons, a neural network has tiny interconnected nodes that work together to process information. Neural networks improve with feedback and training.Token the building block of text that a chatbot uses to process and generate a response. For example, the sentence How are you today? might be separated into the following tokens How, are, you, today, ?. Tokenization helps the chatbot understand the structure and meaning of the input. How Generative AI Works Generative AI is a step forward in the development phase. Instead of just reacting to data input, the system takes in data and then uses predictive algorithms a set of step-by-step instructions to create original content. In the case of a large language model LLM, that content can take the form of original poems, songs, screenplays, and the like produced by AI chatbots such as ChatGPT and Google Bard. The large in LLMs indicates that the language model is trained on a massive quantity of data. Although the outcome makes it seem like the computer is engaged in creative expression, the system is actually just predicting a set of tokens and then selecting one. The model is just predicting the next word. It doesnt understand, explains Rayid Ghani, professor of machine learning at Carnegie Mellon Universitys Heinz College of Information Systems and Public Policy. But as a user playing around with it, it seems to have amazing capabilities, while having very large blind spots. Models like ChatGPT are programmed to select the next token, or word, but not necessarily the most commonly used next word. Chatbots might choose  for example  the fourth most common word in one attempt. When the user submits the exact same prompt to the chatbot the next time, the chatbot could randomly select the second most common word to complete the statement. Thats why we humans can ask a chatbot the same question and receive slightly different responses each time.Tools like Copilot and ChatGPT use that token process to write computer code. Though not always perfect, the initial consensus in the tech industry suggests that these tools can save coders hours of tedious work.Text-to-image models like DALL-E and Stable Diffusion work similarly. The program is trained on lots and lots of pictures and their corresponding descriptions. It learns to recognize patterns and understand the relationships between words and visual elements. So when you give it a prompt that describes an image, it uses those patterns and relationships to generate a new image that fits the description. As a result, these models can create never-before-seen art. A prompt for Carnegie Mellon University Scotty Dog dancing, in the style of pointillism produced this fun gem Philosophers, artists, and creative types are actively debating whether these processes constitute creativity or plagiarism. What AI Is Not Moving Forward AI is changing the way we live, work, and interact with machines. When all thats at stake is our Spotify playlist or which Netflix show we watch next, understanding how AI works is probably not important for a large percentage of the population. But with the advent of generative AI into mainstream consciousness, its time for all of us to start paying attention and to decide what kind of society we want to live in. Interested in how machine learning and artificial intelligence will shape the future? Heinz College empowers data scientists via our Master of Science in Business Intelligence and Data Analytics and Public Policy and Data Analytics programs. The Block Center focuses on how emerging technologies will alter the future of work, how AI and analytics can be harnessed for social good, and how innovation in these spaces can be more inclusive and generate targeted, relevant solutions that reduce inequality and improve quality of life for all. In This Story Prof. Rayid GhaniExpert in AI, policy, and social impact--- Related Articles Advocacy in Action How Heinz College Alumna Emily Reece Helped Save a Lifeline for Foster Families Quentin Auster Leverages Data Science Skills as NobleReach Scholar at FDA How Do the Economic Agendas of Harris, Trump stack up? Heinz College Experts Share Pros and Cons 5000 Forbes Ave, Hamburg Hall, Pittsburgh, PA 15213-3890  412.268.2159 2020 Carnegie Mellon University. All Rights Reserved.", "metadata": {"length": 6939, "has_title": true, "domain": "heinz.cmu.edu"}}
{"url": "https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/", "title": "Fundamentals of machine learning - Training  Microsoft Learn", "content": "This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Fundamentals of machine learning Machine learning is the basis for most modern artificial intelligence solutions. A familiarity with the core concepts on which machine learning is based is an important foundation for understanding AI. Learning objectives After completing this module, you will be able to Prerequisites Before starting this module, you should have Module Assessment Results Assess your understanding of this module. Sign in and answer all questions correctly to earn a pass designation on your profile.", "metadata": {"length": 664, "has_title": true, "domain": "learn.microsoft.com"}}
{"url": "https://www.blog.trainindata.com/machine-learning-fundamentals/", "title": "Machine Learning Fundamentals - Train in Datas Blog", "content": "This site uses cookies We use cookies to recognize your repeated visits and preferences, to measure the effectiveness of our blogs and find out if users find what theyre searching for. By continuing using this site, you consent to the use of these cookies. Learn More. Machine Learning Fundamentals by Priyansh Soni  Mar 11, 2024  Machine Learning At the heart of the digital revolution lies machine learninga powerful tool shaping the future of innovation. At its core, machine learning allows computers to learn from data and make decisions without explicit programming. Understanding the fundamentals of machine learning is the first step towards starting your journey into the fields of machine learning and data science. It is crucial for effectively solving real-world problems using appropriate techniques and models, enabling accurate evaluation, troubleshooting, and fostering innovation in the field. So lets dive into the fundamentals of machine learning. What is Machine Learning? In todays digital age, machines can perform tasks that were once thought to be solely in the realm of human expertise. How can machines carry out these tasks? Thanks to machine learning. Machine learning is a field of computer science that consists of developing procedures that enable computers to learn from data without being explicitly programmed. These procedures are called algorithms. In simple terms, machine learning allows computers to learn from data and make decisions based on what theyve learned. Its like teaching a computer to recognize faces in photos, understand spoken language, translate texts, or even play games like chess or Goall without being explicitly programmed to do so. Machine Learning, Deep Learning and Artificial intelligence You probably hear these terms a lot. How are they related? Machine learning is a subset of artificial intelligence that focuses on developing algorithms that enable computers to learn from data. Deep learning is a specific type of machine learning that uses neural networks to learn complex patterns in data. Artificial neural networks are modeled after the workings of the human brain.. It is said that a neural network can approximate, i.e., learn, any mathematical function, and therefore, their learning potential is enormous. Artificial intelligence AI is a broader concept involving any technique or system that tries to mimic human intelligence. That includes machine learning and deep learning as specific approaches within the field. Data science is an interdisciplinary field that employs scientific methods and machine learning algorithms to extract insights and knowledge from structured and unstructured data. What Can Machine Learning Do? Machine learning allows computers to recognize patterns in data, understand language, identify objects in images or videos, make recommendations, and predict future outcomes based on past data. In fact, machine learning is revolutionizing numerous industries with its ability to analyze vast amounts of data and extract valuable insights. These are some of the key applications Natural Language Processing NLP NLP enables computers to understand, interpret, and generate human language. Thanks to NLP, computers can detect sentiment, translate and make text summaries. Generative AI consists of more modern algorithms that allow computers to return human-like text. These algorithms are called Generative Pretrained Transformers or GPT. They are the state-of-the-art models for NLP. Computer Vision Computer vision teaches computers to interpret and analyze information from images and videos. It enables machines to see and understand the world. Computer vision is used in facial recognition for security systems and authentication, and in self-driving cars for detecting pedestrians, traffic signs, and other objects on the road. Additionally, its used in healthcare for diagnosing diseases from X-ray images and MRI scans. Predictive Analytics Predictive analytics empowers computers to learn patterns from past data, and use them to forecast future trends, behaviors, or outcomes. Data scientists use predictive analytics across industries, for example, to detect fraud, assess credit risk, understand and anticipate customer churn, forecast energy demand, and optimize the supply chain, among many other applications. Recommendation Systems Recommendation systems are algorithms that analyze user preferences. They analyze past behavior, for example, past purchases, viewed films, or listened to and liked songs, to suggest personalized content, products, or services that the customer might be interested in. Recommender systems are used in streaming services like Spotify or Netflix and in e-commerce like Amazon. Speech Recognition Speech recognition involves converting spoken language into text. Once it is in the form of text, we can use NLP to allow computers to understand it. Speech recognition is used in virtual assistants and customer services to understand and respond to users and customers. Other Applications of Machine Learning Other areas, like roboticspowered by reinforcement learningare common examples where machine learning plays a pivotal role. As you can see, machine learning offers boundless applications in the real world. These are possible thanks to different types of machine learning methodologies. What are these methodologies? Types of Machine Learning Algorithms Machine learning methodologies can be broadly categorized into two main types supervised learning and unsupervised learning. Supervised Learning Supervised learning involves training a model on labeled data, where each input is associated with an output. The goal of supervised learning is to learn a mapping function from input variables to output variables. This allows the algorithm to make predictions or decisions when given new, unseen data. As we see in the diagram, initially we have a training set containing many observations, and each observation is labeled. Some are triangles, some are circles, and some are squares. We use that data to train a machine-learning algorithm. The model learns to match observations to shapes based on their characteristics. And later on, we can give new observations to the model, and it will be able to tell us which shape they have. We can use supervised learning for regression and for classification. Regression Regression models predict continuous values. For example, predicting house prices based on features like square footage, number of bedrooms, and location is an example of a regression. Popular algorithms for regression are linear regression, polynomial regression, decision tree regression, random forest regression, and support vector regression. Classification Classification models predict discrete outcomes, or categories. For instance, classifying emails as spam or non-spam based on their content is an example of classification. Popular algorithms for classification are Logistic Regression, Naive Bayes, Support Vector Machines, Decision Trees, Random Forest Classifiers, and K-Nearest Neighbors KNN. Unsupervised Learning Unsupervised learning is a type of machine learning where the algorithm learns patterns and structures from unlabeled data. Unlike supervised learning, there are no predefined labels for unsupervised learning tasks. Instead, the algorithm seeks to discover hidden patterns or groupings within the data. In the following diagram, we pass a dataset without labels to a machine learning model, which, by analyzing the intrinsic data patterns, learns to group observations based on their similarities Unsupervised learning has many applications. It can be used in clustering to find groups of similar observations. It can be used to simplify the data representation through dimensionality reduction. It can also be used to find anomalies. Clustering Clustering algorithms group similar data points together into clusters. The goal is to identify natural groupings or clusters in the data without any prior knowledge of their labels. The grouping is done by identifying similar patterns among variables. Clustering can be used, for example, in customer segmentation to group together customers with similar purchasing behaviors. Some machine learning techniques used for clustering are K-Means Clustering, Hierarchical Clustering, and DBSCAN. Dimensionality Reduction Dimensionality reduction techniques aim to reduce the number of features in a dataset while preserving its essential information. Principal Component Analysis is a popular dimensionality reduction technique that projects high-dimensional data into a lower dimension while preserving as much information as possible. This can help visualize and analyze complex datasets more effectively. Anomaly Detection Anomaly detection with unsupervised learning involves identifying unusual patterns or outliers in data without labeled examples. By analyzing the inherent structure and distribution of the data, unsupervised learning algorithms detect deviations or irregularities that stand out from the typical patterns, thus flagging potential anomalies. Anomaly detection can be done by clustering and finding observations that do not fit in any cluster, by determining distributions and flagging outliers, or by using specific machine learning techniques, like one-class support vector machines or isolation forests. Fundamentals of Machine Learning As you can see, machine learning has many applications in the real world, thanks to different types of machine learning methodologies. However, the fundamentals of machine learning are the same across applications and algorithms. These machine learning basics include key components such as data, algorithms, training, testing, and evaluation techniques, which are essential for building effective models that generalize well to new, unseen data. Lets flesh these components out one by one. Data in Machine Learning Machines learn patterns, make predictions, and generate insights from data. Data is essential for model performance, decision-making, and optimization. In fact, the field of data science is devoted to analyzing, processing, and preparing data, either for machine learning or to extract insight to drive decisions. Data comes in many forms. We can have tables with numbers, images, or text. Images and texts are self-explanatory. Tabular data, however, has different flavors. Lets discover some of them. Features and Labels Tabular data comes in the form of tables, where each row is an observation and each column is a feature or attribute. Features, also called variables, are individual measurable properties or characteristics of the data being analyzed. For example, height is a feature, weight is another feature, as is color, vehicle make, city of residence, and so on. Features serve as input variables for machine learning algorithms and can be numeric, categorical, or binary in nature. They provide the information necessary for the algorithm to learn patterns and make predictions or decisions. Labels, also known as targets or responses, are the outcomes or values we want to predict. In a dataset of house prices, features and variables may include square footage, number of bedrooms, and location, while the label would be the actual sale price of the house. Numerical and Categorical Data Numerical data consists of numerical values that represent quantities or measurements. Examples of numerical variables are the number of rooms in a house, the median income, and the blood pressure, among others. Categorical data consists of categories or labels that represent qualitative attributes or characteristics. Examples of categorical features are gender, marital status, vehicle make, city of residence, and so on. Data Preprocessing The data that is collected either by automated sensors, machines, or systems is not suitable in its raw format to train machine learning models. Instead, data scientists devote a lot of time to preparing data to train machine learning models. Data preprocessing is done to convert the raw data into a processable form that can be fed to a machine-learning model for training and making predictions. In fact, data preprocessing is the initial step in data analysis and machine learning projects. Data preprocessing includes among other things, the following Exploratory Data Analysis Data preprocessing goes hand in hand with exploratory data analysis EDA. Through EDA, data scientists seek to understand data patterns, correlations, and trends to gain insights into the structure, characteristics, and relationships between features. Visualizations, graphs, and plots are actively used during EDA. This step is crucial for data-driven decision-making and hypothesis-testing. EDA also aids in creating predictive features and optimizing model performance. Training and Validation Data Sets After data preprocessing and EDA, we are ready to start training machine learning models. To train machine learning models, we typically split the original data set into two or more sets a training set, a testing set, and a validation set. Training data The training dataset is used to train the machine learning model by adjusting its parameters based on the input features and corresponding target labels. Validation data This set is used to evaluate and adjust a model during training. It acts like pseudo-test data, which provides an independent measure of how well the model generalizes to new data and makes adjustments to improve its effectiveness. Test data This set is used to evaluate the final performance of a trained machine learning model, providing independent examples with input features and target labels that the model has not seen during training or validation. It serves as an unbiased measure to assess the models effectiveness in real-world scenarios. Model Training With the data ready, it is time to train and evaluate the machine learning models. Model training involves feeding the training data into a machine learning algorithm to adjust its parameters and optimize its performance. During model training and evaluation, its important to watch out for two common pitfalls overfitting and underfitting. Overfitting  Underfitting Overfitting occurs when a model learns the training data too well, capturing noise or irrelevant patterns that do not generalize to new data. This leads to poor performance on unseen data. Underfitting occurs when a model is too simplistic to capture the underlying patterns in the data. This leads to poor performance both on the training and test datasets. Bias  Variance Overfitting and underfitting are related to the trade-off between bias and variance in model performance. Bias refers to the error due to overly simplistic assumptions in the model, and variance relates to the models sensitivity to fluctuations in the training data. Bias represents the error introduced by the models assumptions or simplifications. High-bias models underfit the data, leading to poor performance on both training and test datasets. Variance represents the sensitivity of our model to a given data point. High-variance models may overfit the data, capturing noise or irrelevant patterns and failing to generalize to new data. A good model should strike a balance between bias and variance, known as the bias-variance tradeoff. Hyperparameters Hyperparameters are like settings or configurations that govern how a machine learning model operates. These parameters are not learned from the data but are rather adjusted to control the learning of a model. Hyperparameters can be considered like the knobs of the machine learning model, which we can adjust to make changes to how the model fits the data. Examples of hyperparameters are the maximum depth of a decision tree, the number of trees in a random forest, or the kernel type in SVM. Methods like grid search and random search are used to find the optimal values for these hyperparameters in a process called hyperparameter optimization to achieve the best performance from a model. Cross-validation Cross-validation is a technique used to assess the performance and generalization ability of machine learning models. It involves dividing the dataset into multiple subsets, training the model on different combinations of these subsets, and evaluating its performance on the remaining data, aiding in obtaining a more reliable estimate of the models performance. K-fold cross-validation is a popular cross-validation technique. The dataset is divided into K equal-sized subsets folds. The model is trained K times, each time using K-1 folds for training and the remaining fold for validation. This ensures that each data point is used for validation exactly once. The final performance is calculated by averaging the results from the K validation runs. Model Evaluation To assess the performance of a model, we use evaluation metrics. These metrics measure the error in the models predictions. Error in machine learning refers to the difference between the predicted values generated by a model and the actual values observed in the dataset. The smaller the error, the better the performance of the model. There are evaluation metrics for regression and for classification models. Regression Metrics There are several metrics that help us determine the performance of a regression model. Here, I describe the most common ones. Mean Squared Error MSE Measures the average squared difference between the predicted and actual values. A smaller MSE indicates better model performance. Root Mean Squared Error RMSE Similar to MSE but takes the square root of the average squared difference. Its easier to interpret since its in the same units as the target variable. Mean Absolute Error MAE Measures the average absolute difference between the predicted and actual values. It provides a more interpretable measure of error compared to MSE. R-squared Indicates how well the independent variables in a regression model explain the variation in the dependent variable. R-qsuared values vary between 0 and 1, with higher values indicating better model fit. Classification Metrics These are the most common evaluation metrics for classification Accuracy Measures the proportion of correctly classified instances. Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It focuses on the accuracy of positive predictions. Recall Measures the proportion of true positive predictions out of all actual positive instances in the dataset. It focuses on the models ability to capture all positive instances. F1 Score The harmonic mean of precision and recall, the F1 score provides a balance between precision and recall. ROC Curve Receiver Operating Characteristic Curve A graphical plot that illustrates the trade-off between true positive rate TPR and false positive rate FPR across different threshold values. The higher the area under the ROC curve, the better the performance. Confusion matrix a table that summarizes the performance of a classification model by comparing actual and predicted class labels. It provides insights into the models true positive, true negative, false positive, and false negative predictions. Conclusion Machine learning is revolutionizing how we approach digital challenges. It empowers computers to learn autonomously, uncover patterns in data, and transform industries with predictive insights. By grasping the machine learning basics, we open doors to endless possibilities, enabling collaboration between humans and machines for a brighter, more innovative future. Weve gathered a list of data science and machine learning courses and machine learning books that can get you started on your journey. If youve already taken your first steps, you can boost your skills with our advanced machine learning courses. Happy learning and good luck! Follow us  Pages Courses Books Legal Privacy Policy Terms of use Impressum Our reviews on Trustpilot  Train in Data 2024", "metadata": {"length": 20004, "has_title": true, "domain": "blog.trainindata.com"}}
{"url": "https://builtin.com/machine-learning/machine-learning-basics", "title": "Machine Learning Basics  Built In", "content": "Machine Learning Basics Every Beginner Should Know Machine learning is an application of artificial intelligence where a machine learns from past experiences input data and makes future predictions. Machine learning is an application of artificial intelligence where a machine learns from past experiences input data and makes future predictions. Its typically divided into three categories supervised learning, unsupervised learning and reinforcement learning. This article introduces the basics of machine learning theory, laying down the common concepts and techniques involved. This post is intended for people starting with machine learning, making it easy to follow the core concepts and get comfortable with machine learning basics. Machine Learning Explained Machine learning is an application of artificial intelligence in which a machine learns from past experiences or input data to make future predictions. There are three common categories of machine learning supervised learning, unsupervised learning and reinforcement learning. What Is Machine Learning? In 1959, Arthur Samuel, a computer scientist who pioneered the study of artificial intelligence, described machine learning as the study that gives computers the ability to learn without being explicitly programmed. Alan Turings seminal paper introduced a benchmark standard for demonstrating machine intelligence, such that a machine has to be intelligent and responsive in a manner that cannot be differentiated from that of a human being. A more technical definition given by Tom M. Mitchells 1997 paper A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. For example, a handwriting recognition learning problem In order to perform the task T, the system learns from the data set provided. A data set is a collection of many examples. An example is a collection of features. More on Machine LearningAll Machine Learning Models Explained Common Types of Machine Learning Machine learning is generally categorized into three types Supervised learning, unsupervised learning, reinforcement learning. Supervised Learning In supervised learning the machine experiences the examples along with the labels or targets for each example. The labels in the data help the algorithm to correlate the features. Two of the most common supervised machine learning tasks are classification and regression. In classification, the machine learning model takes in data and predicts the most likely category, class or label it belongs to based on its values. Some examples of classification include predicting stock prices and categorizing articles to news, politics or leisure based on its content. In regression, the machine predicts the value of a continuous response variable. Common examples include predicting sales of a new product or a salary for a job based on its description. Unsupervised Learning When we have unclassified and unlabeled data, the system attempts to uncover patterns from the data . There is no label or target given for the examples. One common task is to group similar examples together called clustering. Reinforcement Learning Reinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective goal or maximize along a particular dimension over many steps. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best. This is known as the reinforcement signal. For example, maximizing the points won in a game over a lot of moves. More on Artificial IntelligenceExplore Built Ins AI Coverage Supervised Machine Learning Techniques Regression is a technique used to predict the value of response dependent variables from one or more predictor independent variables. Most commonly used regressions techniques are linear regression and logistic regression. We will discuss the theory behind these two prominent techniques alongside explaining many other key concepts like gradient descent algorithm, overfit and underfit, error analysis, regularization, hyperparameters and cross validation techniques involved in machine learning. Linear Regression In linear regression problems, the goal is to predict a real-value variable y from a given pattern X. In the case of linear regression the output is a linear function of the input. Let y be the output our model predicts y  WXb Here X is a vector or features of an example, W are the weights or vector of parameters that determine how each feature affects the prediction, and b is a bias term. So, our task T is to predict y from X. Now ,we need to measure performance P to know how well the model performs. To calculate the performance of the model, we first calculate the error of each example i as We then take the absolute value of the error to take into account both positive and negative values of error. Finally, we calculate the mean for all recorded absolute errors or the average sum of all absolute errors. Mean absolute error MAE equals the average of all absolute errors A more popular way of measuring model performance is using Mean squared error MSE. This is the average of squared differences between prediction and actual observation. The mean is halved as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the half term. The main aim of training the machine learning algorithm is to adjust the weights W to reduce the MAE or MSE. To minimize the error, the model updates the model parameters W while experiencing the examples of the training set. These error calculations when plotted against the W is also called cost function Jw, since it determines the costpenalty of the model. So, minimizing the error is also called as minimizing the cost function J. Gradient Descent Algorithm In the gradient descent algorithm, we start with random model parameters and calculate the error for each learning iteration, keep updating the model parameters to move closer to the values that results in minimum cost. Repeat until minimum cost In the above equation, we are updating the model parameters after each iteration. The second term of the equation calculates the slope or gradient of the curve at each iteration. The gradient of the cost function is calculated as a partial derivative of cost function J with respect to each model parameter wj, where j takes the value of number of features 1 to n. \u03b1, alpha, is the learning rate, or how quickly we want to move towards the minimum. If \u03b1 is too large, we can overshoot. If \u03b1 is too small, it means small steps of learning, which increases the overall time it takes the model to observe all examples. There are three ways of doing gradient descent Logistic Regression In some problems the response variable isnt normally distributed. For instance, a coin toss can result in two outcomes heads or tails. The Bernoulli distribution describes the probability distribution of a random variable that can take the positive case with probability P or the negative case with probability 1-P. If the response variable represents a probability, it must be constrained to the range 0,1. In logistic regression, the response variable describes the probability that the outcome is the positive case. If the response variable is equal to or exceeds a discrimination threshold, the positive class is predicted. Otherwise, the negative class is predicted. The response variable is modeled as a function of a linear combination of the input variables using the logistic function. Since our hypotheses y has to satisfy 0  y  1, this can be accomplished by plugging logistic function or sigmoid function The function gz maps any real number to the 0, 1 interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification. Now, coming back to our logistic regression problem, lets assume that z is a linear function of a single explanatory variable x. We can then express z as follows And the logistic function can now be written as gx is interpreted as the probability of the dependent variable. gx  0.7, gives us a probability of 70 percent that our output is one. Our probability that our prediction is zero is just the complement of our probability that it is one. For example, if the probability that its one is 70 percent, then the probability that it is zero is 30 percent. The input to the sigmoid function g doesnt need to be a linear function. It can be a circle or any shape. Cost Function We cannot use the same cost function that we used for linear regression because the sigmoid function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. In order to ensure the cost function is convex, and therefore, ensure convergence to the global minimum, the cost function is transformed using the logarithm of the sigmoid function. The cost function for logistic regression looks like Which can be written as So, the cost function for logistic regression is Since the cost function is a convex function, we can run the gradient descent algorithm to find the minimum cost. What Is Underfitting and Overfitting in Machine Learning? We try to make the machine learning algorithm fit the input data by increasing or decreasing the models capacity. In linear regression problems, we increase or decrease the degree of the polynomials. Consider the problem of predicting y from x  R. Since the data doesnt lie in a straight line, the fit is not very good. To increase model capacity, we add another feature by adding the term x\u00b2 to it. This produces a better fit. But if we keep on doing so x\u2075, fifth order polynomial, we may be able to better fit the data but it will not generalize well for new data. Underfitting When the model has fewer features, it isnt able to learn from the data very well. This means the model has a high bias. Overfitting When the model has complex functions, its able to fit the data but is not able to generalize to predict new data. This model has high variance. There are three main options to address the issue of overfitting Regularization in Machine Learning Regularization can be applied to both linear and logistic regression by adding a penalty term to the error function in order to discourage the coefficients or weights from reaching large values. Linear Regression With Regularization The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified linear regression error function Where lambda is our regularization parameter. In order to minimize the error, we use the gradient descent algorithm. We keep updating the model parameters to move closer to the values that result in minimum cost. Then repeat until convergence, with regularization With some manipulation, the above equation can also be represented as The first term in the above equation will always be less than one You can see it as reducing the value of the coefficient by some amount on every update. Logistic Regression With Regularization The cost function of the logistic regression with regularization is Then, repeat until convergence with regularization L1 and L2 Regularization The regularization term used in the previous equations is called L2, or ridge regularization. The L2 penalty aims to minimize the squared magnitude of the weights. There is another regularization called L1, or lasso The L1 penalty aims to minimize the absolute value of the weights. Difference between L1 and L2 Hyperparameters Hyperparameters are higher-level parameters that describe structural information about a model that must be decided before fitting model parameters. Examples of hyperparameters we discussed so far include Learning rate alpha and regularization lambda. Cross-Validation The process to select the optimal values of hyperparameters is called model selection. if we reuse the same test data set over and over again during model selection, it will become part of our training data, and the model will be more likely to over fit. The overall data set is divided into three categories The training set is used to fit the different models, and the performance on the validation set is then used for the model selection. The advantage of keeping a test set that the model hasnt seen before during the training and model selection steps is to avoid overfitting the model. The model is able to better generalize to unseen data. In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is to use cross-validation. More on Machine LearningUnderstanding Feature Importance in Machine Learning 7 Steps of Cross-Validation These are the steps for selecting hyper-parameters using K-fold cross-validation Cross-validation allows us to tune hyperparameters with only our training set. This allows us to keep the test set as a truly unseen data set for selecting the final model. Weve covered some of the key concepts in the field of machine learning, starting with the definition of machine learning and then covering different types of machine learning techniques. We discussed the theory behind the most common regression techniques linear and logistic alongside other key concepts of machine learning. Recent Artificial Intelligence Articles", "metadata": {"length": 13798, "has_title": true, "domain": "builtin.com"}}
{"url": "https://towardsdatascience.com/getting-started-with-the-basics-of-machine-learning-35954a94e961", "title": "Mastering Machine Learning Fundamentals A Comprehensive Guide for Beginners  Towards Data Science", "content": "Sign up Sign in Sign up Sign in A Beginners Guide to Machine Learning and Beyond Article 1 Unlocking the Essentials of Machine Learning for Beginners Ashwin Raj Follow Towards Data Science -- Listen Share Machine Learning has emerged as one of the most sought-after technologies in todays industry landscape. Tech giants like Google, Amazon, and Meta are harnessing the potential of Machine Learning to enhance their products and services. As the opportunities for data scientists skyrocket, there remains a significant group of individuals unsure of what machine learning truly entail In this article, well dive into the world of machine learning, demystifying its fundamentals and shedding light on its immense potential for aspiring data scientists. This article will be the first in a series of articles named Machine Learning Recipes where we will explore every facet of machine learning and data science, first theoretically, then mathematically, and finally practically. If you find this blog helpful, do consider giving it some claps so that Medium knows that youre enjoying what youre reading. For more exciting Machine Learning recipes, make sure to follow me. Lets delve straight into our topic! ML Here, ML There, ML Everywhere Machine learning is an application of artificial intelligence AI that provides a computer or any machine the ability to learn and improve from experience without being explicitly programmed. The idea here is to grant machines the ability to learn things on their own,  adapt to various situations organically. By leveraging such AI technologies, industries are tapping into the potential of machines to revolutionize various sectors. The efficacy of machines lies in their ability to deliver precise result. Unlike human performance which may vary  lead to exhaustion, machines excel at executing repetitive tasks with unparalleled accuracy and unwavering consistency, without getting lethargic With the advent of internet, data generation has surged dramatically. Every action that you take on any website or mobile applications adds to this ever-expanding data pool. The evolution of cloud computing has further driven down costs, opening new avenues for the growth of AIML and related fields Artificial Intelligence vs Machine Learning vs Deep Learning For beginners in the field, distinguishing between Deep Learning DL, Data Science, Artificial Intelligence  Machine Learning can be quite perplexing. Artificial Intelligence, or AI, is an all-encompassing term that encompasses the development of machines capable of simulating human behavior. The goal here is to create intelligent machines that can autonomously perform tasks solve problems and make decisions with minimal human intervention Machine Learning, or ML, on the other hand, is a subset of AI that focuses on the development of statistical models that enable machines to learn and improve from experience. Unlike traditional programming, where explicit instructions are given, these algorithms analyze data to recognize patterns. Deep Learning is a specialized branch of ML that imitates the structure and functioning of the human brain through artificial neural networks. These networks consist of layers of interconnected nodes neurons that process  transform data to uncover intricate patterns from large complex datasets. Lastly, Data Science revolves around the extraction of valuable insights from large volumes of data using scientific methodologies and algorithms. Data Scientists collect, clean, and analyze data to identify patterns  trends that inform decision-making and guide businesses toward optimized outcomes. Explore the Machine Learning Arsenal Machine Learning can be performed using a variety of tools and languages, each catering to different needs and scales of operation. The choice of tools is quite crucial,  depends on the specific requirements of the task at hand. Python has emerged as the de facto programming language for Machine Learning, owing to its versatility, readability, and extensive collection of libraries tailored for data analysis and ML tasks. You may also explore alternatives like Julia, Scala, and C for their performance and efficiency. Ashwin Raj Machine Learning Recipes To train machine learning models, we need data. Data is like the lifeblood of most modern organizations, and it plays a pivotal role in various aspects of decision-making  data analysis. The data used for training such models is commonly stored in CSV files, though, Excel or SQL Tables may also be used. The Diverse Dimensions of ML Algorithms Machine learning encompasses a diverse range of algorithms, each suited for specific situations. Broadly categorized, these algorithms fall under 3 main types Supervised Learning, Unsupervised Learning  Reinforcement Learning. Recently Transfer Learning has also started gaining popularity Supervised Machine Learning Supervised machine learning problems are problems where we want to make predictions based on a set of examples. Some of the most common supervised ML algorithms include Naive Bayes, Linear Regression, XGBoost, Decision Trees, and Support Vector Machines. Unsupervised Machine Learning These algorithms tackle data challenges where the patterns and relationships are not explicitly labeled or classified. By leveraging unsupervised ML algorithms it efficiently identifies patterns, clusters, and trends within the data without requiring human intervention. Various algorithms for performing unsupervised learning include K-means clustering, Hierarchical clustering, GMM, t-SNE, DBSCAN,  Autoencoders Another type of machine learning that is swiftly gaining momentum as a compelling area of study is Reinforcement Learning RL. Often hailed as the beacon of true AI, RL focuses on enabling software agents to make optimal decisions within an environment to maximize cumulative rewards Common Reinforcement Learning techniques include Q-Learning, Policy Gradients, DQN, DDPG, Trust Region Policy Optimization, and Actor-Critics Fueling the Learning Models with Data When it comes to training a model, the amount of data required depends on the specific problem we aim to address. The more diverse and extensive the dataset, the better the models ability to comprehend the trends and patterns There are several popular websites  platforms where you can find datasets for Machine Learning ML models. Some of the common sites include UCI Machine Learning Repos, Kaggle, Reddit dataset, and Google Dataset Search Data is available in massive quantities these days. From logs on smartphones and websites to health devices  we are in a constant process of generating data. Data can be of different types  can broadly be grouped into two types Machine Learning models have the capability to handle both structured and unstructured data. However, before constructing such machine learning models, the conversion of unstructured data into structured data is crucial. Get your ML Models Up and Running Building a machine learning model involves a series of crucial steps that lay the foundation for accurate and effective outcomes. Here are the key steps The first step in building a machine learning model is Data Collection, where we gather  measure information on target variables within an established system. Collecting relevant data is fundamental to the success of any project. Once the data is collected, the next step is Data Cleaning and Manipulation. This phase involves exploring the data, identifying and handling outliers  missing values, and transforming it into the required format for the model. With the prepared data, we proceed to train the model on the training data  test it using test data. This step enables us to assess the models performance and fine-tune it further. Post this, the model can be deployed for inferencing Constant improvement is essential for optimal model performance. We need to regularly fine-tune these ML models to enhance their accuracy, ensuring it yields better results with time. This iterative process helps us achieve the most efficient and reliable machine-learning model that will learn over time Best Resources for Machine Learning Getting started with machine learning can be an exciting yet overwhelming journey. Whether youre looking for interactive tutorials, or comprehensive courses, here are some of the best resources to kickstart your ML journey Machine Learning Offered by Stanford University and DeepLearning.AI. BreakIntoAI with Machine Learning Specialization. Master www.coursera.org Deep Learning Online Training Course  Udacity Deep learning is driving advances in AI that are changing our world. Become an expert in neural networks and more with www.udacity.com With that, we have reached the end of this article. You have chosen the right career at exactly the right time. If you have any questions or if you believe I have made any mistake, feel free to contact me. Get in touch with me over LinkedIn. Read more such Machine Learning Recipes here. Happy Learning! -- -- Published in Towards Data Science Your home for data science and AI. The worlds leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals. Written by Ashwin Raj Do cool things, that matter! No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams", "metadata": {"length": 9376, "has_title": true, "domain": "towardsdatascience.com"}}
